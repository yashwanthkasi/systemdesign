<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-Powered Learning Platform - Master HLD</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #6366f1;
            --primary-dark: #4f46e5;
            --secondary: #8b5cf6;
            --success: #10b981;
            --warning: #f59e0b;
            --danger: #ef4444;
            --bg-light: #f8fafc;
            --bg-dark: #0f172a;
            --card-light: #ffffff;
            --card-dark: #1e293b;
            --text-light: #1e293b;
            --text-dark: #f1f5f9;
            --border-light: #e2e8f0;
            --border-dark: #334155;
            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            transition: background-color 0.3s, color 0.3s;
        }

        body.light-mode {
            background-color: var(--bg-light);
            color: var(--text-light);
        }

        body.dark-mode {
            background-color: var(--bg-dark);
            color: var(--text-dark);
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }

        /* Header */
        .header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px;
            margin-bottom: 30px;
            border-radius: 12px;
            box-shadow: var(--shadow);
        }

        .light-mode .header {
            background: var(--card-light);
        }

        .dark-mode .header {
            background: var(--card-dark);
        }

        .logo {
            font-size: 24px;
            font-weight: 700;
            color: var(--primary);
        }

        .header-actions {
            display: flex;
            gap: 15px;
            align-items: center;
        }

        .theme-toggle, .settings-btn {
            padding: 8px 16px;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.3s;
            background: var(--primary);
            color: white;
        }

        .theme-toggle:hover, .settings-btn:hover {
            background: var(--primary-dark);
            transform: translateY(-2px);
        }

        /* Home View */
        .home-view {
            display: none;
        }

        .home-view.active {
            display: block;
        }

        .welcome-section {
            text-align: center;
            padding: 60px 20px;
            margin-bottom: 40px;
        }

        .welcome-section h1 {
            font-size: 48px;
            margin-bottom: 20px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .welcome-section p {
            font-size: 20px;
            opacity: 0.8;
            max-width: 600px;
            margin: 0 auto;
        }

        .learning-paths {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 30px;
            margin-top: 40px;
        }

        .path-card {
            padding: 30px;
            border-radius: 16px;
            box-shadow: var(--shadow);
            cursor: pointer;
            transition: all 0.3s;
            border: 2px solid transparent;
        }

        .light-mode .path-card {
            background: var(--card-light);
        }

        .dark-mode .path-card {
            background: var(--card-dark);
        }

        .path-card:hover {
            transform: translateY(-5px);
            border-color: var(--primary);
        }

        .path-icon {
            font-size: 48px;
            margin-bottom: 20px;
        }

        .path-title {
            font-size: 24px;
            font-weight: 700;
            margin-bottom: 10px;
        }

        .path-description {
            opacity: 0.7;
            margin-bottom: 20px;
        }

        .path-stats {
            display: flex;
            justify-content: space-between;
            margin-bottom: 20px;
            font-size: 14px;
        }

        .progress-bar {
            width: 100%;
            height: 8px;
            background: rgba(99, 102, 241, 0.2);
            border-radius: 4px;
            overflow: hidden;
            margin-bottom: 15px;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, var(--primary), var(--secondary));
            transition: width 0.5s ease;
        }

        .path-actions {
            display: flex;
            gap: 10px;
        }

        .btn {
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
        }

        .btn-primary {
            background: var(--primary);
            color: white;
            flex: 1;
        }

        .btn-primary:hover {
            background: var(--primary-dark);
        }

        .btn-secondary {
            background: transparent;
            color: var(--primary);
            border: 2px solid var(--primary);
        }

        .btn-secondary:hover {
            background: var(--primary);
            color: white;
        }

        /* Learning View */
        .learning-view {
            display: none;
        }

        .learning-view.active {
            display: grid;
            grid-template-columns: 1fr 350px;
            gap: 30px;
        }

        .content-area {
            padding: 30px;
            border-radius: 16px;
            box-shadow: var(--shadow);
            min-height: 600px;
        }

        .light-mode .content-area {
            background: var(--card-light);
        }

        .dark-mode .content-area {
            background: var(--card-dark);
        }

        .breadcrumb {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
            font-size: 14px;
            opacity: 0.7;
        }

        .breadcrumb-item {
            cursor: pointer;
        }

        .breadcrumb-item:hover {
            color: var(--primary);
        }

        .concept-header {
            margin-bottom: 30px;
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 20px;
        }

        .concept-header-content {
            flex: 1;
        }

        .concept-title {
            font-size: 32px;
            font-weight: 700;
            margin-bottom: 10px;
            color: var(--primary);
        }

        .concept-meta {
            display: flex;
            gap: 20px;
            font-size: 14px;
            opacity: 0.7;
        }

        .regenerate-btn {
            padding: 10px 16px;
            border: none;
            border-radius: 8px;
            background: var(--primary);
            color: white;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            font-size: 14px;
            white-space: nowrap;
        }

        .regenerate-btn:hover {
            background: var(--primary-dark);
            transform: translateY(-2px);
        }

        .regenerate-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .concept-content {
            font-size: 16px;
            line-height: 1.8;
        }

        .concept-content h2 {
            margin-top: 30px;
            margin-bottom: 15px;
            color: var(--primary);
        }

        .concept-content h3 {
            margin-top: 20px;
            margin-bottom: 10px;
        }

        .concept-content ul, .concept-content ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }

        .concept-content li {
            margin-bottom: 10px;
        }

        .concept-content code {
            background: rgba(99, 102, 241, 0.1);
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
        }

        .concept-content pre {
            background: rgba(99, 102, 241, 0.1);
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border-left: 4px solid var(--primary);
        }

        /* Enhanced Content Styling */
        .concept-content .highlight {
            background: linear-gradient(120deg, rgba(251, 191, 36, 0.2), rgba(251, 191, 36, 0.1));
            padding: 2px 6px;
            border-radius: 3px;
            font-weight: 600;
            color: inherit;
        }

        .concept-content .key-concept {
            background: rgba(99, 102, 241, 0.08);
            padding: 3px 8px;
            border-left: 3px solid var(--primary);
            border-radius: 3px;
            font-weight: 700;
            display: inline-block;
            margin: 0 4px;
        }

        .concept-content .content-box {
            padding: 16px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid;
            background-size: 1.5em 1.5em;
            background-position: 12px 12px;
            background-repeat: no-repeat;
            padding-left: 40px;
        }

        .light-mode .concept-content .content-box {
            background-color: rgba(0, 0, 0, 0.02);
        }

        .dark-mode .concept-content .content-box {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .concept-content .story-box {
            border-color: #8b5cf6;
            color: inherit;
        }

        .light-mode .concept-content .story-box {
            background-color: rgba(139, 92, 246, 0.08);
        }

        .dark-mode .concept-content .story-box {
            background-color: rgba(139, 92, 246, 0.12);
        }

        .concept-content .story-box::before {
            content: "üìñ";
            position: absolute;
            margin-left: -28px;
            font-size: 20px;
        }

        .concept-content .analogy-box {
            border-color: #f59e0b;
            color: inherit;
        }

        .light-mode .concept-content .analogy-box {
            background-color: rgba(245, 158, 11, 0.08);
        }

        .dark-mode .concept-content .analogy-box {
            background-color: rgba(245, 158, 11, 0.12);
        }

        .concept-content .analogy-box::before {
            content: "üí°";
            position: absolute;
            margin-left: -28px;
            font-size: 20px;
        }

        .concept-content .example-box {
            border-color: #3b82f6;
            color: inherit;
        }

        .light-mode .concept-content .example-box {
            background-color: rgba(59, 130, 246, 0.08);
        }

        .dark-mode .concept-content .example-box {
            background-color: rgba(59, 130, 246, 0.12);
        }

        .concept-content .example-box::before {
            content: "üíª";
            position: absolute;
            margin-left: -28px;
            font-size: 20px;
        }

        .concept-content .warning-box {
            border-color: #ef4444;
            color: inherit;
        }

        .light-mode .concept-content .warning-box {
            background-color: rgba(239, 68, 68, 0.08);
        }

        .dark-mode .concept-content .warning-box {
            background-color: rgba(239, 68, 68, 0.12);
        }

        .concept-content .warning-box::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            margin-left: -28px;
            font-size: 20px;
        }

        .concept-content .tip-box {
            border-color: #10b981;
            color: inherit;
        }

        .light-mode .concept-content .tip-box {
            background-color: rgba(16, 185, 129, 0.08);
        }

        .dark-mode .concept-content .tip-box {
            background-color: rgba(16, 185, 129, 0.12);
        }

        .concept-content .tip-box::before {
            content: "‚ú®";
            position: absolute;
            margin-left: -28px;
            font-size: 20px;
        }

        .concept-content .real-world-box {
            border-color: var(--primary);
            color: inherit;
        }

        .light-mode .concept-content .real-world-box {
            background-color: rgba(99, 102, 241, 0.08);
        }

        .dark-mode .concept-content .real-world-box {
            background-color: rgba(99, 102, 241, 0.12);
        }

        .concept-content .real-world-box::before {
            content: "üöÄ";
            position: absolute;
            margin-left: -28px;
            font-size: 20px;
        }

        .concept-content .box-title {
            font-weight: 700;
            margin-top: 0;
            margin-bottom: 10px;
            font-size: 15px;
        }

        .loading-spinner {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 60px;
        }

        .spinner {
            width: 50px;
            height: 50px;
            border: 4px solid rgba(99, 102, 241, 0.2);
            border-top-color: var(--primary);
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            to { transform: rotate(360deg); }
        }

        .loading-text {
            margin-top: 20px;
            font-size: 16px;
            opacity: 0.7;
        }

        /* Sidebar */
        .sidebar {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }

        .progress-panel, .navigation-panel {
            padding: 25px;
            border-radius: 16px;
            box-shadow: var(--shadow);
        }

        .light-mode .progress-panel, .light-mode .navigation-panel {
            background: var(--card-light);
        }

        .dark-mode .progress-panel, .dark-mode .navigation-panel {
            background: var(--card-dark);
        }

        .panel-title {
            font-size: 18px;
            font-weight: 700;
            margin-bottom: 20px;
        }

        .progress-item {
            margin-bottom: 20px;
        }

        .progress-label {
            display: flex;
            justify-content: space-between;
            margin-bottom: 8px;
            font-size: 14px;
        }

        .stat-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin-top: 20px;
        }

        .stat-box {
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }

        .light-mode .stat-box {
            background: var(--bg-light);
        }

        .dark-mode .stat-box {
            background: var(--bg-dark);
        }

        .stat-value {
            font-size: 24px;
            font-weight: 700;
            color: var(--primary);
        }

        .stat-label {
            font-size: 12px;
            opacity: 0.7;
            margin-top: 5px;
        }

        .nav-buttons {
            display: flex;
            flex-direction: column;
            gap: 10px;
        }

        .nav-btn {
            padding: 12px;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 600;
            transition: all 0.3s;
            text-align: left;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .nav-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .nav-btn-primary {
            background: var(--primary);
            color: white;
        }

        .nav-btn-primary:hover:not(:disabled) {
            background: var(--primary-dark);
        }

        .nav-btn-secondary {
            background: transparent;
            border: 2px solid var(--primary);
            color: var(--primary);
        }

        .nav-btn-secondary:hover:not(:disabled) {
            background: var(--primary);
            color: white;
        }

        /* Quiz View */
        .quiz-container {
            padding: 30px;
        }

        .quiz-header {
            margin-bottom: 30px;
        }

        .quiz-title {
            font-size: 28px;
            font-weight: 700;
            margin-bottom: 15px;
        }

        .quiz-meta {
            display: flex;
            gap: 20px;
            font-size: 14px;
        }

        .difficulty-selector {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
        }

        .difficulty-btn {
            padding: 8px 16px;
            border: 2px solid var(--border-light);
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s;
            background: transparent;
        }

        .difficulty-btn.active {
            background: var(--primary);
            color: white;
            border-color: var(--primary);
        }

        .question-card {
            padding: 25px;
            border-radius: 12px;
            margin-bottom: 30px;
            border: 2px solid var(--border-light);
        }

        .dark-mode .question-card {
            border-color: var(--border-dark);
        }

        .question-number {
            font-size: 14px;
            font-weight: 700;
            color: var(--primary);
            margin-bottom: 10px;
        }

        .question-text {
            font-size: 18px;
            font-weight: 600;
            margin-bottom: 20px;
        }

        .options-list {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }

        .option {
            padding: 15px;
            border: 2px solid var(--border-light);
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s;
        }

        .dark-mode .option {
            border-color: var(--border-dark);
        }

        .option:hover {
            border-color: var(--primary);
            background: rgba(99, 102, 241, 0.05);
        }

        .option.selected {
            border-color: var(--primary);
            background: rgba(99, 102, 241, 0.1);
        }

        .option.correct {
            border-color: var(--success);
            background: rgba(16, 185, 129, 0.1);
        }

        .option.incorrect {
            border-color: var(--danger);
            background: rgba(239, 68, 68, 0.1);
        }

        .quiz-actions {
            display: flex;
            gap: 15px;
            margin-top: 30px;
        }

        .quiz-results {
            padding: 30px;
            border-radius: 12px;
            text-align: center;
            margin-top: 30px;
        }

        .light-mode .quiz-results {
            background: var(--bg-light);
        }

        .dark-mode .quiz-results {
            background: var(--card-dark);
        }

        .results-score {
            font-size: 48px;
            font-weight: 700;
            color: var(--primary);
            margin-bottom: 10px;
        }

        .results-message {
            font-size: 20px;
            margin-bottom: 30px;
        }

        /* Quiz History */
        .history-view {
            display: none;
            padding: 30px;
        }

        .history-view.active {
            display: block;
        }

        .history-filters {
            display: flex;
            gap: 15px;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }

        .filter-select {
            padding: 10px 15px;
            border: 2px solid var(--border-light);
            border-radius: 8px;
            font-size: 14px;
            cursor: pointer;
        }

        .dark-mode .filter-select {
            background: var(--card-dark);
            color: var(--text-dark);
            border-color: var(--border-dark);
        }

        .history-list {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }

        .history-item {
            padding: 20px;
            border-radius: 12px;
            border: 2px solid var(--border-light);
        }

        .dark-mode .history-item {
            border-color: var(--border-dark);
        }

        .history-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 10px;
        }

        /* Settings Modal */
        .modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.7);
            z-index: 1000;
            align-items: center;
            justify-content: center;
        }

        .modal.active {
            display: flex;
        }

        .modal-content {
            padding: 40px;
            border-radius: 16px;
            max-width: 500px;
            width: 90%;
            max-height: 80vh;
            overflow-y: auto;
        }

        .light-mode .modal-content {
            background: var(--card-light);
        }

        .dark-mode .modal-content {
            background: var(--card-dark);
        }

        .modal-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 30px;
        }

        .modal-title {
            font-size: 24px;
            font-weight: 700;
        }

        .close-btn {
            background: none;
            border: none;
            font-size: 24px;
            cursor: pointer;
            opacity: 0.7;
        }

        .close-btn:hover {
            opacity: 1;
        }

        .settings-group {
            margin-bottom: 25px;
        }

        .settings-label {
            display: block;
            font-weight: 600;
            margin-bottom: 10px;
        }

        .settings-input {
            width: 100%;
            padding: 12px;
            border: 2px solid var(--border-light);
            border-radius: 8px;
            font-size: 14px;
        }

        .dark-mode .settings-input {
            background: var(--bg-dark);
            color: var(--text-dark);
            border-color: var(--border-dark);
        }

        .settings-help {
            font-size: 12px;
            opacity: 0.7;
            margin-top: 5px;
        }

        /* Responsive */
        @media (max-width: 968px) {
            .learning-view.active {
                grid-template-columns: 1fr;
            }

            .sidebar {
                order: -1;
            }
        }

        @media (max-width: 640px) {
            .learning-paths {
                grid-template-columns: 1fr;
            }

            .stat-grid {
                grid-template-columns: 1fr;
            }
        }

        /* Mode Selection Modal */
        .mode-modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.7);
            z-index: 1000;
            justify-content: center;
            align-items: center;
        }

        .mode-modal.active {
            display: flex;
        }

        .mode-modal-content {
            background: var(--card-light);
            padding: 50px 40px;
            border-radius: 16px;
            text-align: center;
            max-width: 600px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        }

        .dark-mode .mode-modal-content {
            background: var(--card-dark);
        }

        .mode-modal-title {
            font-size: 32px;
            font-weight: 700;
            margin-bottom: 15px;
            color: var(--primary);
        }

        .mode-modal-subtitle {
            font-size: 16px;
            opacity: 0.7;
            margin-bottom: 40px;
        }

        .mode-options {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 30px;
        }

        .mode-option {
            padding: 30px 20px;
            border: 2px solid var(--border-light);
            border-radius: 12px;
            cursor: pointer;
            transition: all 0.3s;
            background: var(--bg-light);
        }

        .dark-mode .mode-option {
            background: rgba(100, 100, 100, 0.1);
            border-color: var(--border-dark);
        }

        .light-mode .mode-option {
            background: var(--bg-light);
            border-color: var(--border-light);
        }

        .mode-option:hover {
            transform: translateY(-5px);
            border-color: var(--primary);
            box-shadow: 0 8px 20px rgba(99, 102, 241, 0.2);
        }

        .mode-option-icon {
            font-size: 48px;
            margin-bottom: 15px;
        }

        .mode-option-title {
            font-size: 20px;
            font-weight: 700;
            margin-bottom: 10px;
            color: var(--text-light);
        }

        .dark-mode .mode-option-title {
            color: var(--text-dark);
        }

        .mode-option-description {
            font-size: 14px;
            opacity: 0.7;
            line-height: 1.5;
        }

        .mode-modal-buttons {
            display: flex;
            gap: 15px;
            justify-content: center;
        }

        .mode-cancel-btn {
            padding: 12px 30px;
            border: 2px solid var(--primary);
            background: transparent;
            color: var(--primary);
            border-radius: 8px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
        }

        .mode-cancel-btn:hover {
            background: var(--primary);
            color: white;
        }

        @media (max-width: 640px) {
            .mode-options {
                grid-template-columns: 1fr;
            }

            .mode-modal-content {
                padding: 30px 20px;
                margin: 20px;
            }
        }
    </style>
</head>
<body class="light-mode">
    <div class="container">
        <!-- Header -->
        <div class="header">
            <div class="logo">üéì AI Learning Platform</div>
            <div class="header-actions">
                <button class="theme-toggle" onclick="toggleTheme()">üåô Toggle Theme</button>
                <button class="settings-btn" onclick="openSettings()">‚öôÔ∏è Settings</button>
            </div>
        </div>

        <!-- Home View -->
        <div class="home-view active" id="homeView">
            <div class="welcome-section">
                <h1>Master System Design with AI</h1>
                <p>A personalized, AI-powered learning journey that adapts to your pace and style</p>
            </div>

            <div class="learning-paths" id="learningPaths">
                <!-- Learning paths will be rendered here -->
            </div>
        </div>

        <!-- Mode Selection Modal -->
        <div class="mode-modal" id="modeModal">
            <div class="mode-modal-content">
                <div class="mode-modal-title">Choose Your Learning Mode</div>
                <div class="mode-modal-subtitle">Select how you'd like to learn</div>
                
                <div class="mode-options">
                    <div class="mode-option" onclick="selectMode('online')">
                        <div class="mode-option-icon">üåê</div>
                        <div class="mode-option-title">Online Mode</div>
                        <div class="mode-option-description">AI-powered interactive learning with real-time explanations and personalization</div>
                    </div>
                    <div class="mode-option" onclick="selectMode('offline')">
                        <div class="mode-option-icon">üìö</div>
                        <div class="mode-option-title">Offline Mode</div>
                        <div class="mode-option-description">Pre-loaded content. Learn without internet connection</div>
                    </div>
                </div>

                <div class="mode-modal-buttons">
                    <button class="mode-cancel-btn" onclick="cancelModeSelection()">Cancel</button>
                </div>
            </div>
        </div>

        <!-- Learning View -->
        <div class="learning-view" id="learningView">
            <div class="content-area">
                <div class="breadcrumb" id="breadcrumb">
                    <span class="breadcrumb-item" onclick="goHome()">Home</span>
                    <span>‚Ä∫</span>
                    <span class="breadcrumb-item" id="pathName">HLD Master</span>
                </div>

                <div id="contentContainer">
                    <!-- Content will be dynamically loaded here -->
                </div>
            </div>

            <div class="sidebar">
                <div class="progress-panel">
                    <div class="panel-title">Your Progress</div>
                    <div class="progress-item">
                        <div class="progress-label">
                            <span>Overall Progress</span>
                            <span id="overallProgress">0%</span>
                        </div>
                        <div class="progress-bar">
                            <div class="progress-fill" id="progressFill" style="width: 0%"></div>
                        </div>
                    </div>
                    <div class="stat-grid">
                        <div class="stat-box">
                            <div class="stat-value" id="conceptsCompleted">0</div>
                            <div class="stat-label">Concepts</div>
                        </div>
                        <div class="stat-box">
                            <div class="stat-value" id="quizzesCompleted">0</div>
                            <div class="stat-label">Quizzes</div>
                        </div>
                        <div class="stat-box">
                            <div class="stat-value" id="averageScore">0%</div>
                            <div class="stat-label">Avg Score</div>
                        </div>
                        <div class="stat-box">
                            <div class="stat-value" id="streakDays">0</div>
                            <div class="stat-label">Day Streak</div>
                        </div>
                    </div>
                </div>

                <div class="navigation-panel">
                    <div class="panel-title">Navigation</div>
                    <div class="nav-buttons">
                        <button class="nav-btn nav-btn-primary" id="takeQuizBtn" onclick="startQuiz()">
                            üìù Take Quiz
                        </button>
                        <button class="nav-btn nav-btn-secondary" id="nextConceptBtn" onclick="nextConcept()" disabled>
                            ‚û°Ô∏è Next Concept
                        </button>
                        <button class="nav-btn nav-btn-secondary" id="prevConceptBtn" onclick="prevConcept()">
                            ‚¨ÖÔ∏è Previous Concept
                        </button>
                        <button class="nav-btn nav-btn-secondary" onclick="viewHistory()">
                            üìä Quiz History
                        </button>
                        <button class="nav-btn nav-btn-secondary" onclick="goHome()">
                            üè† Back to Home
                        </button>
                    </div>
                </div>
            </div>
        </div>

        <!-- Quiz History View -->
        <div class="history-view" id="historyView">
            <div class="breadcrumb">
                <span class="breadcrumb-item" onclick="goHome()">Home</span>
                <span>‚Ä∫</span>
                <span class="breadcrumb-item" onclick="returnToLearning()">Learning</span>
                <span>‚Ä∫</span>
                <span>Quiz History</span>
            </div>

            <h1 style="margin: 20px 0;">Quiz History</h1>

            <div class="history-filters">
                <select class="filter-select" id="conceptFilter" onchange="filterHistory()">
                    <option value="all">All Concepts</option>
                </select>
                <select class="filter-select" id="difficultyFilter" onchange="filterHistory()">
                    <option value="all">All Difficulties</option>
                    <option value="easy">Easy</option>
                    <option value="medium">Medium</option>
                    <option value="hard">Hard</option>
                    <option value="expert">Expert</option>
                </select>
                <button class="btn btn-primary" onclick="generateWeakAreasQuiz()">
                    üéØ Practice Weak Areas
                </button>
            </div>

            <div class="history-list" id="historyList">
                <!-- History items will be rendered here -->
            </div>
        </div>
    </div>

    <!-- Settings Modal -->
    <div class="modal" id="settingsModal">
        <div class="modal-content">
            <div class="modal-header">
                <div class="modal-title">Settings</div>
                <button class="close-btn" onclick="closeSettings()">√ó</button>
            </div>
            <div class="settings-group">
                <label class="settings-label">Groq API Key</label>
                <input type="password" class="settings-input" id="apiKeyInput" placeholder="Enter your Groq API key">
                <div class="settings-help">Get your API key from https://console.groq.com</div>
            </div>
            <div class="settings-group">
                <label class="settings-label">Model</label>
                <select class="settings-input" id="modelSelect">
                    <option value="llama-3.3-70b-versatile">Llama 3.3 70B (Recommended)</option>
                    <option value="llama-3.1-70b-versatile">Llama 3.1 70B</option>
                    <option value="mixtral-8x7b-32768">Mixtral 8x7B</option>
                </select>
            </div>
            <button class="btn btn-primary" onclick="saveSettings()" style="width: 100%; margin-top: 20px;">
                Save Settings
            </button>
            <button class="btn btn-secondary" onclick="resetProgress()" style="width: 100%; margin-top: 10px;">
                üóëÔ∏è Reset All Progress
            </button>
        </div>
    </div>

    <script>
        // ==================== STATE MANAGEMENT ====================
        const APP_STATE = {
            currentPath: null,
            currentConceptIndex: 0,
            theme: 'light',
            apiKey: '',
            model: 'llama-3.3-70b-versatile',
            isQuizMode: false,
            isOfflineMode: false,
            pendingPathId: null
        };

        // ==================== FALLBACK CONTENT (Offline Mode) ====================
        const FALLBACK_CONTENT = {
            // Day 1: Fundamentals
            'hld_0': `## Introduction to System Design

[STORY]
**The Day Twitter Broke**

It was February 3, 2013 ‚Äì Super Bowl Sunday. As Beyonc√© took the halftime stage, something unexpected happened: the stadium lights went dark. Within seconds, Oreo's social media team tweeted "Power out? No problem. You can still dunk in the dark." 

But here's what most people don't know: Twitter's infrastructure nearly collapsed that night. Over 24 million tweets flooded in during the game. The engineering team watched in horror as servers strained under 10x normal load. Some tweets took minutes to appear. Others vanished entirely.

**This is why system design matters.** It's the difference between "going viral" and "going down."

[ANALOGY]
## Think of System Design Like City Planning

Imagine you're designing a city from scratch. You need to think about:
- **Roads** (how data flows between services)
- **Power grid** (ensuring reliable service)
- **Water supply** (handling peak demand)
- **Emergency services** (what happens when things fail?)

A badly designed city has traffic jams, power outages, and chaos. A well-designed system handles rush hour smoothly.

## What is System Design?

**{System Design}** is the process of defining the architecture, components, modules, interfaces, and data flow of a system to satisfy specified requirements.

In simpler terms: **It's deciding HOW to build something before you build it.**

### Key Objectives

1. **{Scalability}** - Can your system handle 10x users tomorrow?
   - *Example:* Netflix serves 200+ million users because they designed for scale from day one

2. **{Reliability}** - Does it work when things break?
   - *Example:* Amazon's shopping cart survives even if half their servers die

3. **{Performance}** - Is it fast enough?
   - *Example:* Google returns search results in under 200ms

4. **{Maintainability}** - Can your team actually work on it?
   - *Example:* Netflix has 1000+ microservices, each owned by small teams

[EXAMPLE]
## Practical Example: Instagram's Architecture

When Instagram launched, they had:
- 2 engineers
- 1 server
- Simple architecture: User ‚Üí Web Server ‚Üí Database

After going viral (25 million users in 2 years):
- 100+ engineers
- Thousands of servers
- Complex architecture with CDNs, caches, message queues, and sharded databases

\`\`\`
Simple Architecture (Day 1):
User ‚Üí Web Server ‚Üí PostgreSQL

Scaled Architecture (Year 2):
User ‚Üí CDN ‚Üí Load Balancer ‚Üí [App Servers] ‚Üí Cache (Redis) ‚Üí [Sharded DBs]
                                    ‚Üì
                              Message Queue ‚Üí Workers
\`\`\`

[TIP]
## Approaching System Design

**Start with requirements, not solutions.** Before drawing boxes and arrows:
1. What does the system need to DO? (Functional requirements)
2. How WELL does it need to do it? (Non-functional: speed, scale, reliability)
3. What are the CONSTRAINTS? (Budget, time, team size)

[WARNING]
## Common Beginner Mistakes

1. **Premature optimization** - Don't design for 1 billion users on day one
2. **Ignoring failure modes** - Everything fails. Plan for it.
3. **Over-engineering** - The simplest solution that works is often best
4. **Not asking questions** - Requirements are never complete. Clarify!

[REAL-WORLD]
## How Big Companies Think

- **Netflix**: "We design for failure. Our systems expect components to fail constantly."
- **Google**: "Design for 10x your current scale, not 100x."
- **Amazon**: "Everything should be a service that can scale independently."

The best system designers aren't those who know the most technologies‚Äîthey're the ones who ask the best questions and make the right trade-offs.`,

            'hld_1': `## Requirements Gathering & Constraints

[STORY]
**The $440 Million Button**

In 2009, a major e-commerce company discovered something shocking: a single form field was costing them $300 million per year in lost sales.

The culprit? A "Register" button during checkout. Users abandoned their carts rather than create an account. When they replaced it with "Continue as Guest," annual revenue increased by $300 million.

**The engineering team had built exactly what was specified.** The problem? Nobody specified the RIGHT requirements.

This is why requirements gathering isn't bureaucratic busywork‚Äîit's the foundation that determines whether you build the right system.

## Functional vs Non-Functional Requirements

### **{Functional Requirements}** - What the system DOES
These are the features users directly interact with:
- "Users can upload photos"
- "Users can follow other users"
- "Users can search for content"

### **{Non-Functional Requirements}** - How WELL it does it
These define quality attributes:
- **Latency**: "Search results appear within 200ms"
- **Availability**: "System is up 99.99% of the time"
- **Scale**: "Support 10 million daily active users"
- **Consistency**: "Users always see their own posts immediately"

[ANALOGY]
## Think Like a Restaurant Owner

**Functional**: What's on the menu?
- Serve burgers, fries, drinks

**Non-Functional**: How good is the experience?
- Wait time under 5 minutes
- Open 18 hours/day
- Handle 500 customers at lunch rush
- Food quality consistent across locations

Both matter. A restaurant with great food but 2-hour waits fails. Fast food that tastes terrible also fails.

## The Art of Asking Questions

When given a system to design, NEVER jump to solutions. Start with clarifying questions:

### Scale Questions
- How many users? (1000? 1 million? 1 billion?)
- How many requests per second?
- How much data storage needed?

### Feature Questions
- What's the core feature vs nice-to-have?
- Mobile app, web, or both?
- Real-time or batch processing acceptable?

### Constraint Questions
- What's the budget?
- Team size and expertise?
- Any regulatory requirements (GDPR, HIPAA)?

[EXAMPLE]
## Worked Example: Designing a URL Shortener

**Initial request**: "Build a URL shortener like bit.ly"

**Clarifying questions and answers**:

| Question | Answer | Impact |
|----------|--------|--------|
| Expected users? | 100M monthly | Need distributed system |
| Read/write ratio? | 100:1 reads | Optimize for reads, cache heavily |
| URL length limit? | 7 characters | 62^7 = 3.5 trillion combinations |
| Custom URLs allowed? | Yes | Need collision checking |
| Analytics needed? | Click counts, geolocation | Additional storage and processing |
| URL expiration? | Optional, default never | Need TTL handling |

**Requirements Document**:

\`\`\`
FUNCTIONAL:
- Shorten URL ‚Üí returns 7-char code
- Redirect short URL ‚Üí original URL
- Custom aliases (optional)
- Analytics dashboard

NON-FUNCTIONAL:
- Latency: <100ms for redirects
- Availability: 99.9% uptime
- Scale: 1B redirects/month, 10M new URLs/month
- Durability: URLs never lost
\`\`\`

[TIP]
## The 80/20 Rule of Requirements

Focus your design on the 20% of features that deliver 80% of value. In a URL shortener:
- ‚úÖ Fast redirects (CRITICAL - 99% of traffic)
- ‚úÖ URL creation (Important - daily use)
- ‚ö†Ô∏è Analytics (Nice to have)
- ‚ö†Ô∏è Custom domains (Premium feature)

[WARNING]
## Common Requirements Mistakes

1. **Assuming scale** - "Let's design for 1 billion users" when you have 1000
2. **Gold plating** - Adding features nobody asked for
3. **Vague requirements** - "It should be fast" (How fast? For whom?)
4. **Forgetting edge cases** - What if the URL is invalid? What if storage fails?
5. **Ignoring costs** - Features have price tags

[REAL-WORLD]
## How Companies Handle Requirements

**Amazon's Working Backwards**:
1. Write the press release first
2. Write the FAQ
3. Define the user experience
4. THEN design the system

**Netflix's Requirements Process**:
- Every system has an explicit "blast radius" - what breaks if this fails?
- Non-functional requirements are first-class citizens, not afterthoughts`,

            'hld_2': `## Back-of-Envelope Calculations

[STORY]
**The YouTube Storage Crisis**

When YouTube was acquired by Google in 2006, engineers did a calculation that terrified everyone:

- 65,000 new videos uploaded daily
- Average video size: 50MB
- Daily new storage: 3.25 TB
- Yearly: 1.2 PB (petabytes)
- Plus redundancy (3x): 3.6 PB/year

At the time, this was mind-boggling. But here's the thing‚Äîtheir estimates were WRONG. They underestimated by 10x.

**Good estimation isn't about being exact. It's about being in the right ballpark so you don't build a bicycle when you need an airplane.**

## Why Back-of-Envelope Math Matters

1. **Feasibility check** - Is this even possible?
2. **Architecture decisions** - Single server or distributed?
3. **Cost estimation** - Can we afford this?
4. **Interview performance** - Shows structured thinking

## Essential Numbers to Memorize

\`\`\`
TIME:
- 1 day = 86,400 seconds ‚âà 100,000 seconds
- 1 month ‚âà 2.5 million seconds
- 1 year ‚âà 30 million seconds

STORAGE:
- 1 character = 1 byte (ASCII) or 2-4 bytes (UTF-8)
- 1 integer = 4-8 bytes
- 1 UUID = 16 bytes
- Average tweet/post = ~500 bytes
- Average image (compressed) = 200 KB
- Average video (1 min, 720p) = 50 MB

THROUGHPUT:
- HDD read: 100 MB/s
- SSD read: 500 MB/s
- Network (1 Gbps): 125 MB/s
- Memory read: 10 GB/s

LATENCY:
- Memory access: 100 ns
- SSD access: 100 Œºs
- Network (same datacenter): 500 Œºs
- Network (cross-continent): 150 ms
\`\`\`

[EXAMPLE]
## Worked Example: Twitter QPS Calculation

**Given:**
- 300 million monthly active users (MAU)
- 50% use daily ‚Üí 150M DAU
- Average user reads 100 tweets/day
- Average user writes 2 tweets/day

**Calculate Queries Per Second (QPS):**

\`\`\`
READ QPS:
- Daily reads = 150M users √ó 100 tweets = 15 billion reads/day
- QPS = 15B / 86,400 seconds ‚âà 175,000 QPS
- Peak (2x average) ‚âà 350,000 QPS

WRITE QPS:
- Daily writes = 150M √ó 2 = 300 million writes/day
- QPS = 300M / 86,400 ‚âà 3,500 QPS
- Peak ‚âà 7,000 QPS

READ:WRITE RATIO = 175,000 : 3,500 = 50:1
\`\`\`

**Implication:** Optimize heavily for reads (caching, read replicas)

## Storage Calculation Framework

\`\`\`
Total Storage = (Daily New Data) √ó (Retention Period) √ó (Replication Factor)
\`\`\`

**Example: Instagram Photo Storage**

\`\`\`
Given:
- 500M DAU
- 20% upload at least 1 photo/day = 100M photos/day
- Average photo size (after compression) = 500 KB
- Retention: Forever
- Replication: 3x

Daily storage = 100M √ó 500 KB = 50 TB/day
Yearly storage = 50 TB √ó 365 = 18.25 PB/year
With replication = 54.75 PB/year

5-year storage = ~275 PB
\`\`\`

[ANALOGY]
## Think in Powers of 10

| Users | Scale | Example |
|-------|-------|---------|
| 1K | Hobby project | Your side project |
| 10K | Small startup | Local business app |
| 100K | Growing startup | Regional service |
| 1M | Successful startup | Mid-size SaaS |
| 10M | Scale-up | Notion, Figma |
| 100M | Big tech | Twitter, Spotify |
| 1B | Mega scale | Facebook, YouTube |

Each 10x increase typically requires architectural changes.

[TIP]
## Sanity Check Your Estimates

Always validate against known systems:
- Twitter: ~500M tweets/day ‚Üí ‚úì Our 300M estimate is reasonable
- YouTube: 500 hours uploaded/minute ‚Üí Check if storage estimate is in range
- Google: 8.5 billion searches/day ‚Üí 100K QPS is achievable

**If your numbers are wildly different from reality, re-check assumptions.**

## Bandwidth Calculation

\`\`\`
Bandwidth = QPS √ó Average Request Size
\`\`\`

**Example: Video Streaming Service**

\`\`\`
Given:
- 10M concurrent viewers (peak)
- 720p video = 3 Mbps per stream

Peak bandwidth = 10M √ó 3 Mbps = 30 Pbps (petabits/second)
                = 3.75 PB/s (petabytes/second)

This is why Netflix uses CDNs everywhere!
\`\`\`

[WARNING]
## Common Estimation Mistakes

1. **Forgetting replication** - Data is stored 3x minimum for durability
2. **Ignoring peak vs average** - Systems must handle 2-10x average load
3. **Missing metadata** - Photos need timestamps, user IDs, tags...
4. **Underestimating growth** - Plan for 2-3 years ahead
5. **Off-by-1000 errors** - KB vs MB vs GB (triple-check units!)

[REAL-WORLD]
## How Google Estimates

Google's engineering interviews explicitly test estimation:
- "How much storage does Gmail need?"
- "Estimate Google Maps tile storage"
- "How many servers for Google Search?"

Their rule: **Be within 10x of reality.** That's good enough for architecture decisions.`,

            // Day 2: Scalability Basics
            'hld_3': `## Vertical vs Horizontal Scaling

[STORY]
**Netflix's $100 Million Architecture Decision**

In 2008, Netflix faced a crisis. Their monolithic DVD-rental application was running on a massive Oracle database. As streaming launched, they hit a wall:

The biggest server money could buy couldn't handle their growth.

They had two choices:
1. Buy an even bigger server (if one existed)
2. Fundamentally redesign their architecture

They chose option 2. Over the next 7 years, they migrated to AWS, broke their monolith into 1000+ microservices, and built one of the most resilient systems on the planet.

**The decision to scale horizontally instead of vertically wasn't just technical‚Äîit was existential.**

## The Two Scaling Strategies

### **{Vertical Scaling}** (Scale Up)
Add more power to your existing machine:
- More CPU cores
- More RAM
- Faster storage

\`\`\`
Before: [Server: 8 cores, 32GB RAM]
After:  [Server: 64 cores, 512GB RAM]
\`\`\`

**Pros:**
- ‚úÖ Simple - no code changes
- ‚úÖ No distributed system complexity
- ‚úÖ Strong consistency (single machine)

**Cons:**
- ‚ùå Hardware limits (can't buy a 10,000-core CPU)
- ‚ùå Single point of failure
- ‚ùå Expensive (costs grow exponentially)
- ‚ùå Downtime during upgrades

### **{Horizontal Scaling}** (Scale Out)
Add more machines to your pool:

\`\`\`
Before: [Server 1]
After:  [Server 1] [Server 2] [Server 3] ... [Server N]
\`\`\`

**Pros:**
- ‚úÖ Theoretically unlimited scale
- ‚úÖ Fault tolerant (one server dies, others continue)
- ‚úÖ Cost-effective (commodity hardware)
- ‚úÖ No downtime for scaling

**Cons:**
- ‚ùå Complex distributed systems
- ‚ùå Data consistency challenges
- ‚ùå Network overhead
- ‚ùå Harder to debug

[ANALOGY]
## The Restaurant Analogy

**Vertical Scaling**: Hire a super-chef who can cook 10x faster
- Eventually, even the best chef maxes out
- If they're sick, restaurant closes

**Horizontal Scaling**: Hire more regular chefs
- Add chefs as demand grows
- One chef sick? Others cover
- But need coordination (who cooks what?)

## When to Use Each

| Situation | Recommended | Why |
|-----------|-------------|-----|
| Startup MVP | Vertical | Simplicity matters more than scale |
| Database (reads) | Horizontal | Read replicas are straightforward |
| Database (writes) | Vertical first | Distributed writes are hard |
| Stateless services | Horizontal | Easy to add more instances |
| Legacy systems | Vertical | May not support distribution |
| Need 99.99% uptime | Horizontal | Redundancy required |

[EXAMPLE]
## Real Architecture: Scaling a Web App

**Stage 1: Single Server (Vertical)**
\`\`\`
User ‚Üí [Web + App + DB on one server]
Cost: $100/month
Handles: 1,000 users
\`\`\`

**Stage 2: Separate DB (Vertical)**
\`\`\`
User ‚Üí [Web + App Server] ‚Üí [Database Server]
Cost: $300/month
Handles: 10,000 users
\`\`\`

**Stage 3: Add Load Balancer (Horizontal)**
\`\`\`
User ‚Üí [Load Balancer] ‚Üí [App 1] ‚Üí [DB Primary]
                       ‚Üí [App 2]    ‚Üì
                       ‚Üí [App 3] ‚Üí [DB Replica]
Cost: $1,000/month
Handles: 100,000 users
\`\`\`

**Stage 4: Full Horizontal**
\`\`\`
User ‚Üí CDN ‚Üí LB ‚Üí [App Servers x10] ‚Üí Cache ‚Üí [Sharded DBs]
Cost: $10,000/month
Handles: 1,000,000+ users
\`\`\`

[TIP]
## The Right Approach

**Start vertical, go horizontal when needed.**

1. **Day 1**: Single server (simple, cheap)
2. **First scaling pain**: Bigger server (buys time)
3. **Hitting limits**: Add load balancer + multiple app servers
4. **Database pain**: Add read replicas, then consider sharding
5. **Global scale**: Multi-region, CDNs, edge computing

Don't over-engineer on day one. Instagram ran on a single server for months.

[WARNING]
## Scaling Anti-Patterns

1. **Premature horizontal scaling** - Adds complexity before it's needed
2. **Ignoring the database** - App servers scale easily; databases don't
3. **Stateful services** - Kill horizontal scaling ability
4. **Shared file systems** - Become bottlenecks
5. **Not monitoring** - Can't scale what you can't measure

[REAL-WORLD]
## How Big Companies Scale

**Google**: 
- Search index sharded across millions of machines
- No single machine could hold the index

**Amazon**:
- Every service scales independently
- "Two-pizza teams" own services end-to-end

**Netflix**:
- Stateless services scale to thousands of instances
- Database tier carefully managed with Cassandra

The pattern: **Stateless horizontal scaling for compute, careful strategy for data.**`,

            'hld_4': `## Load Balancers Deep Dive

[STORY]
**The Super Bowl Effect**

Every year, something predictable happens during Super Bowl commercials: websites crash.

In 2015, a major car company aired a commercial. Within 60 seconds, their website received 100x normal traffic. Without proper load balancing, their servers melted. Millions in ad spend wasted.

Meanwhile, companies like Amazon handle Black Friday‚Äî10x traffic spikes‚Äîwithout breaking a sweat.

**The difference? Load balancers that distribute traffic intelligently.**

## What is a Load Balancer?

A **{Load Balancer}** distributes incoming network traffic across multiple servers to ensure no single server bears too much demand.

\`\`\`
Without Load Balancer:
Users ‚Üí [Single Server] üò∞ (overwhelmed)

With Load Balancer:
Users ‚Üí [Load Balancer] ‚Üí [Server 1] üòä
                        ‚Üí [Server 2] üòä
                        ‚Üí [Server 3] üòä
\`\`\`

## Load Balancing Algorithms

### 1. **{Round Robin}**
Requests distributed sequentially to each server.

\`\`\`
Request 1 ‚Üí Server A
Request 2 ‚Üí Server B
Request 3 ‚Üí Server C
Request 4 ‚Üí Server A (cycles back)
\`\`\`

**Pros:** Simple, even distribution
**Cons:** Ignores server load, all servers must be equal

### 2. **{Weighted Round Robin}**
Like round robin, but servers have different weights.

\`\`\`
Server A (weight 3): Gets 3 requests
Server B (weight 1): Gets 1 request
Server C (weight 2): Gets 2 requests
\`\`\`

**Use when:** Servers have different capacities

### 3. **{Least Connections}**
Send to the server with fewest active connections.

\`\`\`
Server A: 10 connections ‚Üê Next request goes here
Server B: 25 connections
Server C: 18 connections
\`\`\`

**Pros:** Adapts to actual load
**Cons:** Doesn't account for connection "weight"

### 4. **{IP Hash}**
Hash the client IP to determine server.

\`\`\`
hash(192.168.1.1) % 3 = Server 1
hash(192.168.1.2) % 3 = Server 2
\`\`\`

**Pros:** Same client always hits same server (session affinity)
**Cons:** Can cause imbalance if IPs cluster

### 5. **{Least Response Time}**
Send to the server responding fastest.

**Pros:** Optimizes for user experience
**Cons:** Requires continuous monitoring

[ANALOGY]
## The Bank Teller Analogy

Imagine a bank with multiple tellers:

- **Round Robin**: Take a number, go to next available teller in sequence
- **Least Connections**: Go to teller with shortest line
- **Weighted**: Senior tellers get more customers
- **IP Hash**: Same customer always sees same teller (knows your history)

## Layer 4 vs Layer 7 Load Balancers

### **{Layer 4 (Transport)}** - TCP/UDP level
- Sees: IP addresses, ports
- Cannot see: HTTP content, URLs, cookies
- Speed: Very fast (just forwards packets)
- Use for: High-throughput, simple routing

### **{Layer 7 (Application)}** - HTTP level
- Sees: URLs, headers, cookies, content
- Can do: Route /api to backend, /images to CDN
- Speed: Slower (must parse HTTP)
- Use for: Complex routing, SSL termination

\`\`\`
Layer 7 Example:
/api/*      ‚Üí API Servers
/images/*   ‚Üí CDN
/websocket  ‚Üí WebSocket Servers
\`\`\`

[EXAMPLE]
## Real Configuration: NGINX Load Balancer

\`\`\`nginx
upstream backend {
    least_conn;  # Use least connections algorithm
    
    server 10.0.0.1:8080 weight=3;  # More powerful
    server 10.0.0.2:8080 weight=2;
    server 10.0.0.3:8080 weight=1;
    
    keepalive 32;  # Connection pooling
}

server {
    listen 80;
    
    location / {
        proxy_pass http://backend;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
}
\`\`\`

## Health Checks

Load balancers must detect failed servers:

\`\`\`
Every 5 seconds:
  GET /health ‚Üí Server 1 ‚úì
  GET /health ‚Üí Server 2 ‚úì
  GET /health ‚Üí Server 3 ‚úó (remove from pool)
\`\`\`

**Types:**
- **Active**: LB periodically pings servers
- **Passive**: LB notices failed requests

[TIP]
## Load Balancer Best Practices

1. **Always have multiple LBs** - Single LB = single point of failure
2. **Use health checks** - Don't send traffic to dead servers
3. **Enable connection draining** - Finish existing requests before removing server
4. **Monitor everything** - Request rates, latency, error rates
5. **Plan for SSL termination** - Let LB handle HTTPS

[WARNING]
## Common Mistakes

1. **Single load balancer** - What happens when IT fails?
2. **No health checks** - Sends traffic to crashed servers
3. **Sticky sessions everywhere** - Kills horizontal scaling benefits
4. **Ignoring connection limits** - LB has max connections too
5. **No monitoring** - Can't see when things go wrong

[REAL-WORLD]
## Production Load Balancers

**AWS ALB (Application Load Balancer)**
- Layer 7, content-based routing
- Native AWS integration
- Pay per request

**AWS NLB (Network Load Balancer)**
- Layer 4, extreme performance
- Millions of requests/second
- Static IPs

**NGINX / HAProxy**
- Self-managed, highly configurable
- Free (open source)
- Used by Netflix, Airbnb, GitHub

**Cloudflare**
- Global anycast network
- DDoS protection included
- Edge load balancing`,

            'hld_5': `## Stateless vs Stateful Architecture

[STORY]
**The Nightmare of Session Loss**

It's 2008. You're shopping on an e-commerce site. You've spent 30 minutes adding items to your cart. You click checkout and... your cart is empty.

What happened? The load balancer sent your checkout request to a different server than the one storing your session. That server had no idea who you were.

**This single architectural flaw cost e-commerce companies millions in abandoned carts.** The solution? Stateless architecture.

## Understanding State

**{State}** = Data that must be remembered between requests

**Stateful Server**: Remembers client data in memory
\`\`\`
Server A memory: {user123: {cart: ["item1", "item2"]}}
\`\`\`

**Stateless Server**: No client data in memory
\`\`\`
Server A memory: {} (empty - all data in external store)
\`\`\`

[ANALOGY]
## The Restaurant Waiter Analogy

**Stateful (Your Personal Waiter)**:
- Same waiter remembers your order, allergies, preferences
- If they go home sick, new waiter knows nothing about you
- Can't easily add more waiters during rush hour

**Stateless (Any Waiter)**:
- Your order is written on a ticket
- ANY waiter can serve you by reading the ticket
- Easy to add more waiters during rush hour
- Waiter goes home? No problem, ticket still exists

## Why Stateless Wins for Scale

### Stateful Problems:
\`\`\`
Request 1 ‚Üí Server A (stores session)
Request 2 ‚Üí Server B (no session! ERROR)

Solutions (all bad):
1. Sticky sessions (defeats load balancing)
2. Session replication (expensive, slow)
\`\`\`

### Stateless Solution:
\`\`\`
Request 1 ‚Üí Server A (reads session from Redis)
Request 2 ‚Üí Server B (reads same session from Redis)

Both work! Any server can handle any request.
\`\`\`

[EXAMPLE]
## Implementing Stateless Authentication

**Stateful Approach (Sessions)**:
\`\`\`
1. User logs in
2. Server creates session: {sessionId: "abc123", userId: 42}
3. Server stores in memory
4. Sends sessionId cookie to browser
5. Every request: Server looks up sessionId in memory

Problem: Different server = session not found
\`\`\`

**Stateless Approach (JWT)**:
\`\`\`
1. User logs in
2. Server creates JWT token containing: {userId: 42, exp: ...}
3. Server signs token with secret key
4. Sends token to browser
5. Every request: ANY server can verify token with secret key

No shared state needed!
\`\`\`

\`\`\`javascript
// Stateless JWT Example
const jwt = require('jsonwebtoken');

// Login - create token
app.post('/login', (req, res) => {
    const user = authenticateUser(req.body);
    const token = jwt.sign(
        { userId: user.id, role: user.role },
        process.env.JWT_SECRET,
        { expiresIn: '24h' }
    );
    res.json({ token });
});

// Any endpoint - verify token
app.get('/api/data', (req, res) => {
    const token = req.headers.authorization;
    const decoded = jwt.verify(token, process.env.JWT_SECRET);
    // decoded.userId available - no database lookup needed!
});
\`\`\`

## The State Spectrum

Not everything can be stateless. Here's where state lives:

| Component | Stateless? | Where State Lives |
|-----------|------------|-------------------|
| Web Servers | ‚úÖ Yes | Client (JWT) or Redis |
| API Servers | ‚úÖ Yes | External data stores |
| Databases | ‚ùå No | That's their job! |
| Cache | ‚ùå No | In-memory by design |
| Message Queues | ‚ùå No | Messages are state |
| WebSockets | ‚ö†Ô∏è Tricky | Connection is state |

## Making WebSockets "Stateless"

WebSockets are inherently stateful (persistent connection), but we can minimize issues:

\`\`\`
Architecture:
Client ‚Üí Load Balancer ‚Üí [WS Server 1] ‚Üí Redis Pub/Sub
                       ‚Üí [WS Server 2] ‚Üí Redis Pub/Sub

When Server 1 needs to message a user on Server 2:
1. Publish to Redis channel
2. All servers receive message
3. Server 2 delivers to connected client
\`\`\`

[TIP]
## Rules for Stateless Design

1. **Store session data externally** - Redis, database, or client
2. **Use tokens, not sessions** - JWT, OAuth tokens
3. **Design for any-server handling** - Request should work on any instance
4. **Keep servers identical** - No server-specific configuration
5. **Make requests self-contained** - Include all needed context

[WARNING]
## Stateless Isn't Always Better

Some things REQUIRE state:
- **Real-time games** - Millisecond latency needed
- **Video streaming** - Buffering state
- **Long-running processes** - Workflow state

The goal: **Minimize state, externalize what remains.**

[REAL-WORLD]
## How Companies Handle State

**Netflix**:
- Stateless streaming services
- State in Cassandra (distributed DB)
- Redis for session cache

**Uber**:
- Stateless API servers
- Driver/rider locations in real-time systems
- Trip state in databases

**Slack**:
- WebSocket connections are stateful
- Redis pub/sub for cross-server messaging
- Message history in databases`,

            // Day 3: Networking & Communication
            'hld_6': `## HTTP, REST, and API Design

[STORY]
**The API That Launched a Billion Dollar Industry**

In 2006, Amazon did something radical: they exposed their internal services as public APIs. AWS was born.

But here's the secret sauce‚Äîthey didn't invent new protocols. They used simple HTTP and REST. This decision meant any developer, anywhere, could integrate in minutes.

Today, APIs power everything: Stripe processes $640B/year through REST APIs. Twilio sends billions of messages. The modern internet runs on well-designed APIs.

## Understanding HTTP

**{HTTP}** (HyperText Transfer Protocol) is the foundation of web communication.

\`\`\`
HTTP Request:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ GET /users/123 HTTP/1.1            ‚îÇ ‚Üê Method + Path
‚îÇ Host: api.example.com              ‚îÇ ‚Üê Headers
‚îÇ Authorization: Bearer token123     ‚îÇ
‚îÇ Content-Type: application/json     ‚îÇ
‚îÇ                                    ‚îÇ
‚îÇ (optional body for POST/PUT)       ‚îÇ ‚Üê Body
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

HTTP Response:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ HTTP/1.1 200 OK                    ‚îÇ ‚Üê Status
‚îÇ Content-Type: application/json     ‚îÇ ‚Üê Headers
‚îÇ                                    ‚îÇ
‚îÇ {"id": 123, "name": "John"}        ‚îÇ ‚Üê Body
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## HTTP Methods (Verbs)

| Method | Purpose | Idempotent | Safe |
|--------|---------|------------|------|
| GET | Read data | ‚úÖ | ‚úÖ |
| POST | Create new resource | ‚ùå | ‚ùå |
| PUT | Replace entire resource | ‚úÖ | ‚ùå |
| PATCH | Partial update | ‚ùå | ‚ùå |
| DELETE | Remove resource | ‚úÖ | ‚ùå |

**{Idempotent}**: Same request multiple times = same result
**{Safe}**: Doesn't modify server state

## REST Principles

**{REST}** (Representational State Transfer) is an architectural style:

### 1. Resources Have URLs
\`\`\`
/users           ‚Üí Collection of users
/users/123       ‚Üí Specific user
/users/123/posts ‚Üí User's posts
\`\`\`

### 2. Use HTTP Methods Correctly
\`\`\`
GET    /users      ‚Üí List users
POST   /users      ‚Üí Create user
GET    /users/123  ‚Üí Get user 123
PUT    /users/123  ‚Üí Replace user 123
PATCH  /users/123  ‚Üí Update user 123
DELETE /users/123  ‚Üí Delete user 123
\`\`\`

### 3. Stateless
Each request contains all information needed. No server-side sessions.

### 4. Use Standard Status Codes
\`\`\`
2xx: Success
  200 OK - Request succeeded
  201 Created - Resource created
  204 No Content - Success, no body

4xx: Client Error
  400 Bad Request - Invalid input
  401 Unauthorized - Authentication required
  403 Forbidden - Not allowed
  404 Not Found - Resource doesn't exist
  429 Too Many Requests - Rate limited

5xx: Server Error
  500 Internal Server Error - Server bug
  502 Bad Gateway - Upstream error
  503 Service Unavailable - Overloaded
\`\`\`

[EXAMPLE]
## Designing a REST API: Blog Platform

\`\`\`
Resource: Posts

GET    /posts              ‚Üí List all posts
GET    /posts?author=123   ‚Üí Filter by author
GET    /posts/456          ‚Üí Get post 456
POST   /posts              ‚Üí Create new post
PUT    /posts/456          ‚Üí Replace post 456
PATCH  /posts/456          ‚Üí Update post 456
DELETE /posts/456          ‚Üí Delete post 456

Nested Resources:
GET    /posts/456/comments ‚Üí Comments on post 456
POST   /posts/456/comments ‚Üí Add comment to post 456
\`\`\`

**Request Example:**
\`\`\`json
POST /posts
Content-Type: application/json

{
    "title": "System Design 101",
    "content": "Let me tell you about scaling...",
    "tags": ["system-design", "tutorial"]
}
\`\`\`

**Response:**
\`\`\`json
HTTP/1.1 201 Created
Location: /posts/789

{
    "id": 789,
    "title": "System Design 101",
    "content": "Let me tell you about scaling...",
    "tags": ["system-design", "tutorial"],
    "created_at": "2024-01-15T10:30:00Z",
    "author": {
        "id": 123,
        "name": "Jane Developer"
    }
}
\`\`\`

## Pagination

**Offset-based** (simple, has issues):
\`\`\`
GET /posts?limit=20&offset=40
\`\`\`
Problem: Slow for large offsets, inconsistent if data changes

**Cursor-based** (recommended):
\`\`\`
GET /posts?limit=20&cursor=eyJpZCI6MTAwfQ
\`\`\`
The cursor encodes position, handles real-time data better

[TIP]
## API Design Best Practices

1. **Use nouns, not verbs**: \`/users\` not \`/getUsers\`
2. **Plural resource names**: \`/users\` not \`/user\`
3. **Nest logically**: \`/users/123/posts\` not \`/getUserPosts\`
4. **Version your API**: \`/v1/users\` or \`Accept: application/vnd.api+json;version=1\`
5. **Use query params for filtering**: \`/posts?status=published&sort=-created_at\`
6. **Return useful errors**:
\`\`\`json
{
    "error": {
        "code": "VALIDATION_ERROR",
        "message": "Title is required",
        "field": "title"
    }
}
\`\`\`

[WARNING]
## Common API Design Mistakes

1. **Verbs in URLs**: \`/createUser\` instead of \`POST /users\`
2. **Returning 200 for errors**: Always use proper status codes
3. **Inconsistent naming**: \`/users\` vs \`/blog_posts\` vs \`/Comments\`
4. **No pagination**: Returning 10,000 items crashes clients
5. **Breaking changes without versioning**: Angers all your users

[REAL-WORLD]
## APIs Worth Studying

**Stripe** - Gold standard for REST design:
- Excellent error messages
- Consistent patterns
- Great documentation

**GitHub** - REST + GraphQL:
- Well-designed resources
- Comprehensive webhooks
- Rate limiting done right

**Twilio** - Developer-focused:
- Clear resource hierarchy
- Helpful error messages
- SDK in every language`,

            'hld_7': `## WebSockets and Real-Time Communication

[STORY]
**The Birth of Real-Time Web**

In 2010, Facebook engineers faced a problem: users wanted to see new messages instantly, but HTTP wasn't designed for this.

Their solution? Poll the server every few seconds.

The result? 1 million users √ó polling every 3 seconds = 333,000 requests/second... for mostly empty responses. Servers were melting to tell users "no new messages."

**WebSockets changed everything.** One persistent connection, instant messages, 100x less server load.

## The Problem with HTTP

HTTP is **request-response**: client asks, server answers, connection closes.

\`\`\`
Polling (wasteful):
Client: "Any messages?" ‚Üí Server: "No"
Client: "Any messages?" ‚Üí Server: "No"
Client: "Any messages?" ‚Üí Server: "No"
Client: "Any messages?" ‚Üí Server: "Yes! Here's 1 message"

(3 wasted round trips for 1 message)
\`\`\`

## Communication Models Compared

### 1. **Short Polling**
\`\`\`
Every 5 seconds: GET /messages ‚Üí Response
\`\`\`
- ‚ùå Wasteful (empty responses)
- ‚ùå Delayed (up to polling interval)
- ‚úÖ Simple to implement

### 2. **Long Polling**
\`\`\`
GET /messages ‚Üí Server holds connection until data available ‚Üí Response
(Immediately reconnect)
\`\`\`
- ‚úÖ Near real-time
- ‚ö†Ô∏è Resource intensive (connections held)
- ‚úÖ Works through firewalls

### 3. **Server-Sent Events (SSE)**
\`\`\`
GET /events ‚Üí Server pushes updates over one connection
\`\`\`
- ‚úÖ Real-time server ‚Üí client
- ‚ùå One-way only (server to client)
- ‚úÖ Auto-reconnection built-in
- ‚úÖ Simple HTTP (works everywhere)

### 4. **{WebSockets}**
\`\`\`
Upgrade to WebSocket ‚Üí Bidirectional messages
\`\`\`
- ‚úÖ True bidirectional
- ‚úÖ Low latency
- ‚úÖ Efficient (no HTTP overhead per message)
- ‚ö†Ô∏è More complex to scale

## WebSocket Deep Dive

### The Handshake
\`\`\`
Client Request:
GET /chat HTTP/1.1
Host: server.example.com
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==

Server Response:
HTTP/1.1 101 Switching Protocols
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=

(Now speaking WebSocket protocol)
\`\`\`

### Message Flow
\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Persistent Connection     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Client ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  ‚îÇ Server ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                                         ‚îÇ
    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ "Hello" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ
    ‚îÇ                                         ‚îÇ
    ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ "Hi there!" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
    ‚îÇ                                         ‚îÇ
    ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ "New message from Bob" ‚îÄ‚îÇ
    ‚îÇ                                         ‚îÇ
    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ "typing..." ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ
\`\`\`

[EXAMPLE]
## Implementing WebSocket Chat

**Server (Node.js):**
\`\`\`javascript
const WebSocket = require('ws');
const wss = new WebSocket.Server({ port: 8080 });

const clients = new Set();

wss.on('connection', (ws) => {
    clients.add(ws);
    
    ws.on('message', (message) => {
        // Broadcast to all connected clients
        clients.forEach(client => {
            if (client.readyState === WebSocket.OPEN) {
                client.send(message);
            }
        });
    });
    
    ws.on('close', () => {
        clients.delete(ws);
    });
});
\`\`\`

**Client (JavaScript):**
\`\`\`javascript
const ws = new WebSocket('ws://server.example.com:8080');

ws.onopen = () => {
    console.log('Connected!');
    ws.send(JSON.stringify({ type: 'join', room: 'general' }));
};

ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    displayMessage(data);
};

ws.onclose = () => {
    console.log('Disconnected, reconnecting...');
    setTimeout(connectWebSocket, 1000); // Auto-reconnect
};
\`\`\`

## Scaling WebSockets

Single server is limited. How to scale?

### Challenge: Cross-Server Communication
\`\`\`
User A connected to Server 1
User B connected to Server 2
User A sends message to User B... how?
\`\`\`

### Solution: Pub/Sub Backbone (Redis)
\`\`\`
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Redis PubSub  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üë             ‚Üë
            Subscribe/Publish   Subscribe/Publish
                      ‚îÇ             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ             ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Server 1    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   Server 2    ‚îÇ
‚îÇ (User A here) ‚îÇ                         ‚îÇ (User B here) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Message flow:
1. User A sends message via WS to Server 1
2. Server 1 publishes to Redis channel
3. Server 2 (subscribed) receives message
4. Server 2 sends to User B via WS
\`\`\`

[TIP]
## WebSocket Best Practices

1. **Implement heartbeats** - Detect dead connections
\`\`\`javascript
setInterval(() => {
    ws.send(JSON.stringify({ type: 'ping' }));
}, 30000);
\`\`\`

2. **Handle reconnection** - Connections drop, plan for it
3. **Use message queuing** - Don't lose messages during reconnect
4. **Compress large messages** - Use permessage-deflate extension
5. **Authenticate on connect** - Verify tokens during handshake

[WARNING]
## WebSocket Pitfalls

1. **Memory leaks** - Not cleaning up closed connections
2. **No reconnection logic** - Connection drops, user stuck
3. **Blocking operations** - One slow message blocks all
4. **No message validation** - Trust issues with client data
5. **Horizontal scaling ignored** - Works on one server, fails on many

[REAL-WORLD]
## How Companies Handle Real-Time

**Slack**:
- WebSockets for messages
- Redis pub/sub for cross-server
- Fallback to long-polling

**Discord**:
- Custom WebSocket gateway
- Voice uses UDP (not WS)
- Handles millions of concurrent connections

**Figma**:
- Custom CRDT for real-time collaboration
- WebSockets for cursor positions
- Operational transforms for document sync`,

            'hld_8': `## RPC and gRPC

[STORY]
**The Microservices Communication Crisis**

When Uber decomposed their monolith into 2,000+ microservices, they faced a problem: services were spending more time serializing JSON and parsing HTTP than doing actual work.

A simple "get user profile" call:
- Serialize request to JSON: 2ms
- HTTP overhead: 5ms
- Deserialize response: 3ms
- Actual database lookup: 1ms

**90% overhead!**

They switched to gRPC. Same call: 1.5ms total. 10x faster.

## What is RPC?

**{RPC}** (Remote Procedure Call) makes calling a function on another server feel like calling a local function.

\`\`\`
Without RPC (manual HTTP):
const response = await fetch('http://user-service/users/123');
const user = await response.json();

With RPC:
const user = await userService.getUser(123);
// Feels like a local function call!
\`\`\`

## REST vs RPC Philosophy

| Aspect | REST | RPC |
|--------|------|-----|
| Focus | Resources (nouns) | Actions (verbs) |
| URL style | /users/123 | /GetUser |
| Flexibility | High (any client) | Lower (needs client lib) |
| Performance | HTTP + JSON overhead | Binary, fast |
| Type safety | None (runtime) | Strong (compile time) |
| Best for | Public APIs | Internal services |

## Enter gRPC

**{gRPC}** is Google's modern RPC framework:
- Binary protocol (Protocol Buffers)
- HTTP/2 (multiplexing, streaming)
- Strongly typed
- Code generation for 10+ languages

### The Stack
\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Your Application Code        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Generated Stubs             ‚îÇ ‚Üê From .proto files
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ            gRPC Core                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ             HTTP/2                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ              TCP                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[EXAMPLE]
## Defining a gRPC Service

**user.proto:**
\`\`\`protobuf
syntax = "proto3";

package user;

// Service definition
service UserService {
    // Unary RPC
    rpc GetUser (GetUserRequest) returns (User);
    
    // Server streaming
    rpc ListUsers (ListUsersRequest) returns (stream User);
    
    // Client streaming  
    rpc UploadAvatar (stream AvatarChunk) returns (UploadResponse);
    
    // Bidirectional streaming
    rpc Chat (stream ChatMessage) returns (stream ChatMessage);
}

// Message definitions
message GetUserRequest {
    int64 user_id = 1;
}

message User {
    int64 id = 1;
    string name = 2;
    string email = 3;
    repeated string roles = 4;
}
\`\`\`

**Generated Python Client:**
\`\`\`python
# Auto-generated from .proto file
import grpc
import user_pb2
import user_pb2_grpc

channel = grpc.insecure_channel('localhost:50051')
stub = user_pb2_grpc.UserServiceStub(channel)

# Call feels like local function!
response = stub.GetUser(user_pb2.GetUserRequest(user_id=123))
print(f"User: {response.name}, Email: {response.email}")
\`\`\`

**Server Implementation:**
\`\`\`python
class UserServicer(user_pb2_grpc.UserServiceServicer):
    def GetUser(self, request, context):
        user = database.get_user(request.user_id)
        return user_pb2.User(
            id=user.id,
            name=user.name,
            email=user.email
        )

server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
user_pb2_grpc.add_UserServiceServicer_to_server(UserServicer(), server)
server.add_insecure_port('[::]:50051')
server.start()
\`\`\`

## gRPC Communication Patterns

### 1. Unary (Request-Response)
\`\`\`
Client ‚îÄ‚îÄ‚îÄ Request ‚îÄ‚îÄ‚îÄ‚Üí Server
       ‚Üê‚îÄ‚îÄ Response ‚îÄ‚îÄ‚îÄ
\`\`\`
Most common, like REST.

### 2. Server Streaming
\`\`\`
Client ‚îÄ‚îÄ‚îÄ Request ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Server
       ‚Üê‚îÄ‚îÄ Response 1 ‚îÄ‚îÄ
       ‚Üê‚îÄ‚îÄ Response 2 ‚îÄ‚îÄ
       ‚Üê‚îÄ‚îÄ Response N ‚îÄ‚îÄ
\`\`\`
Great for: Large result sets, real-time updates

### 3. Client Streaming
\`\`\`
Client ‚îÄ‚îÄ‚îÄ Request 1 ‚îÄ‚îÄ‚îÄ‚Üí Server
       ‚îÄ‚îÄ‚îÄ Request 2 ‚îÄ‚îÄ‚îÄ‚Üí
       ‚îÄ‚îÄ‚îÄ Request N ‚îÄ‚îÄ‚îÄ‚Üí
       ‚Üê‚îÄ‚îÄ Response ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
\`\`\`
Great for: File uploads, aggregations

### 4. Bidirectional Streaming
\`\`\`
Client ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Server
       Messages flow both ways
\`\`\`
Great for: Chat, collaborative editing

## Why Protocol Buffers?

\`\`\`
JSON (readable, verbose):
{"userId": 123, "name": "John", "email": "john@example.com"}
Size: 62 bytes

Protocol Buffers (binary, compact):
[binary data]
Size: 28 bytes (55% smaller)

Parse time: 10x faster than JSON
\`\`\`

[TIP]
## When to Use gRPC

**Use gRPC for:**
- Internal microservice communication
- High-performance requirements
- Strongly-typed contracts needed
- Streaming data (bidirectional)
- Polyglot environments (many languages)

**Use REST for:**
- Public APIs (browser compatibility)
- Simple integrations
- When human readability matters
- When you need flexibility over performance

[WARNING]
## gRPC Challenges

1. **Browser support** - Native gRPC doesn't work in browsers (need gRPC-Web proxy)
2. **Debugging** - Binary protocol is hard to inspect
3. **Load balancing** - HTTP/2 needs connection-aware LBs
4. **Breaking changes** - Proto changes can break clients
5. **Learning curve** - Protocol Buffers syntax takes time

[REAL-WORLD]
## Who Uses gRPC?

**Netflix**: Internal service communication
**Uber**: 2000+ services communicate via gRPC
**Square**: Payment processing
**Dropbox**: File sync protocol
**CoreOS/Kubernetes**: etcd uses gRPC

The pattern: **REST for public APIs, gRPC for internal services.**`,

            // ==================== DAY 4: Databases & Storage I ====================
            'hld_9': `## SQL vs NoSQL Databases

[STORY]
## The Great Database Debate

In 2009, a heated debate erupted in tech circles. NoSQL advocates declared "SQL is dead!" pointing to Facebook's Cassandra, Google's BigTable, and Amazon's DynamoDB. They argued that relational databases couldn't scale to handle billions of users.

A decade later? **Both SQL and NoSQL thrive.** Google created Spanner‚Äîa globally distributed SQL database. Amazon offers both DynamoDB (NoSQL) and Aurora (SQL). The "war" was a false dichotomy.

The truth: **databases are tools, not religions.** Choose based on your specific requirements.

## Understanding the Fundamental Differences

### SQL (Relational) Databases

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SQL DATABASE                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ   Users     ‚îÇ    ‚îÇ   Orders    ‚îÇ                 ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                 ‚îÇ
‚îÇ  ‚îÇ id (PK)     ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ user_id(FK) ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ name        ‚îÇ    ‚îÇ product_id  ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ email       ‚îÇ    ‚îÇ amount      ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ  ‚úì Strict schema (columns defined upfront)          ‚îÇ
‚îÇ  ‚úì ACID transactions (Atomicity, Consistency,       ‚îÇ
‚îÇ    Isolation, Durability)                           ‚îÇ
‚îÇ  ‚úì Powerful JOINs across tables                     ‚îÇ
‚îÇ  ‚úì Complex queries with SQL                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

**Examples:** PostgreSQL, MySQL, Oracle, SQL Server

### NoSQL (Non-Relational) Databases

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   NoSQL DATABASE                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                      ‚îÇ
‚îÇ  Document Store (MongoDB):                          ‚îÇ
‚îÇ  {                                                  ‚îÇ
‚îÇ    "_id": "user123",                                ‚îÇ
‚îÇ    "name": "John",                                  ‚îÇ
‚îÇ    "orders": [                                      ‚îÇ
‚îÇ      {"product": "laptop", "amount": 999},          ‚îÇ
‚îÇ      {"product": "mouse", "amount": 29}             ‚îÇ
‚îÇ    ]                                                ‚îÇ
‚îÇ  }                                                  ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ  ‚úì Flexible schema (add fields anytime)             ‚îÇ
‚îÇ  ‚úì Horizontal scaling (shard across servers)        ‚îÇ
‚îÇ  ‚úì Denormalized data (nested documents)             ‚îÇ
‚îÇ  ‚úì Eventually consistent (trade-off for speed)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

**Types of NoSQL:**
- **Document:** MongoDB, CouchDB
- **Key-Value:** Redis, DynamoDB
- **Column-Family:** Cassandra, HBase
- **Graph:** Neo4j, Amazon Neptune

## The Trade-Off Matrix

| Feature | SQL | NoSQL |
|---------|-----|-------|
| **Schema** | Rigid, predefined | Flexible, dynamic |
| **Scaling** | Vertical (bigger server) | Horizontal (more servers) |
| **Consistency** | Strong (ACID) | Eventual (BASE) |
| **Queries** | Complex JOINs | Limited, denormalized |
| **Transactions** | Multi-row | Usually single-document |
| **Use Case** | Complex relationships | High-volume, simple access |

## Decision Framework

\`\`\`
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ What are your needs?‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº               ‚ñº               ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Complex ‚îÇ     ‚îÇ Massive ‚îÇ     ‚îÇ Flexible‚îÇ
        ‚îÇ Queries ‚îÇ     ‚îÇ Scale   ‚îÇ     ‚îÇ Schema  ‚îÇ
        ‚îÇ & JOINs ‚îÇ     ‚îÇ Needed  ‚îÇ     ‚îÇ Needed  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ               ‚îÇ               ‚îÇ
             ‚ñº               ‚ñº               ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   SQL   ‚îÇ     ‚îÇ NoSQL   ‚îÇ     ‚îÇ NoSQL   ‚îÇ
        ‚îÇ (RDBMS) ‚îÇ     ‚îÇ         ‚îÇ     ‚îÇ         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[EXAMPLE]
## Real-World Decision Examples

### E-Commerce Platform
**Choice: SQL (PostgreSQL)**
- Complex product catalogs with categories
- Order transactions require ACID
- Inventory levels must be consistent
- Reports need complex JOINs

### Social Media Feed
**Choice: NoSQL (Cassandra)**
- Massive write throughput (millions of posts/day)
- Simple access pattern (get posts by user_id)
- Eventual consistency acceptable
- Horizontal scaling essential

### Gaming Leaderboard
**Choice: NoSQL (Redis)**
- Real-time updates needed
- Simple key-value with sorted sets
- Speed critical, persistence optional
- Read-heavy workload

### Banking System
**Choice: SQL (Oracle/PostgreSQL)**
- Transactions MUST be ACID
- Complex reporting required
- Audit trails essential
- Consistency over availability

[TIP]
## Practical Advice

1. **Start with SQL** unless you have specific NoSQL requirements
2. **Consider hybrid** - Many companies use both (SQL for transactions, NoSQL for caching/analytics)
3. **Understand your access patterns** - NoSQL excels when patterns are predictable
4. **Don't follow hype** - Choose based on requirements, not trends

[WARNING]
## Common Mistakes

1. **Choosing NoSQL for "scale"** when you have 1000 users
2. **Using NoSQL then needing JOINs** - painful workarounds
3. **Ignoring operational complexity** of distributed NoSQL
4. **Premature optimization** - SQL handles most workloads fine

[REAL-WORLD]
## How Tech Giants Choose

**Google:** Spanner (SQL for global consistency), Bigtable (NoSQL for analytics)
**Amazon:** Aurora (SQL for e-commerce), DynamoDB (NoSQL for cart/session)
**Netflix:** Cassandra (NoSQL for viewing history), MySQL (SQL for billing)
**Uber:** PostgreSQL (SQL for trips), Redis (NoSQL for real-time)

The pattern: **Use both strategically based on workload characteristics.**`,

            'hld_10': `## Database Sharding Strategies

[STORY]
## Twitter's Growing Pains

In 2010, Twitter was drowning. Their single MySQL database couldn't handle the load. Queries that once took milliseconds now took seconds. The "Fail Whale" became their unofficial mascot.

The solution? **Sharding** - splitting their data across multiple independent databases. But this wasn't just a technical change; it fundamentally altered how they built features.

> "Sharding is easy to describe but hard to implement correctly." - Twitter Engineering

## What is Sharding?

**{Sharding}** is horizontal partitioning of data across multiple database instances.

\`\`\`
Before Sharding:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Single Database             ‚îÇ
‚îÇ  Users: 100 million                 ‚îÇ
‚îÇ  Queries: 50,000/sec                ‚îÇ
‚îÇ  Status: OVERWHELMED üî•             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

After Sharding:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Shard 1  ‚îÇ ‚îÇ  Shard 2  ‚îÇ ‚îÇ  Shard 3  ‚îÇ ‚îÇ  Shard 4  ‚îÇ
‚îÇ Users A-F ‚îÇ ‚îÇ Users G-L ‚îÇ ‚îÇ Users M-R ‚îÇ ‚îÇ Users S-Z ‚îÇ
‚îÇ  25M each ‚îÇ ‚îÇ  25M each ‚îÇ ‚îÇ  25M each ‚îÇ ‚îÇ  25M each ‚îÇ
‚îÇ 12.5K QPS ‚îÇ ‚îÇ 12.5K QPS ‚îÇ ‚îÇ 12.5K QPS ‚îÇ ‚îÇ 12.5K QPS ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚úì Each shard manageable!
\`\`\`

## Sharding Strategies

### 1. Hash-Based Sharding

\`\`\`
shard_id = hash(user_id) % number_of_shards

User 12345 ‚Üí hash(12345) = 7823 ‚Üí 7823 % 4 = 3 ‚Üí Shard 3
User 67890 ‚Üí hash(67890) = 2156 ‚Üí 2156 % 4 = 0 ‚Üí Shard 0
\`\`\`

**Pros:**
- Even distribution (no hot spots)
- Simple to implement

**Cons:**
- Adding shards requires rehashing (data migration)
- Range queries span all shards

### 2. Range-Based Sharding

\`\`\`
Shard 1: user_id 1 - 1,000,000
Shard 2: user_id 1,000,001 - 2,000,000
Shard 3: user_id 2,000,001 - 3,000,000
\`\`\`

**Pros:**
- Range queries efficient
- Easy to add new shards (just extend range)

**Cons:**
- Uneven distribution (new users all go to last shard)
- Hot spots if access patterns favor certain ranges

### 3. Directory-Based Sharding

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Lookup Service           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ user_id ‚Üí shard_id    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 12345   ‚Üí shard_3     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 67890   ‚Üí shard_1     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 11111   ‚Üí shard_2     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

**Pros:**
- Maximum flexibility
- Easy rebalancing

**Cons:**
- Lookup service is single point of failure
- Extra network hop for every query

## Choosing a Shard Key

The **{shard key}** determines how data is distributed. This is the most critical decision.

\`\`\`
Good Shard Key Properties:
‚úì High cardinality (many unique values)
‚úì Even distribution
‚úì Matches query patterns
‚úì Immutable (doesn't change)

Example: E-commerce
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Shard by user_id:                                   ‚îÇ
‚îÇ ‚úì Orders for a user on same shard                   ‚îÇ
‚îÇ ‚úì User dashboard queries are fast                   ‚îÇ
‚îÇ ‚úó "All orders today" spans all shards               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Shard by order_date:                                ‚îÇ
‚îÇ ‚úì Daily reports are fast                            ‚îÇ
‚îÇ ‚úó User order history spans multiple shards          ‚îÇ
‚îÇ ‚úó Today's shard is HOT (all new orders)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[EXAMPLE]
## Instagram's Sharding Strategy

Instagram shards by **user_id** using PostgreSQL:

\`\`\`
Photo ID structure:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  41 bits: timestamp (ms since custom epoch)     ‚îÇ
‚îÇ  13 bits: shard ID (8192 possible shards)       ‚îÇ
‚îÇ  10 bits: sequence number (1024 per ms)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
- IDs are sortable by time
- Shard is embedded in ID (no lookup needed)
- Can generate IDs without coordination
\`\`\`

## Cross-Shard Operations

The hardest part of sharding is operations that span multiple shards.

\`\`\`
Query: "Find all orders over $100 in the last week"

Without Sharding:
SELECT * FROM orders WHERE amount > 100 AND date > '2024-01-01'
‚Üí Single query, fast

With Sharding (4 shards):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Shard 1 ‚îÇ ‚îÇ Shard 2 ‚îÇ ‚îÇ Shard 3 ‚îÇ ‚îÇ Shard 4 ‚îÇ
‚îÇ  Query  ‚îÇ ‚îÇ  Query  ‚îÇ ‚îÇ  Query  ‚îÇ ‚îÇ  Query  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ           ‚îÇ           ‚îÇ           ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   Combine   ‚îÇ
              ‚îÇ   Results   ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚Üí 4 queries + merge, slower
\`\`\`

**Solutions:**
1. **Denormalize** - Copy data to avoid cross-shard joins
2. **Application-level joins** - Query multiple shards, merge in app
3. **Specialized query layer** - Tools like Vitess, CockroachDB

[TIP]
## Sharding Best Practices

1. **Delay sharding** until truly necessary (it adds massive complexity)
2. **Choose shard key carefully** - changing it later is painful
3. **Plan for rebalancing** - data will become uneven over time
4. **Monitor shard sizes** - detect imbalance early
5. **Test cross-shard queries** - they will be slow

[WARNING]
## Sharding Anti-Patterns

1. **Hot shards** - One shard getting all traffic (celebrity problem)
2. **Cross-shard transactions** - Extremely complex, avoid if possible
3. **Poor shard key** - Leads to constant rebalancing
4. **Too many shards initially** - Start small, split as needed
5. **Ignoring locality** - Related data should be on same shard

[REAL-WORLD]
## How Companies Shard

**Facebook:** Shards by user_id, uses TAO for graph data
**Uber:** Shards by city/region for geographic locality
**Pinterest:** Shards by user_id, uses MySQL
**Slack:** Shards by workspace_id (natural isolation)

The pattern: **Shard by the entity that's most commonly queried together.**`,

            'hld_11': `## Database Replication Patterns

[STORY]
## The Night GitHub Went Down

On October 21, 2018, GitHub experienced a 24-hour outage. A routine maintenance operation caused a network partition. Their primary database in the US East Coast couldn't reach replicas. When connectivity resumed, the databases had diverged‚Äîsome had newer data than others.

The incident taught the industry: **replication isn't just about copies‚Äîit's about consistency, failover, and understanding what can go wrong.**

## Why Replication?

\`\`\`
Single Database:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Database     ‚îÇ ‚Üê Single point of failure!
‚îÇ   (Primary)     ‚îÇ   If it dies, everything dies.
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

With Replication:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Primary      ‚îÇ‚îÄ‚îÄ‚îÄWrite‚îÄ‚îÄ‚îÄ‚Üí ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   (Leader)      ‚îÇ             ‚îÇ    Replica 1    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ   (Follower)    ‚îÇ
         ‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄWrite‚îÄ‚îÄ‚îÄ‚Üí ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ    Replica 2    ‚îÇ
                          ‚îÇ   (Follower)    ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
‚úì High availability (replica takes over if primary fails)
‚úì Read scalability (distribute reads across replicas)
‚úì Geographic distribution (replicas near users)
‚úì Backup (replicas can serve as hot backups)
\`\`\`

## Replication Patterns

### 1. Leader-Follower (Master-Slave)

\`\`\`
                    Writes
                      ‚îÇ
                      ‚ñº
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ   Leader    ‚îÇ
               ‚îÇ  (Primary)  ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ          ‚îÇ          ‚îÇ
           ‚ñº          ‚ñº          ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ Follower ‚îÇ ‚îÇ Follower ‚îÇ ‚îÇ Follower ‚îÇ
     ‚îÇ    1     ‚îÇ ‚îÇ    2     ‚îÇ ‚îÇ    3     ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚ñ≤          ‚ñ≤          ‚ñ≤
           ‚îÇ          ‚îÇ          ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   Reads
\`\`\`

**How it works:**
- All writes go to leader
- Leader replicates changes to followers
- Reads can go to any node

**Use cases:** Most applications, read-heavy workloads

### 2. Leader-Leader (Multi-Master)

\`\`\`
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Leader 1  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   Leader 2  ‚îÇ
        ‚îÇ  (Primary)  ‚îÇ     ‚îÇ  (Primary)  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                   ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº           ‚ñº       ‚ñº           ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇFollower‚îÇ ‚îÇFollower‚îÇ ‚îÇFollower‚îÇ ‚îÇFollower‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   
   Both leaders accept writes!
   Conflict resolution needed.
\`\`\`

**Challenges:**
- Write conflicts (same row updated on both)
- Conflict resolution strategies needed
- More complex to operate

**Use cases:** Multi-datacenter writes, high availability requirements

### 3. Leaderless (Peer-to-Peer)

\`\`\`
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Node 1 ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
             ‚îÇ            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
        ‚îÇ  Node 2 ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚ñ∫ All nodes are equal
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ     Any node accepts writes
             ‚îÇ            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
        ‚îÇ  Node 3 ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        
   Quorum: W + R > N
   (Write nodes + Read nodes > Total nodes)
\`\`\`

**Examples:** Cassandra, DynamoDB

**Use cases:** High availability, partition tolerance

## Synchronous vs Asynchronous Replication

\`\`\`
Synchronous:
Client ‚îÄ‚îÄWrite‚îÄ‚îÄ‚ñ∫ Leader ‚îÄ‚îÄReplicate‚îÄ‚îÄ‚ñ∫ Follower
                    ‚îÇ                      ‚îÇ
                    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄAcknowledge‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
Client ‚óÑ‚îÄ‚îÄACK‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚úì Guaranteed consistency
‚úó Higher latency (wait for replica)
‚úó Availability reduced (if replica slow/down)


Asynchronous:
Client ‚îÄ‚îÄWrite‚îÄ‚îÄ‚ñ∫ Leader ‚îÄ‚îÄACK‚îÄ‚îÄ‚ñ∫ Client
                    ‚îÇ
                    ‚îî‚îÄ‚îÄReplicate‚îÄ‚îÄ‚ñ∫ Follower (later)

‚úì Lower latency
‚úì Higher availability
‚úó Replication lag (reads may see stale data)
‚úó Data loss if leader fails before replication
\`\`\`

## Replication Lag

\`\`\`
Time: 0ms      100ms     200ms     300ms
      ‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ
Leader: WRITE‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
              ‚îÇ         ‚îÇ         ‚îÇ
Replica:      ‚îÇ    RECEIVE‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄAPPLY‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
              ‚îÇ         ‚îÇ         ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  Replication Lag
                  (200-300ms)

During lag, reads from replica return STALE data!
\`\`\`

**Handling replication lag:**
1. **Read-your-writes:** Route user's reads to leader after their writes
2. **Monotonic reads:** Always read from same replica
3. **Consistent prefix:** Ensure causal ordering preserved

[EXAMPLE]
## Failover Process

\`\`\`
Normal Operation:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Leader  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Replica ‚îÇ
‚îÇ  (OK)   ‚îÇ     ‚îÇ  (OK)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Leader Fails:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Leader  ‚îÇ  X  ‚îÇ Replica ‚îÇ
‚îÇ (DEAD)  ‚îÇ     ‚îÇ  (OK)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Failover:
1. Detect failure (health checks)
2. Elect new leader (consensus)
3. Promote replica to leader
4. Redirect traffic
5. Update DNS/config

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  (old)  ‚îÇ     ‚îÇ  NEW    ‚îÇ
‚îÇ         ‚îÇ     ‚îÇ LEADER  ‚îÇ‚óÑ‚îÄ‚îÄ Traffic
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

**Failover challenges:**
- **Split brain:** Two nodes think they're leader
- **Data loss:** Unreplicated writes on failed leader
- **Client reconnection:** Updating connection strings

[TIP]
## Replication Best Practices

1. **Monitor replication lag** - Alert when it exceeds threshold
2. **Test failover regularly** - Don't wait for real failures
3. **Use semi-synchronous** - At least one replica must acknowledge
4. **Consider read replicas** - Scale reads without affecting writes
5. **Plan for network partitions** - They will happen

[WARNING]
## Common Replication Pitfalls

1. **Assuming instant replication** - There's always lag
2. **Ignoring split-brain scenarios** - Can cause data corruption
3. **Manual failover in production** - Automate it
4. **Not testing replica promotion** - Find issues before crisis
5. **Underestimating network issues** - Partitions are common

[REAL-WORLD]
## How Companies Handle Replication

**Netflix:** Cassandra with 3 replicas, asynchronous, eventual consistency
**GitHub:** MySQL with ProxySQL, semi-synchronous replication
**Stripe:** PostgreSQL with synchronous replication for financial data
**LinkedIn:** Espresso with multi-datacenter replication

The pattern: **Choose consistency level based on data criticality.**`,

            // ==================== DAY 5: Databases & Storage II ====================
            'hld_12': `## Indexing and Query Optimization

[STORY]
## The Query That Took 10 Hours

A developer at a startup ran a simple query: "Find all users who signed up last month." On their test database with 1,000 users, it took 50ms. In production with 50 million users, it took **10 hours** and nearly crashed the system.

The difference? A missing **index**. With the right index, the same query took **15 milliseconds**‚Äîa 2.4 million times improvement.

> "The difference between a good database and a great one is usually just good indexing." - Database wisdom

## How Indexes Work

Without an index, the database must scan every row (**full table scan**):

\`\`\`
Query: SELECT * FROM users WHERE email = 'john@example.com'

Without Index (Full Table Scan):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Row 1:  alice@... ‚Üí Not a match                 ‚îÇ
‚îÇ Row 2:  bob@...   ‚Üí Not a match                 ‚îÇ
‚îÇ Row 3:  carol@... ‚Üí Not a match                 ‚îÇ
‚îÇ ...                                             ‚îÇ
‚îÇ Row 999,999: john@example.com ‚Üí MATCH!          ‚îÇ
‚îÇ Row 1,000,000: zara@... ‚Üí Still checking...     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Time: O(n) = 1,000,000 row comparisons

With B-Tree Index on email:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    [j-p]                        ‚îÇ
‚îÇ                   /      \\                      ‚îÇ
‚îÇ              [a-i]        [q-z]                 ‚îÇ
‚îÇ             /    \\       /    \\                 ‚îÇ
‚îÇ          [a-d] [e-i]  [q-t]  [u-z]              ‚îÇ
‚îÇ                  ‚îÇ                              ‚îÇ
‚îÇ            john@example.com ‚Üí Row 999,999       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Time: O(log n) = ~20 comparisons for 1M rows!
\`\`\`

## Index Types

### 1. B-Tree Index (Most Common)

\`\`\`
Structure:
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   [M]         ‚îÇ  Root
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚ñº            ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ[A-L]  ‚îÇ   ‚îÇ[M-R]  ‚îÇ   ‚îÇ[S-Z]  ‚îÇ  Internal
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ           ‚îÇ           ‚îÇ
    ‚ñº           ‚ñº           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇAlice  ‚îÇ   ‚îÇMike   ‚îÇ   ‚îÇSteve  ‚îÇ  Leaf (data)
‚îÇBob    ‚îÇ   ‚îÇNancy  ‚îÇ   ‚îÇTom    ‚îÇ
‚îÇCarol  ‚îÇ   ‚îÇOscar  ‚îÇ   ‚îÇUma    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Best for:
‚úì Equality queries (WHERE x = value)
‚úì Range queries (WHERE x > value)
‚úì Sorting (ORDER BY x)
‚úì Most general-purpose use
\`\`\`

### 2. Hash Index

\`\`\`
hash('john@example.com') ‚Üí bucket 7823

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇBucket 0 ‚îÇBucket 1 ‚îÇ  ...    ‚îÇBucket N ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ email1  ‚îÇ email2  ‚îÇ         ‚îÇ email99 ‚îÇ
‚îÇ ‚Üí row   ‚îÇ ‚Üí row   ‚îÇ         ‚îÇ ‚Üí row   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Best for:
‚úì Exact equality (WHERE x = value)
‚úó NOT for ranges (WHERE x > value)
‚úó NOT for sorting
\`\`\`

### 3. Full-Text Index

\`\`\`
Document: "The quick brown fox jumps over the lazy dog"

Inverted Index:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Word   ‚îÇ  Document IDs    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  brown  ‚îÇ  [1, 45, 892]    ‚îÇ
‚îÇ  fox    ‚îÇ  [1, 23, 445]    ‚îÇ
‚îÇ  quick  ‚îÇ  [1, 67, 234]    ‚îÇ
‚îÇ  lazy   ‚îÇ  [1, 89]         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Best for:
‚úì Text search (LIKE '%keyword%')
‚úì Natural language queries
‚úì Relevance scoring
\`\`\`

## Composite (Multi-Column) Indexes

\`\`\`
CREATE INDEX idx_user_status_date 
ON orders(user_id, status, created_at);

Index Order Matters!
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Index: (user_id, status, created_at)           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚úì WHERE user_id = 1                            ‚îÇ
‚îÇ ‚úì WHERE user_id = 1 AND status = 'active'      ‚îÇ
‚îÇ ‚úì WHERE user_id = 1 AND status = 'active'      ‚îÇ
‚îÇ         AND created_at > '2024-01-01'          ‚îÇ
‚îÇ                                                ‚îÇ
‚îÇ ‚úó WHERE status = 'active' (user_id not used!)  ‚îÇ
‚îÇ ‚úó WHERE created_at > '2024-01-01'              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Rule: Index used left-to-right. Gaps break it!
\`\`\`

## Query Execution Plans

\`\`\`sql
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'john@example.com';

-- Without index:
Seq Scan on users  (cost=0.00..18584.00 rows=1 width=72)
  Filter: (email = 'john@example.com'::text)
  Rows Removed by Filter: 999999
  Actual Time: 2847.234ms

-- With index:
Index Scan using idx_users_email on users
  (cost=0.42..8.44 rows=1 width=72)
  Index Cond: (email = 'john@example.com'::text)
  Actual Time: 0.052ms
\`\`\`

**Key terms:**
- **Seq Scan:** Full table scan (bad for large tables)
- **Index Scan:** Using index (good!)
- **cost:** Estimated work (lower is better)
- **rows:** Estimated rows returned

[EXAMPLE]
## Indexing Strategy for E-Commerce

\`\`\`sql
-- Products table
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255),
    category_id INT,
    price DECIMAL(10,2),
    created_at TIMESTAMP,
    is_active BOOLEAN
);

-- Common queries and their indexes:

-- 1. Product search by category
-- Query: SELECT * FROM products WHERE category_id = 5 AND is_active = true
CREATE INDEX idx_category_active ON products(category_id, is_active);

-- 2. Price range search
-- Query: SELECT * FROM products WHERE price BETWEEN 100 AND 500
CREATE INDEX idx_price ON products(price);

-- 3. Recent products
-- Query: SELECT * FROM products ORDER BY created_at DESC LIMIT 20
CREATE INDEX idx_created_desc ON products(created_at DESC);

-- 4. Full-text search
-- Query: SELECT * FROM products WHERE name ILIKE '%wireless%'
CREATE INDEX idx_name_gin ON products USING gin(to_tsvector('english', name));
\`\`\`

[TIP]
## Indexing Best Practices

1. **Index columns in WHERE clauses** - Most impactful
2. **Index columns in JOIN conditions** - Speeds up joins
3. **Consider column order** - Most selective first
4. **Index for your queries** - Not for every column
5. **Monitor index usage** - Remove unused indexes

\`\`\`sql
-- Find unused indexes (PostgreSQL)
SELECT indexrelname, idx_scan
FROM pg_stat_user_indexes
WHERE idx_scan = 0;
\`\`\`

[WARNING]
## Indexing Anti-Patterns

1. **Over-indexing** - Every index slows writes
2. **Indexing low-cardinality columns** - boolean has only 2 values
3. **Ignoring index maintenance** - Fragmentation hurts performance
4. **Wrong composite order** - Leftmost column must be in query
5. **Indexing frequently updated columns** - Constant index rebuilds

\`\`\`
Write overhead per index:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 0 indexes:  INSERT = 1 write            ‚îÇ
‚îÇ 1 index:    INSERT = 2 writes           ‚îÇ
‚îÇ 5 indexes:  INSERT = 6 writes           ‚îÇ
‚îÇ 10 indexes: INSERT = 11 writes (SLOW!)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[REAL-WORLD]
## How Companies Optimize

**Stack Overflow:** 1 developer, careful indexing handles 1B+ page views/month
**Shopify:** Automated index recommendations based on query patterns
**GitHub:** Code search uses specialized inverted indexes
**Uber:** Time-based partitioning + indexing for trip data

The pattern: **Profile queries first, index strategically, monitor constantly.**`,

            'hld_13': `## Object Storage and Blob Storage

[STORY]
## Instagram's 50 Billion Photos

When Instagram was acquired by Facebook in 2012, they had 50 million users uploading 5 million photos daily. Today, they store over **50 billion photos**. Storing these in a traditional database would be impossible‚Äîand absurdly expensive.

The solution? **Object storage**‚Äîa simple, infinitely scalable system designed for unstructured data like images, videos, and files.

> "If you're storing binary data in a relational database, you're doing it wrong." - Cloud architect wisdom

## What is Object Storage?

\`\`\`
Traditional File System:              Object Storage:
                                      
/home/                                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  /user/                              ‚îÇ   Flat Namespace        ‚îÇ
    /documents/                       ‚îÇ                         ‚îÇ
      /reports/                       ‚îÇ bucket/object-key-1     ‚îÇ
        file.pdf                      ‚îÇ bucket/object-key-2     ‚îÇ
                                      ‚îÇ bucket/object-key-3     ‚îÇ
Hierarchical paths                    ‚îÇ bucket/subfolder/key-4  ‚îÇ
                                      ‚îÇ                         ‚îÇ
                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      Key-value with metadata
\`\`\`

**Key characteristics:**
- **Flat namespace:** Objects accessed by unique key (no directories)
- **Unlimited scale:** Petabytes, exabytes, no practical limit
- **Simple API:** PUT, GET, DELETE (no update‚Äîreplace entire object)
- **Built-in redundancy:** Automatically replicated across data centers
- **Cheap storage:** Pennies per GB per month

## Object Storage Architecture

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     APPLICATION                          ‚îÇ
‚îÇ                          ‚îÇ                               ‚îÇ
‚îÇ            PUT image.jpg ‚îÇ                               ‚îÇ
‚îÇ                          ‚ñº                               ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ              ‚îÇ  Object Storage   ‚îÇ                       ‚îÇ
‚îÇ              ‚îÇ      (S3)         ‚îÇ                       ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                        ‚îÇ                                 ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ     ‚ñº                  ‚ñº                  ‚ñº              ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ ‚îÇ Copy 1 ‚îÇ        ‚îÇ Copy 2 ‚îÇ        ‚îÇ Copy 3 ‚îÇ          ‚îÇ
‚îÇ ‚îÇ  DC 1  ‚îÇ        ‚îÇ  DC 2  ‚îÇ        ‚îÇ  DC 3  ‚îÇ          ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ 11 9s durability = 99.999999999%                        ‚îÇ
‚îÇ Lose 1 object in 100 billion over 10,000 years!         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Object Storage Operations

\`\`\`python
# AWS S3 Example

import boto3

s3 = boto3.client('s3')

# Upload (PUT)
s3.put_object(
    Bucket='my-bucket',
    Key='images/photo123.jpg',
    Body=open('photo.jpg', 'rb'),
    ContentType='image/jpeg',
    Metadata={'user-id': '12345', 'upload-date': '2024-01-15'}
)

# Download (GET)
response = s3.get_object(Bucket='my-bucket', Key='images/photo123.jpg')
image_data = response['Body'].read()

# Delete
s3.delete_object(Bucket='my-bucket', Key='images/photo123.jpg')

# List objects
response = s3.list_objects_v2(Bucket='my-bucket', Prefix='images/')
for obj in response['Contents']:
    print(obj['Key'], obj['Size'])
\`\`\`

## Storage Classes and Lifecycle

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   STORAGE TIERS                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  HOT (Standard)                                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Frequent access                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Highest cost (~$0.023/GB/month)                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Instant retrieval                                  ‚îÇ
‚îÇ           ‚îÇ                                             ‚îÇ
‚îÇ           ‚îÇ After 30 days                               ‚îÇ
‚îÇ           ‚ñº                                             ‚îÇ
‚îÇ  WARM (Infrequent Access)                               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Occasional access                                  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Lower cost (~$0.0125/GB/month)                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Retrieval fee applies                              ‚îÇ
‚îÇ           ‚îÇ                                             ‚îÇ
‚îÇ           ‚îÇ After 90 days                               ‚îÇ
‚îÇ           ‚ñº                                             ‚îÇ
‚îÇ  COLD (Glacier)                                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Rare access (archives)                             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Lowest cost (~$0.004/GB/month)                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Minutes to hours retrieval                         ‚îÇ
‚îÇ           ‚îÇ                                             ‚îÇ
‚îÇ           ‚îÇ After 365 days                              ‚îÇ
‚îÇ           ‚ñº                                             ‚îÇ
‚îÇ  DELETE (Expiration)                                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Cost savings: 80-90% by moving old data to cold storage!
\`\`\`

## CDN Integration

\`\`\`
Without CDN:
User (Tokyo) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ S3 (Virginia)
               ~200ms latency           Origin

With CDN:
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
User (Tokyo) ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ CloudFront Edge ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∫ S3 (Virginia)
  ~20ms            ‚îÇ    (Tokyo)      ‚îÇ       Origin
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         Cache
                         
First request: Edge fetches from origin, caches
Subsequent:    Edge serves directly (~10x faster!)
\`\`\`

[EXAMPLE]
## Image Upload Architecture

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  IMAGE UPLOAD FLOW                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  1. Client requests upload URL                          ‚îÇ
‚îÇ     Client ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ API Server                           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  2. Server generates presigned URL (secure, temporary)  ‚îÇ
‚îÇ     API Server ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ S3 (generate presigned URL)      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  3. Client uploads directly to S3 (bypasses server!)    ‚îÇ
‚îÇ     Client ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ S3 (PUT with presigned URL)          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  4. S3 triggers Lambda for processing                   ‚îÇ
‚îÇ     S3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Lambda (resize, thumbnails)              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  5. Serve via CDN                                       ‚îÇ
‚îÇ     Client ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CloudFront ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ S3                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
‚úì Server doesn't handle large files (saves bandwidth)
‚úì Direct upload scales infinitely
‚úì Async processing doesn't block user
‚úì CDN ensures fast delivery globally
\`\`\`

[TIP]
## Object Storage Best Practices

1. **Use presigned URLs** - Don't proxy large files through your server
2. **Set appropriate lifecycle policies** - Move old data to cheaper tiers
3. **Enable versioning** - Protect against accidental deletes
4. **Use CDN for read-heavy workloads** - Reduces latency and costs
5. **Organize keys logically** - Use prefixes for efficient listing

\`\`\`
Good key structure:
images/2024/01/15/user123-photo456.jpg
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
 type    date        unique identifier

Bad key structure:
photo456.jpg (no organization, hard to manage)
\`\`\`

[WARNING]
## Common Mistakes

1. **Storing in database** - BLOBs in SQL kill performance
2. **Not using CDN** - Every request hits origin (expensive, slow)
3. **Ignoring lifecycle** - Paying hot storage prices for cold data
4. **Public buckets** - Security nightmare (data breaches)
5. **No backup strategy** - Even with 11 9s, have a plan

[REAL-WORLD]
## How Companies Use Object Storage

**Netflix:** All video content on S3, served via their own CDN
**Dropbox:** Started on S3, built Magic Pocket (own object storage)
**Airbnb:** All listing photos on S3 with CloudFront CDN
**Spotify:** Audio files on Google Cloud Storage with global CDN

The pattern: **Object storage + CDN = scalable media delivery.**`,

            'hld_14': `## Time-Series Databases

[STORY]
## Uber's Metrics Explosion

In 2016, Uber's monitoring system was drowning. They were collecting **500 million metrics per second**‚ÄîCPU usage, request latencies, trip counts, driver locations. Their existing database couldn't keep up.

The solution? A specialized **time-series database** (TSDB) called M3, designed specifically for metrics that change over time.

> "Regular databases treat time as just another column. Time-series databases treat time as the fundamental organizing principle." - TSDB Engineer

## What is Time-Series Data?

\`\`\`
Time-series data: Values that change over time

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Timestamp           ‚îÇ Metric    ‚îÇ Value ‚îÇ Tags      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 2024-01-15 10:00:00  ‚îÇ cpu.usage ‚îÇ 45.2  ‚îÇ host=web1 ‚îÇ
‚îÇ 2024-01-15 10:00:01  ‚îÇ cpu.usage ‚îÇ 47.8  ‚îÇ host=web1 ‚îÇ
‚îÇ 2024-01-15 10:00:02  ‚îÇ cpu.usage ‚îÇ 46.1  ‚îÇ host=web1 ‚îÇ
‚îÇ 2024-01-15 10:00:03  ‚îÇ cpu.usage ‚îÇ 52.3  ‚îÇ host=web1 ‚îÇ
‚îÇ 2024-01-15 10:00:04  ‚îÇ cpu.usage ‚îÇ 48.9  ‚îÇ host=web1 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Characteristics:
‚úì Append-only (immutable history)
‚úì Time-ordered (always increasing)
‚úì High write volume (millions/second)
‚úì Time-based queries (last hour, last day)
‚úì Aggregations (averages, sums over time)
\`\`\`

## Why Regular Databases Struggle

\`\`\`
Traditional RDBMS with time-series:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                      ‚îÇ
‚îÇ  Problem 1: Write Volume                            ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                              ‚îÇ
‚îÇ  1M metrics √ó 1 write/sec = 1M writes/sec           ‚îÇ
‚îÇ  Regular DB: ~10K writes/sec max                    ‚îÇ
‚îÇ  Gap: 100x!                                         ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ  Problem 2: Storage                                 ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                               ‚îÇ
‚îÇ  1M metrics √ó 86,400 sec/day √ó 8 bytes = 690 GB/day ‚îÇ
‚îÇ  After 1 year: 252 TB (just for one metric!)        ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ  Problem 3: Query Patterns                          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚îÇ
‚îÇ  "Average CPU over last hour" requires:             ‚îÇ
‚îÇ  - Regular DB: Scan 3,600 rows, compute average     ‚îÇ
‚îÇ  - TSDB: Pre-computed rollup, instant answer        ‚îÇ
‚îÇ                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Time-Series Database Optimizations

### 1. Time-Based Compression

\`\`\`
Raw data:
Timestamp: 1705312800, 1705312801, 1705312802, 1705312803
Values:    45.2, 47.8, 46.1, 52.3

Compressed (delta-of-delta):
Base timestamp: 1705312800
Deltas: +1, +1, +1 (just 2 bits each!)

Base value: 45.2
Deltas: +2.6, -1.7, +6.2 (fewer bits than full floats)

Result: 10-15x compression ratio!
\`\`\`

### 2. Downsampling (Rollups)

\`\`\`
Raw data (1-second resolution):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 10:00:00 ‚îÇ 45.2  ‚îÇ
‚îÇ 10:00:01 ‚îÇ 47.8  ‚îÇ  Kept for 7 days
‚îÇ 10:00:02 ‚îÇ 46.1  ‚îÇ  (high resolution)
‚îÇ ...      ‚îÇ ...   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1-minute rollup:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 10:00    ‚îÇ min   ‚îÇ max   ‚îÇ avg   ‚îÇ
‚îÇ          ‚îÇ 42.1  ‚îÇ 58.3  ‚îÇ 47.2  ‚îÇ  Kept for 30 days
‚îÇ 10:01    ‚îÇ 44.5  ‚îÇ 55.1  ‚îÇ 48.9  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1-hour rollup:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 10:00    ‚îÇ 38.2  ‚îÇ 72.1  ‚îÇ 51.4  ‚îÇ  Kept for 1 year
‚îÇ 11:00    ‚îÇ 41.5  ‚îÇ 68.9  ‚îÇ 49.2  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Storage savings: 95%+ for old data!
\`\`\`

### 3. Time-Based Partitioning

\`\`\`
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Query Router   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                    ‚îÇ                    ‚îÇ
        ‚ñº                    ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  January 2024 ‚îÇ   ‚îÇ February 2024 ‚îÇ   ‚îÇ  March 2024   ‚îÇ
‚îÇ   Partition   ‚îÇ   ‚îÇ   Partition   ‚îÇ   ‚îÇ   Partition   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
‚úì Queries only hit relevant partitions
‚úì Easy to delete old data (drop partition)
‚úì Parallel queries across partitions
\`\`\`

## Common Query Patterns

\`\`\`sql
-- InfluxDB/PromQL style queries

-- Average CPU over last hour
SELECT mean("cpu_usage") 
FROM "metrics" 
WHERE time > now() - 1h 
GROUP BY time(1m), "host"

-- Rate of requests per second
rate(http_requests_total[5m])

-- 95th percentile latency
histogram_quantile(0.95, 
  rate(http_request_duration_bucket[5m]))

-- Anomaly detection (value > 2 std devs from mean)
SELECT * FROM metrics 
WHERE value > mean + 2 * stddev
\`\`\`

[EXAMPLE]
## Monitoring Architecture

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                MONITORING STACK                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ  App 1  ‚îÇ ‚îÇ  App 2  ‚îÇ ‚îÇ  App 3  ‚îÇ ‚îÇ  App N  ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ       ‚îÇ           ‚îÇ           ‚îÇ           ‚îÇ              ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                       ‚îÇ                                  ‚îÇ
‚îÇ                       ‚ñº                                  ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ              ‚îÇ   Prometheus    ‚îÇ ‚óÑ‚îÄ‚îÄ Scrapes metrics     ‚îÇ
‚îÇ              ‚îÇ   (TSDB)        ‚îÇ     every 15 seconds    ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                       ‚îÇ                                  ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ         ‚ñº             ‚ñº             ‚ñº                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  Grafana  ‚îÇ ‚îÇ Alerting  ‚îÇ ‚îÇ Long-term ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ Dashboard ‚îÇ ‚îÇ  Rules    ‚îÇ ‚îÇ  Storage  ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Popular Time-Series Databases

| Database | Best For | Write Speed | Query Language |
|----------|----------|-------------|----------------|
| **Prometheus** | Kubernetes monitoring | 100K/s | PromQL |
| **InfluxDB** | IoT, general metrics | 1M/s | InfluxQL/Flux |
| **TimescaleDB** | SQL familiarity | 500K/s | SQL |
| **ClickHouse** | Analytics, logs | 1M+/s | SQL |
| **Cassandra** | Massive scale | 1M+/s | CQL |

[TIP]
## Time-Series Best Practices

1. **Define retention policies upfront** - Don't store raw data forever
2. **Use appropriate precision** - Milliseconds often unnecessary
3. **Tag wisely** - High-cardinality tags (user_id) kill performance
4. **Pre-aggregate common queries** - Recording rules save query time
5. **Monitor your monitoring** - TSDBs need monitoring too!

[WARNING]
## Common Pitfalls

1. **High cardinality** - Too many unique tag values
   \`\`\`
   Bad: metric{user_id="..."} (millions of users)
   Good: metric{region="us-east"} (few regions)
   \`\`\`

2. **Not downsampling** - Keeping raw data forever
3. **Wrong granularity** - 1ms resolution when 1min is enough
4. **Ignoring retention** - Storage costs grow quickly
5. **Complex queries on raw data** - Use pre-computed rollups

[REAL-WORLD]
## How Companies Handle Metrics

**Uber:** M3 (custom TSDB), 500M metrics/second
**Netflix:** Atlas, 2B metrics/minute, 2.5PB storage
**Cloudflare:** ClickHouse for analytics, 6M+ queries/second
**Datadog:** Custom TSDB, trillions of data points/day

The pattern: **Specialized TSDBs + aggressive downsampling + retention policies.**`,

            // ==================== DAY 6: Caching Strategies ====================
            'hld_15': `## Caching Fundamentals and Layers

[STORY]
## The 100x Speedup

In 2010, Facebook faced a crisis. Every page load required 500+ database queries. As they grew to 500 million users, their database couldn't keep up. Response times climbed to 5+ seconds.

Their solution? **Memcached**‚Äîan in-memory cache. After implementation, response times dropped from 5 seconds to **50 milliseconds**. A 100x improvement that saved the company.

> "There are only two hard things in Computer Science: cache invalidation and naming things." - Phil Karlton

## The Cache Hierarchy

\`\`\`
Speed ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Size

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  CPU Cache (L1/L2/L3)  ‚îÇ ~1 ns    ‚îÇ  KB-MB   ‚îÇ Fastest  ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  RAM                    ‚îÇ ~100 ns  ‚îÇ  GB      ‚îÇ          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  Application Cache      ‚îÇ ~1 ms    ‚îÇ  GB-TB   ‚îÇ          ‚îÇ
‚îÇ  (Redis/Memcached)                                      ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  CDN Cache              ‚îÇ ~50 ms   ‚îÇ  TB-PB   ‚îÇ          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  Database               ‚îÇ ~10-100ms‚îÇ  TB-PB   ‚îÇ Slowest  ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  Disk                   ‚îÇ ~10 ms   ‚îÇ  PB+     ‚îÇ          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Cache Layers in Web Architecture

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     USER REQUEST                         ‚îÇ
‚îÇ                          ‚îÇ                               ‚îÇ
‚îÇ                          ‚ñº                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Layer 1: Browser Cache                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - HTTP cache headers (Cache-Control, ETag)       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - localStorage/sessionStorage                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Service Worker cache                           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                          ‚îÇ Miss                          ‚îÇ
‚îÇ                          ‚ñº                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Layer 2: CDN Cache                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Edge locations worldwide                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Static assets, API responses                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Geographic proximity = low latency             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                          ‚îÇ Miss                          ‚îÇ
‚îÇ                          ‚ñº                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Layer 3: Application Cache (Redis/Memcached)     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Session data                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Frequently accessed database results           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Computed/aggregated data                       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                          ‚îÇ Miss                          ‚îÇ
‚îÇ                          ‚ñº                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Layer 4: Database                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Query cache                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Buffer pool (in-memory pages)                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Materialized views                             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Caching Patterns

### 1. Cache-Aside (Lazy Loading)

\`\`\`
Application checks cache first, loads from DB on miss:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  Cache  ‚îÇ     ‚îÇ   DB    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ  1. GET key   ‚îÇ               ‚îÇ
     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ               ‚îÇ
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ  2. Miss!     ‚îÇ               ‚îÇ
     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ               ‚îÇ
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ  3. Query DB                  ‚îÇ
     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ  4. DB Result                 ‚îÇ
     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ  5. SET key   ‚îÇ               ‚îÇ
     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ               ‚îÇ
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ  6. Return to user            ‚îÇ
     ‚ñº               ‚ñº               ‚ñº
\`\`\`

**Pros:** Only requested data cached, cache failures don't break app
**Cons:** Cache miss = 3 trips (cache, DB, cache again), stale data possible

### 2. Read-Through

\`\`\`
Cache is responsible for loading data:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ     Cache       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   DB    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ (with loader)   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

App only talks to cache. Cache loads from DB on miss.
\`\`\`

**Pros:** Simpler application code
**Cons:** Cache becomes critical dependency

### 3. Write-Through

\`\`\`
Write to cache AND database synchronously:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  App    ‚îÇ     ‚îÇ  Cache  ‚îÇ     ‚îÇ   DB    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ  1. Write     ‚îÇ               ‚îÇ
     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ               ‚îÇ
     ‚îÇ               ‚îÇ  2. Write     ‚îÇ
     ‚îÇ               ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ               ‚îÇ  3. ACK       ‚îÇ
     ‚îÇ               ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
     ‚îÇ  4. ACK       ‚îÇ               ‚îÇ
     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ               ‚îÇ
     ‚ñº               ‚ñº               ‚ñº
\`\`\`

**Pros:** Cache always consistent with DB
**Cons:** Higher write latency (two writes)

### 4. Write-Behind (Write-Back)

\`\`\`
Write to cache, async write to database:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  App    ‚îÇ     ‚îÇ  Cache  ‚îÇ     ‚îÇ   DB    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ  1. Write     ‚îÇ               ‚îÇ
     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ               ‚îÇ
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚îÇ  2. ACK       ‚îÇ               ‚îÇ
     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ               ‚îÇ
     ‚îÇ               ‚îÇ  3. Async     ‚îÇ
     ‚îÇ               ‚îÇ  ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ‚ñ∫‚îÇ
     ‚ñº               ‚ñº               ‚ñº
\`\`\`

**Pros:** Very fast writes, batching possible
**Cons:** Data loss risk if cache fails before DB write

## Cache Invalidation Strategies

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              INVALIDATION STRATEGIES                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  1. TTL (Time-To-Live)                                  ‚îÇ
‚îÇ     cache.set("user:123", data, ttl=300)  # 5 minutes   ‚îÇ
‚îÇ     ‚úì Simple, automatic expiration                      ‚îÇ
‚îÇ     ‚úó Stale during TTL, fresh after expire              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  2. Event-Based Invalidation                            ‚îÇ
‚îÇ     on_user_update(user_id):                            ‚îÇ
‚îÇ         cache.delete(f"user:{user_id}")                 ‚îÇ
‚îÇ     ‚úì Immediate freshness                               ‚îÇ
‚îÇ     ‚úó Must track all cache keys                         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  3. Version-Based                                       ‚îÇ
‚îÇ     key = f"user:{user_id}:v{version}"                  ‚îÇ
‚îÇ     ‚úì Old versions auto-expire                          ‚îÇ
‚îÇ     ‚úó More complex key management                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Caching Best Practices

1. **Cache expensive computations** - DB queries, API calls, heavy processing
2. **Set appropriate TTLs** - Balance freshness vs hit rate
3. **Use cache stampede prevention** - Lock or probabilistic expiry
4. **Monitor hit rates** - Below 80% suggests issues
5. **Plan for cache failure** - App should work (slower) without cache

[WARNING]
## Common Caching Mistakes

1. **Caching too little** - Miss the big wins
2. **Caching too much** - Staleness, memory waste
3. **Ignoring invalidation** - Serving stale data
4. **Cache stampede** - Many requests hit DB simultaneously on expiry
5. **Not monitoring** - Can't improve what you don't measure

[REAL-WORLD]
## How Companies Cache

**Facebook:** Memcached clusters, 75% of requests served from cache
**Twitter:** Redis for timelines, 5000+ cache servers
**Instagram:** Memcached for user sessions, Django cache framework
**Pinterest:** Redis + Memcached, 99% cache hit rate

The pattern: **Multiple cache layers, aggressive caching, careful invalidation.**`,

            'hld_16': `## Distributed Caching with Redis/Memcached

[STORY]
## Twitter's Timeline Challenge

In 2012, Twitter faced a massive challenge: delivering personalized timelines to 200 million users. Each user follows hundreds of accounts, and timelines needed to load in under 200ms.

Their solution? **Redis**‚Äîused to store pre-computed timelines. When a tweet is posted, it's fanned out to followers' cached timelines. When you open Twitter, your timeline is ready instantly.

> "Redis isn't just a cache‚Äîit's a data structure server that happens to be incredibly fast." - Twitter Engineering

## Redis vs Memcached

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    COMPARISON                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ       MEMCACHED       ‚îÇ           REDIS                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Simple key-value      ‚îÇ Rich data structures            ‚îÇ
‚îÇ Multi-threaded        ‚îÇ Single-threaded (fast anyway!)  ‚îÇ
‚îÇ No persistence        ‚îÇ Optional persistence (RDB/AOF)  ‚îÇ
‚îÇ No replication        ‚îÇ Built-in replication            ‚îÇ
‚îÇ Volatile only         ‚îÇ Pub/Sub, Lua scripting          ‚îÇ
‚îÇ Slightly faster       ‚îÇ More features                   ‚îÇ
‚îÇ                       ‚îÇ                                 ‚îÇ
‚îÇ Best for:             ‚îÇ Best for:                       ‚îÇ
‚îÇ - Pure caching        ‚îÇ - Complex data structures       ‚îÇ
‚îÇ - Maximum simplicity  ‚îÇ - Persistence needed            ‚îÇ
‚îÇ - Memory efficiency   ‚îÇ - Pub/Sub messaging             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Redis Data Structures

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  REDIS DATA STRUCTURES                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  STRINGS                                                ‚îÇ
‚îÇ  SET user:123 "John Doe"                                ‚îÇ
‚îÇ  GET user:123  ‚Üí "John Doe"                             ‚îÇ
‚îÇ  Use: Caching, counters, session tokens                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  HASHES (Objects)                                       ‚îÇ
‚îÇ  HSET user:123 name "John" email "john@example.com"     ‚îÇ
‚îÇ  HGET user:123 name  ‚Üí "John"                           ‚îÇ
‚îÇ  Use: Storing objects, user profiles                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  LISTS (Ordered)                                        ‚îÇ
‚îÇ  LPUSH notifications:123 "New message"                  ‚îÇ
‚îÇ  LRANGE notifications:123 0 9  ‚Üí Last 10 notifications  ‚îÇ
‚îÇ  Use: Activity feeds, queues, recent items              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  SETS (Unique)                                          ‚îÇ
‚îÇ  SADD followers:123 456 789 101                         ‚îÇ
‚îÇ  SISMEMBER followers:123 456  ‚Üí true                    ‚îÇ
‚îÇ  Use: Tags, unique visitors, social connections         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  SORTED SETS (Ranked)                                   ‚îÇ
‚îÇ  ZADD leaderboard 100 "player1" 85 "player2"            ‚îÇ
‚îÇ  ZREVRANGE leaderboard 0 9  ‚Üí Top 10 players            ‚îÇ
‚îÇ  Use: Leaderboards, priority queues, rate limiting      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Distributed Redis Architecture

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  REDIS CLUSTER                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ   Data is sharded across multiple nodes using           ‚îÇ
‚îÇ   hash slots (16,384 total slots)                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ   Key "user:123" ‚Üí hash ‚Üí slot 5432 ‚Üí Node 2            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ  Node 1   ‚îÇ  ‚îÇ  Node 2   ‚îÇ  ‚îÇ  Node 3   ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ Slots     ‚îÇ  ‚îÇ Slots     ‚îÇ  ‚îÇ Slots     ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ 0-5460    ‚îÇ  ‚îÇ 5461-10922‚îÇ  ‚îÇ 10923-16383‚îÇ         ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ         ‚îÇ              ‚îÇ              ‚îÇ                 ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ Replica 1 ‚îÇ  ‚îÇ Replica 2 ‚îÇ  ‚îÇ Replica 3 ‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ   If Node 2 fails, Replica 2 is promoted automatically  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Common Use Cases

### 1. Session Storage

\`\`\`python
# Store session
redis.setex(f"session:{session_id}", 3600, json.dumps({
    "user_id": 123,
    "logged_in_at": "2024-01-15T10:00:00Z",
    "cart": ["item1", "item2"]
}))

# Retrieve session
session = json.loads(redis.get(f"session:{session_id}"))
\`\`\`

### 2. Rate Limiting

\`\`\`python
def is_rate_limited(user_id, limit=100, window=60):
    key = f"rate:{user_id}"
    current = redis.incr(key)
    
    if current == 1:
        redis.expire(key, window)  # Set expiry on first request
    
    return current > limit

# Usage
if is_rate_limited(user_id):
    return "Too many requests", 429
\`\`\`

### 3. Leaderboard

\`\`\`python
# Add/update score
redis.zadd("game:leaderboard", {"player123": 1500})

# Get top 10
top_players = redis.zrevrange("game:leaderboard", 0, 9, withscores=True)
# [("player456", 2000), ("player123", 1500), ...]

# Get player's rank
rank = redis.zrevrank("game:leaderboard", "player123")  # 0-indexed
\`\`\`

### 4. Pub/Sub Messaging

\`\`\`python
# Publisher
redis.publish("notifications", json.dumps({
    "type": "new_message",
    "user_id": 123,
    "message": "Hello!"
}))

# Subscriber
pubsub = redis.pubsub()
pubsub.subscribe("notifications")

for message in pubsub.listen():
    if message["type"] == "message":
        data = json.loads(message["data"])
        process_notification(data)
\`\`\`

## Persistence Options

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              REDIS PERSISTENCE                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  RDB (Snapshotting)                                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Point-in-time snapshots                            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ save 900 1      # Save if 1 key changed in 900s    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Compact file, fast restarts                        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ May lose recent data on crash                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  AOF (Append Only File)                                 ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Logs every write operation                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ appendfsync always/everysec/no                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ More durable, but larger files                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Slower restart (replays all operations)            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Hybrid (RDB + AOF)                                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Best of both worlds                                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Recommended for production                         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Redis Best Practices

1. **Use appropriate data structures** - Don't just use strings for everything
2. **Set memory limits** - maxmemory + eviction policy
3. **Use pipelining** - Batch commands to reduce round trips
4. **Monitor memory usage** - INFO memory, MEMORY USAGE key
5. **Use connection pooling** - Don't create new connections per request

[WARNING]
## Common Redis Mistakes

1. **Storing large values** - Keep values under 100KB
2. **Too many keys** - Use hashes for related data
3. **Not setting TTLs** - Memory fills up
4. **Blocking operations** - KEYS * in production (use SCAN)
5. **Single instance in production** - Use replication/cluster

[REAL-WORLD]
## How Companies Use Redis

**GitHub:** Session storage, job queues, rate limiting
**Twitter:** Timeline caching, 5000+ Redis instances
**Pinterest:** Object caching, 99%+ hit rate
**Airbnb:** Session storage, ML feature serving

The pattern: **Redis for complex use cases, Memcached for pure caching simplicity.**`,

            'hld_17': `## Cache Eviction Policies and Strategies

[STORY]
## The Memory Wall

A startup's Redis instance was at 100% memory. New data couldn't be cached, response times spiked, and the application ground to a halt. The team panicked and threw money at bigger instances.

The real problem? They had **no eviction policy**. Old, unused data was hogging memory while frequently accessed data was being evicted. A proper eviction strategy would have saved them 70% of their cache costs.

> "It's not about how much you cache‚Äîit's about caching the right things." - Cache wisdom

## Why Eviction Matters

\`\`\`
Cache Memory: 10 GB
Data to Cache: 100 GB

Reality: You can only cache 10% of your data.
Question: Which 10%?

Bad eviction: Cache fills with old, unused data
Good eviction: Cache holds most-accessed data

Result:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Bad Eviction Policy                                    ‚îÇ
‚îÇ  Cache hit rate: 40%                                    ‚îÇ
‚îÇ  Database load: High (60% of requests hit DB)           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Good Eviction Policy                                   ‚îÇ
‚îÇ  Cache hit rate: 95%                                    ‚îÇ
‚îÇ  Database load: Low (5% of requests hit DB)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Eviction Policies Explained

### 1. LRU (Least Recently Used)

\`\`\`
Most common policy. Evicts items not accessed for longest time.

Access order: A, B, C, D, E (cache size = 3)

Step 1: Cache [A]           Access A
Step 2: Cache [A, B]        Access B
Step 3: Cache [A, B, C]     Access C (full!)
Step 4: Cache [B, C, D]     Access D ‚Üí Evict A (least recent)
Step 5: Cache [B, D, E]     Access E ‚Üí Evict C (least recent)
Step 6: Cache [D, E, B]     Access B ‚Üí B moves to front

How it works:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  D  ‚îÇ  E  ‚îÇ  B  ‚îÇ  ‚Üê Most recently used
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚ñ≤
  Least recently used (evicted next)
\`\`\`

**Best for:** General-purpose caching, temporal locality

### 2. LFU (Least Frequently Used)

\`\`\`
Evicts items with lowest access count.

Access pattern: A(10x), B(2x), C(5x), D(1x) - cache size = 3

Cache: [A:10, C:5, B:2]
New item E arrives ‚Üí Evict B (lowest frequency)
Cache: [A:10, C:5, E:1]

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Item  ‚îÇ  Frequency  ‚îÇ  Decision                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   A    ‚îÇ     10      ‚îÇ  Keep (high frequency)           ‚îÇ
‚îÇ   B    ‚îÇ      2      ‚îÇ  EVICT (lowest frequency)        ‚îÇ
‚îÇ   C    ‚îÇ      5      ‚îÇ  Keep                            ‚îÇ
‚îÇ   D    ‚îÇ      1      ‚îÇ  Already evicted                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

**Best for:** Skewed access patterns (some items accessed much more)

### 3. FIFO (First In, First Out)

\`\`\`
Evicts oldest item regardless of access.

Insert order: A, B, C, D (cache size = 3)

Cache: [A, B, C]
Insert D ‚Üí Evict A (first in)
Cache: [B, C, D]

Simple but ignores access patterns!
\`\`\`

**Best for:** Simple scenarios, streaming data

### 4. TTL (Time-To-Live)

\`\`\`
Items expire after set time.

SET user:123 "data" EX 3600  # Expires in 1 hour

Timeline:
0:00  - Item cached
0:30  - Still valid
0:59  - Still valid
1:00  - EXPIRED (removed)
1:01  - Cache miss ‚Üí Reload from DB
\`\`\`

**Best for:** Data that becomes stale over time

### 5. Random

\`\`\`
Randomly evicts items. Surprisingly effective!

Why? In high-throughput scenarios:
- LRU tracking has overhead
- Random is O(1)
- Statistically similar hit rates

Used by: Some CDNs, high-throughput systems
\`\`\`

## Redis Eviction Policies

\`\`\`
Redis maxmemory-policy options:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Policy              ‚îÇ  Behavior                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  noeviction          ‚îÇ  Error when memory full          ‚îÇ
‚îÇ  allkeys-lru         ‚îÇ  LRU among all keys              ‚îÇ
‚îÇ  volatile-lru        ‚îÇ  LRU among keys with TTL         ‚îÇ
‚îÇ  allkeys-lfu         ‚îÇ  LFU among all keys              ‚îÇ
‚îÇ  volatile-lfu        ‚îÇ  LFU among keys with TTL         ‚îÇ
‚îÇ  allkeys-random      ‚îÇ  Random among all keys           ‚îÇ
‚îÇ  volatile-random     ‚îÇ  Random among keys with TTL      ‚îÇ
‚îÇ  volatile-ttl        ‚îÇ  Evict keys with shortest TTL    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Configuration:
maxmemory 4gb
maxmemory-policy allkeys-lru
\`\`\`

## Choosing the Right Policy

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               DECISION FLOWCHART                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  Is recent access a good predictor of future access?    ‚îÇ
‚îÇ                     ‚îÇ                                    ‚îÇ
‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ           ‚îÇ                   ‚îÇ                          ‚îÇ
‚îÇ          YES                 NO                          ‚îÇ
‚îÇ           ‚îÇ                   ‚îÇ                          ‚îÇ
‚îÇ           ‚ñº                   ‚ñº                          ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ    ‚îÇ    LRU    ‚îÇ      ‚îÇ Is frequency  ‚îÇ                 ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ predictive?   ‚îÇ                 ‚îÇ
‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                               ‚îÇ                          ‚îÇ
‚îÇ                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ                     ‚îÇ                   ‚îÇ                ‚îÇ
‚îÇ                    YES                 NO                ‚îÇ
‚îÇ                     ‚îÇ                   ‚îÇ                ‚îÇ
‚îÇ                     ‚ñº                   ‚ñº                ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ              ‚îÇ    LFU    ‚îÇ      ‚îÇ  Random/  ‚îÇ           ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ   FIFO    ‚îÇ           ‚îÇ
‚îÇ                                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[EXAMPLE]
## Access Pattern Analysis

\`\`\`
Scenario: E-commerce product cache

Access distribution (Zipfian):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Product ID  ‚îÇ  Access %     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Top 100     ‚îÇ  50%          ‚îÇ
‚îÇ  Top 1000    ‚îÇ  80%          ‚îÇ
‚îÇ  Top 10000   ‚îÇ  95%          ‚îÇ
‚îÇ  All 1M      ‚îÇ  100%         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Analysis:
- 0.01% of products get 50% of traffic
- LFU would be ideal (keeps popular items)
- LRU would also work (recent = frequent)
- Random would be suboptimal

Recommendation: LFU or LRU with TTL
\`\`\`

[TIP]
## Eviction Best Practices

1. **Monitor eviction rates** - High eviction = cache too small or wrong policy
2. **Set memory limits explicitly** - Don't let cache grow unbounded
3. **Use TTL as safety net** - Even with LRU/LFU, set max TTL
4. **Test with production patterns** - Synthetic benchmarks lie
5. **Consider hybrid approaches** - LRU + TTL is common

[WARNING]
## Eviction Anti-Patterns

1. **No eviction policy** - Cache fills, new items can't be cached
2. **Cache too small** - Constant eviction, poor hit rate
3. **Wrong policy for workload** - LRU for frequency-based access
4. **Ignoring metrics** - Flying blind
5. **Homogeneous TTLs** - Everything expires at once (stampede!)

\`\`\`
BAD: All TTLs = 3600 seconds
     At T+3600, everything expires ‚Üí stampede!

GOOD: TTL = 3600 + random(0, 600)
     Expiration spread over 10 minutes ‚Üí smooth load
\`\`\`

[REAL-WORLD]
## How Companies Handle Eviction

**Facebook:** Custom LRU with probabilistic eviction
**Netflix:** EVCache with intelligent TTLs based on content popularity
**Twitter:** LRU for timelines, LFU for user data
**Cloudflare:** Random eviction at edge (performance critical)

The pattern: **Match eviction policy to access patterns, always monitor.**`,

            // ==================== DAY 7: Advanced Load Balancing ====================
            'hld_18': `## Consistent Hashing

[STORY]
## The Memcached Meltdown

In 2008, a team added a 11th server to their 10-server Memcached cluster. They expected a 10% improvement. Instead, they got a **90% cache miss rate** and their database nearly melted.

The culprit? **Modulo hashing**. When server count changed from 10 to 11, the formula \`hash(key) % servers\` remapped 90% of keys to different servers. Every remapped key was a cache miss.

The solution? **Consistent hashing**‚Äîa brilliant algorithm that minimizes disruption when servers change.

## The Problem with Modulo Hashing

\`\`\`
Simple modulo: server = hash(key) % num_servers

With 10 servers:
key "user:123" ‚Üí hash = 7823 ‚Üí 7823 % 10 = 3 ‚Üí Server 3

Add 11th server:
key "user:123" ‚Üí hash = 7823 ‚Üí 7823 % 11 = 1 ‚Üí Server 1 (DIFFERENT!)

Result:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  10 ‚Üí 11 servers: ~90% of keys remap!                   ‚îÇ
‚îÇ  1M cached items: 900,000 cache misses                  ‚îÇ
‚îÇ  Database: 900,000 sudden queries (CRASH!)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## How Consistent Hashing Works

\`\`\`
Concept: Map both servers AND keys to a circular ring (0 to 2^32)

Step 1: Place servers on the ring
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ                    0/2^32                               ‚îÇ
‚îÇ                      ‚îÇ                                   ‚îÇ
‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ              ‚ï±               ‚ï≤                          ‚îÇ
‚îÇ             ‚ï±                 ‚ï≤                         ‚îÇ
‚îÇ         Server A           Server B                     ‚îÇ
‚îÇ            ‚îÇ                   ‚îÇ                         ‚îÇ
‚îÇ            ‚îÇ                   ‚îÇ                         ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ                    ‚îÇ                                     ‚îÇ
‚îÇ                Server C                                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 2: To find server for a key, walk clockwise to first server
key "user:123" ‚Üí hash position ‚Üí walk clockwise ‚Üí Server B
\`\`\`

## Adding/Removing Servers

\`\`\`
Before adding Server D:
                    0
                    ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ               ‚îÇ
         [A]               [B]
            ‚îÇ               ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                   [C]

Keys between C and A ‚Üí Server A
Keys between A and B ‚Üí Server B
Keys between B and C ‚Üí Server C

After adding Server D (between B and C):
                    0
                    ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ               ‚îÇ
         [A]               [B]
            ‚îÇ               ‚îÇ
            ‚îÇ          [D]‚îÄ‚îÄ‚îò   ‚Üê New server
            ‚îÇ           ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                   [C]

Only keys between B and D move to Server D!
Other keys stay on same server!

Impact: Only ~1/N keys remap (N = number of servers)
With 10 servers: ~10% remap instead of 90%!
\`\`\`

## Virtual Nodes

\`\`\`
Problem: Uneven distribution with few servers

Solution: Each server gets multiple "virtual nodes"

Physical Server A ‚Üí Virtual nodes: A1, A2, A3, A4, A5
Physical Server B ‚Üí Virtual nodes: B1, B2, B3, B4, B5

                    0
                    ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         A2                   B1
          ‚îÇ                   ‚îÇ
         B3                   A4
          ‚îÇ                   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   A1
                    ‚îÇ
                   B2

Benefits:
‚úì More even distribution
‚úì Gradual rebalancing when server added/removed
‚úì Can weight servers (powerful server = more virtual nodes)
\`\`\`

## Implementation Example

\`\`\`python
import hashlib
from bisect import bisect_right

class ConsistentHash:
    def __init__(self, nodes=None, virtual_nodes=100):
        self.virtual_nodes = virtual_nodes
        self.ring = {}  # hash -> node
        self.sorted_keys = []
        
        if nodes:
            for node in nodes:
                self.add_node(node)
    
    def _hash(self, key):
        return int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    def add_node(self, node):
        for i in range(self.virtual_nodes):
            virtual_key = f"{node}:{i}"
            hash_val = self._hash(virtual_key)
            self.ring[hash_val] = node
            self.sorted_keys.append(hash_val)
        self.sorted_keys.sort()
    
    def remove_node(self, node):
        for i in range(self.virtual_nodes):
            virtual_key = f"{node}:{i}"
            hash_val = self._hash(virtual_key)
            del self.ring[hash_val]
            self.sorted_keys.remove(hash_val)
    
    def get_node(self, key):
        if not self.ring:
            return None
        hash_val = self._hash(key)
        idx = bisect_right(self.sorted_keys, hash_val) % len(self.sorted_keys)
        return self.ring[self.sorted_keys[idx]]

# Usage
ch = ConsistentHash(['server1', 'server2', 'server3'])
print(ch.get_node('user:123'))  # ‚Üí server2
ch.add_node('server4')  # Only ~25% of keys move!
\`\`\`

[TIP]
## Consistent Hashing Best Practices

1. **Use enough virtual nodes** - 100-200 per server for good distribution
2. **Monitor distribution** - Check for hotspots
3. **Use cryptographic hash** - MD5/SHA for uniform distribution
4. **Plan for rebalancing** - Even with consistent hashing, some data moves

[WARNING]
## Consistent Hashing Pitfalls

1. **Too few virtual nodes** - Uneven distribution
2. **Hot keys** - One popular key still overwhelms one server
3. **Replication complexity** - Need to handle N replicas on ring
4. **Memory overhead** - Virtual nodes consume memory

[REAL-WORLD]
## Who Uses Consistent Hashing

**Amazon DynamoDB:** Partitions data across nodes
**Apache Cassandra:** Distributes data across ring
**Discord:** Distributes chat messages across servers
**Akamai CDN:** Routes requests to edge servers

The pattern: **Consistent hashing = minimal disruption during scaling.**`,

            'hld_19': `## Service Discovery and Registration

[STORY]
## The Microservices Maze

A startup migrated from monolith to 50 microservices. At first, they hardcoded IP addresses in config files. Then servers started failing, auto-scaling kicked in, and suddenly **nothing could find anything**.

They learned the hard way: in dynamic environments, services need to **discover** each other automatically.

> "In a microservices world, the network is the computer‚Äîand service discovery is how it knows itself." - Distributed systems wisdom

## The Discovery Problem

\`\`\`
Static Configuration (Old Way):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  config.yaml:                                           ‚îÇ
‚îÇ    user-service: 10.0.1.5:8080                         ‚îÇ
‚îÇ    order-service: 10.0.1.6:8080                        ‚îÇ
‚îÇ    payment-service: 10.0.1.7:8080                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Problems:                                              ‚îÇ
‚îÇ  ‚úó Server dies? Config is wrong.                        ‚îÇ
‚îÇ  ‚úó Auto-scaling adds servers? Config doesn't know.      ‚îÇ
‚îÇ  ‚úó Deploy new version? Must update all configs.         ‚îÇ
‚îÇ  ‚úó 50 services √ó 10 instances = 500 endpoints to track! ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Service Discovery (Modern Way):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  1. Service registers: "I'm user-service at 10.0.2.15"  ‚îÇ
‚îÇ  2. Client asks: "Where is user-service?"               ‚îÇ
‚îÇ  3. Registry answers: "10.0.2.15, 10.0.2.16, 10.0.2.17" ‚îÇ
‚îÇ  4. Client picks one and calls it                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚úì Dynamic! Servers come and go automatically.          ‚îÇ
‚îÇ  ‚úì Load balanced! Multiple instances available.         ‚îÇ
‚îÇ  ‚úì Health-checked! Dead servers removed.                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Service Discovery Patterns

### 1. Client-Side Discovery

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    1. Query     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ Client ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   Registry   ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ (Consul/etcd)‚îÇ           ‚îÇ
‚îÇ       ‚îÇ                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ       ‚îÇ 2. Get list: [A, B, C]                         ‚îÇ
‚îÇ       ‚îÇ                                                 ‚îÇ
‚îÇ       ‚îÇ 3. Pick one (load balance)                     ‚îÇ
‚îÇ       ‚ñº                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  ‚îÇService ‚îÇ  ‚îÇService ‚îÇ  ‚îÇService ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ   A    ‚îÇ  ‚îÇ   B    ‚îÇ  ‚îÇ   C    ‚îÇ                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Pros: Client controls load balancing
Cons: Client needs discovery logic
\`\`\`

### 2. Server-Side Discovery

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ Client ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Load Balancer ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ (knows registry)‚îÇ            ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                                  ‚îÇ                       ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ              ‚ñº                   ‚ñº                   ‚ñº  ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         ‚îÇService ‚îÇ          ‚îÇService ‚îÇ          ‚îÇService ‚îÇ
‚îÇ         ‚îÇ   A    ‚îÇ          ‚îÇ   B    ‚îÇ          ‚îÇ   C    ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Pros: Client is simple, just calls LB
Cons: LB is single point of failure
\`\`\`

## Service Registration

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SERVICE LIFECYCLE                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  1. STARTUP                                             ‚îÇ
‚îÇ     Service starts ‚Üí Registers with registry            ‚îÇ
‚îÇ     "I am order-service at 10.0.2.15:8080"              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  2. HEARTBEAT                                           ‚îÇ
‚îÇ     Service sends periodic health signals               ‚îÇ
‚îÇ     Every 10 seconds: "I'm still alive!"                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  3. HEALTH CHECK                                        ‚îÇ
‚îÇ     Registry actively checks service health             ‚îÇ
‚îÇ     GET /health ‚Üí 200 OK (healthy) or timeout (dead)    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  4. DEREGISTRATION                                      ‚îÇ
‚îÇ     Service shutting down ‚Üí Tells registry              ‚îÇ
‚îÇ     "I'm going away, remove me from the list"           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  5. FAILURE DETECTION                                   ‚îÇ
‚îÇ     If heartbeat/health check fails:                    ‚îÇ
‚îÇ     Registry marks service as unhealthy                 ‚îÇ
‚îÇ     After threshold: Remove from available list         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Popular Service Discovery Tools

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Tool      ‚îÇ  Consistency ‚îÇ  Features                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Consul    ‚îÇ  CP (Raft)   ‚îÇ  Service mesh, KV store,    ‚îÇ
‚îÇ            ‚îÇ              ‚îÇ  multi-datacenter           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  etcd      ‚îÇ  CP (Raft)   ‚îÇ  Simple KV, Kubernetes      ‚îÇ
‚îÇ            ‚îÇ              ‚îÇ  backing store              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ZooKeeper ‚îÇ  CP (ZAB)    ‚îÇ  Mature, complex,           ‚îÇ
‚îÇ            ‚îÇ              ‚îÇ  coordination primitives    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Eureka    ‚îÇ  AP          ‚îÇ  Netflix OSS, eventual      ‚îÇ
‚îÇ            ‚îÇ              ‚îÇ  consistency, simple        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Kubernetes Service Discovery

\`\`\`
Kubernetes has built-in service discovery!

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  apiVersion: v1                                         ‚îÇ
‚îÇ  kind: Service                                          ‚îÇ
‚îÇ  metadata:                                              ‚îÇ
‚îÇ    name: user-service                                   ‚îÇ
‚îÇ  spec:                                                  ‚îÇ
‚îÇ    selector:                                            ‚îÇ
‚îÇ      app: user                                          ‚îÇ
‚îÇ    ports:                                               ‚îÇ
‚îÇ      - port: 80                                         ‚îÇ
‚îÇ        targetPort: 8080                                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Result:                                                ‚îÇ
‚îÇ  ‚úì DNS name: user-service.default.svc.cluster.local    ‚îÇ
‚îÇ  ‚úì Load balancing across all pods with app=user        ‚îÇ
‚îÇ  ‚úì Automatic health checking                           ‚îÇ
‚îÇ  ‚úì No external registry needed!                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Service Discovery Best Practices

1. **Use health checks** - Don't just check if port is open
2. **Implement graceful shutdown** - Deregister before stopping
3. **Handle discovery failures** - Cache last known good endpoints
4. **Use DNS caching wisely** - Low TTL for dynamic environments
5. **Monitor registry health** - It's critical infrastructure

[WARNING]
## Common Mistakes

1. **Hardcoding IPs** - Defeats the purpose
2. **Not handling stale data** - Client caches outdated endpoints
3. **Ignoring network partitions** - Registry might be unreachable
4. **No fallback mechanism** - Single registry = single point of failure
5. **Overly aggressive health checks** - Can cause flapping

[REAL-WORLD]
## How Companies Discover Services

**Netflix:** Eureka for service discovery, Zuul for routing
**Uber:** Custom discovery with Ringpop and Hyperbahn
**Airbnb:** SmartStack (Nerve + Synapse) with HAProxy
**Kubernetes users:** Built-in DNS-based discovery

The pattern: **Let infrastructure handle discovery, not application code.**`,

            'hld_20': `## API Gateway Pattern

[STORY]
## The Mobile App Nightmare

A mobile team needed data from 7 microservices for one screen: user profile, orders, recommendations, notifications, settings, analytics, and ads. Their app made 7 separate API calls, each with its own authentication, error handling, and retry logic.

Battery drain skyrocketed. Latency was terrible. The code was unmaintainable. They needed a single entry point‚Äîan **API Gateway**.

> "An API Gateway is like a hotel concierge‚Äîone person who knows everyone and can coordinate anything." - API architect

## What is an API Gateway?

\`\`\`
Without API Gateway:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Mobile App ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Service A (auth)                     ‚îÇ
‚îÇ      ‚îÇ                                                   ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Service B (auth)                   ‚îÇ
‚îÇ      ‚îÇ                                                   ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Service C (auth)                   ‚îÇ
‚îÇ      ‚îÇ                                                   ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Service D (auth)                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Problems:                                              ‚îÇ
‚îÇ  ‚úó Multiple round trips (high latency)                  ‚îÇ
‚îÇ  ‚úó Auth logic duplicated in each service                ‚îÇ
‚îÇ  ‚úó Client coupled to internal service structure         ‚îÇ
‚îÇ  ‚úó Protocol translation (gRPC internally, REST public)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

With API Gateway:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Mobile App ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ API Gateway ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Service A         ‚îÇ
‚îÇ                        ‚îÇ                                 ‚îÇ
‚îÇ                        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Service B         ‚îÇ
‚îÇ                        ‚îÇ                                 ‚îÇ
‚îÇ                        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Service C         ‚îÇ
‚îÇ                        ‚îÇ                                 ‚îÇ
‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Service D         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Benefits:                                              ‚îÇ
‚îÇ  ‚úì Single entry point                                   ‚îÇ
‚îÇ  ‚úì Centralized auth                                     ‚îÇ
‚îÇ  ‚úì Request aggregation                                  ‚îÇ
‚îÇ  ‚úì Protocol translation                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Gateway Responsibilities

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                API GATEWAY FUNCTIONS                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  1. ROUTING                                             ‚îÇ
‚îÇ     /users/* ‚Üí user-service                             ‚îÇ
‚îÇ     /orders/* ‚Üí order-service                           ‚îÇ
‚îÇ     /products/* ‚Üí product-service                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  2. AUTHENTICATION & AUTHORIZATION                      ‚îÇ
‚îÇ     Verify JWT tokens                                   ‚îÇ
‚îÇ     Check permissions                                   ‚îÇ
‚îÇ     Reject unauthorized requests                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  3. RATE LIMITING                                       ‚îÇ
‚îÇ     100 requests/minute per user                        ‚îÇ
‚îÇ     1000 requests/minute per API key                    ‚îÇ
‚îÇ     Protect backend services                            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  4. REQUEST/RESPONSE TRANSFORMATION                     ‚îÇ
‚îÇ     Add headers, modify payloads                        ‚îÇ
‚îÇ     Protocol translation (REST ‚Üî gRPC)                  ‚îÇ
‚îÇ     Response aggregation                                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  5. CACHING                                             ‚îÇ
‚îÇ     Cache common responses                              ‚îÇ
‚îÇ     Reduce backend load                                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  6. MONITORING & LOGGING                                ‚îÇ
‚îÇ     Centralized request logging                         ‚îÇ
‚îÇ     Metrics collection                                  ‚îÇ
‚îÇ     Distributed tracing initiation                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Request Aggregation Example

\`\`\`
Mobile app needs: User profile + Recent orders + Recommendations

Without Gateway (3 requests):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Request 1: GET /users/123           ‚Üí 100ms           ‚îÇ
‚îÇ  Request 2: GET /users/123/orders    ‚Üí 150ms           ‚îÇ
‚îÇ  Request 3: GET /recommendations/123 ‚Üí 200ms           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Total: 450ms (sequential) or 200ms (parallel)         ‚îÇ
‚îÇ  3 round trips from mobile (battery drain!)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

With Gateway (1 request):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Request: GET /mobile/dashboard/123                    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Gateway internally (parallel):                        ‚îÇ
‚îÇ    ‚îú‚îÄ‚îÄ GET user-service/users/123          ‚Üí 100ms     ‚îÇ
‚îÇ    ‚îú‚îÄ‚îÄ GET order-service/users/123/orders  ‚Üí 150ms     ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ GET rec-service/recommendations/123 ‚Üí 200ms     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Total: ~220ms (gateway overhead + max service time)   ‚îÇ
‚îÇ  1 round trip from mobile!                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Gateway Patterns

### 1. Single Gateway

\`\`\`
All traffic through one gateway:

     Internet
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  API Gateway  ‚îÇ ‚Üê Single entry point
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚ñº         ‚ñº
Services  Services

Pros: Simple, centralized control
Cons: Single point of failure, bottleneck
\`\`\`

### 2. Backend for Frontend (BFF)

\`\`\`
Different gateways for different clients:

  Web App      Mobile App      Partner API
     ‚îÇ              ‚îÇ               ‚îÇ
     ‚ñº              ‚ñº               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Web BFF ‚îÇ  ‚îÇMobile BFF ‚îÇ  ‚îÇPartner BFF‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ             ‚îÇ              ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº         ‚ñº
           Services  Services

Pros: Optimized for each client type
Cons: More gateways to maintain
\`\`\`

## Popular API Gateways

| Gateway | Best For | Key Features |
|---------|----------|--------------|
| **Kong** | General purpose | Plugins, Lua scripting |
| **AWS API Gateway** | AWS ecosystem | Serverless, managed |
| **Nginx** | Performance | Lightweight, fast |
| **Traefik** | Kubernetes | Auto-discovery, Let's Encrypt |
| **Envoy** | Service mesh | gRPC, observability |

[TIP]
## API Gateway Best Practices

1. **Keep gateway stateless** - Easy to scale horizontally
2. **Don't put business logic in gateway** - It's infrastructure
3. **Implement circuit breakers** - Protect against cascade failures
4. **Use caching strategically** - Reduce backend load
5. **Monitor gateway metrics** - It sees all traffic

[WARNING]
## Gateway Anti-Patterns

1. **Fat gateway** - Too much logic, becomes monolith
2. **Single gateway for everything** - Becomes bottleneck
3. **No rate limiting** - Backend services overwhelmed
4. **Tight coupling** - Gateway knows too much about services
5. **Ignoring security** - Gateway is your perimeter

[REAL-WORLD]
## How Companies Use API Gateways

**Netflix:** Zuul for edge routing, handles 50B+ requests/day
**Amazon:** API Gateway for serverless, internal gateways per team
**Uber:** Custom gateway handles routing, auth, rate limiting
**Airbnb:** Envoy-based gateway for service mesh

The pattern: **Gateway as infrastructure, not application logic.**`,

            // ==================== DAY 8: CAP Theorem and Consistency ====================
            'hld_21': `## CAP Theorem Explained

[STORY]
## The Great Partition of 2012

When Hurricane Sandy hit the US East Coast in 2012, it knocked out a major AWS data center. Companies running in that region faced an impossible choice: stay consistent (reject all writes until connection restored) or stay available (accept writes, deal with conflicts later).

This wasn't a bug‚Äîit was a fundamental law of distributed systems: the **CAP theorem**.

> "The CAP theorem is not about trade-offs you make once. It's about trade-offs you make every millisecond during a network partition." - Distributed systems engineer

## The CAP Theorem

\`\`\`
In a distributed system, during a network partition,
you can only have TWO of THREE guarantees:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ                    CONSISTENCY                          ‚îÇ
‚îÇ                        ‚ï±‚ï≤                                ‚îÇ
‚îÇ                       ‚ï±  ‚ï≤                               ‚îÇ
‚îÇ                      ‚ï±    ‚ï≤                              ‚îÇ
‚îÇ                     ‚ï±      ‚ï≤                             ‚îÇ
‚îÇ                    ‚ï±        ‚ï≤                            ‚îÇ
‚îÇ                   ‚ï±   CAP    ‚ï≤                           ‚îÇ
‚îÇ                  ‚ï±   THEOREM  ‚ï≤                          ‚îÇ
‚îÇ                 ‚ï±              ‚ï≤                         ‚îÇ
‚îÇ                ‚ï±                ‚ï≤                        ‚îÇ
‚îÇ     AVAILABILITY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PARTITION               ‚îÇ
‚îÇ                                 TOLERANCE               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

C - Consistency: Every read receives the most recent write
A - Availability: Every request receives a response
P - Partition Tolerance: System continues despite network failures
\`\`\`

## Understanding Each Property

### Consistency (C)

\`\`\`
All nodes see the same data at the same time.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A  ‚îÇ         ‚îÇ Node B  ‚îÇ
‚îÇ x = 5   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ x = 5   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

If Node A writes x = 10:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A  ‚îÇ         ‚îÇ Node B  ‚îÇ
‚îÇ x = 10  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ x = 10  ‚îÇ  ‚Üê Both updated atomically
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Any read from either node returns x = 10.
\`\`\`

### Availability (A)

\`\`\`
Every request receives a (non-error) response.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A  ‚îÇ         ‚îÇ Node B  ‚îÇ
‚îÇ  (OK)   ‚îÇ         ‚îÇ  (OK)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚ñ≤                   ‚ñ≤
     ‚îÇ                   ‚îÇ
  Request            Request
     ‚îÇ                   ‚îÇ
   User 1             User 2

Both users get a response, always.
(Even if the data might be stale)
\`\`\`

### Partition Tolerance (P)

\`\`\`
System operates despite network failures between nodes.

Normal:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A  ‚îÇ Network ‚îÇ Node B  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Partition:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    X    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄX‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Node B  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  (cut)  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

System must continue operating!
\`\`\`

## Why You Must Choose P

\`\`\`
In distributed systems, network partitions WILL happen:
- Cables get cut
- Routers fail
- Data centers lose power
- Cloud regions become isolated

If you don't tolerate partitions (no P):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  You have a single-node system!                         ‚îÇ
‚îÇ  Not distributed = no CAP trade-off                     ‚îÇ
‚îÇ  But also no horizontal scaling, no fault tolerance     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Reality: P is mandatory. Choose between C and A.
\`\`\`

## CP vs AP Systems

\`\`\`
During a network partition:

CP System (Consistency + Partition Tolerance):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    X    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄX‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Node B  ‚îÇ
‚îÇ (serve) ‚îÇ         ‚îÇ(REJECT) ‚îÇ  ‚Üê Node B rejects requests
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    to maintain consistency

User ‚Üí Node B: "Sorry, unavailable until partition heals"
Benefit: No stale/conflicting data
Cost: Some users can't access system


AP System (Availability + Partition Tolerance):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    X    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node A  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄX‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Node B  ‚îÇ
‚îÇ x = 10  ‚îÇ         ‚îÇ x = 5   ‚îÇ  ‚Üê Both serve (different data!)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

User ‚Üí Node A: "x = 10"
User ‚Üí Node B: "x = 5"
Benefit: Always available
Cost: Temporary inconsistency, conflict resolution needed
\`\`\`

## Real-World Examples

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CP Systems (Consistency Priority)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Bank transactions (must be accurate)                 ‚îÇ
‚îÇ  ‚Ä¢ Inventory for limited items (can't oversell)         ‚îÇ
‚îÇ  ‚Ä¢ Leader election (only one leader!)                   ‚îÇ
‚îÇ  ‚Ä¢ Configuration management (all nodes same config)     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Examples: ZooKeeper, etcd, Consul, HBase              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AP Systems (Availability Priority)                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Social media likes (temporary inconsistency OK)      ‚îÇ
‚îÇ  ‚Ä¢ Shopping cart (resolve conflicts later)              ‚îÇ
‚îÇ  ‚Ä¢ DNS (eventual consistency acceptable)                ‚îÇ
‚îÇ  ‚Ä¢ Session storage (availability critical)              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Examples: Cassandra, DynamoDB, CouchDB                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[EXAMPLE]
## Scenario: E-Commerce Inventory

\`\`\`
Product: Limited edition sneakers (100 pairs)

CP Approach:
- During partition, reject orders from isolated region
- Users see "temporarily unavailable"
- No overselling guaranteed
- Some lost sales

AP Approach:
- Accept orders from both regions during partition
- Risk: 150 orders for 100 pairs
- Post-partition: Cancel 50 orders, refund
- Better availability, customer service headache

Hybrid (common):
- AP for adding to cart (always available)
- CP for final checkout (consistency critical)
\`\`\`

[TIP]
## CAP Best Practices

1. **Default to AP** for most use cases (availability usually wins)
2. **Use CP only when consistency is critical** (money, inventory)
3. **Design for partition recovery** - conflicts will happen
4. **Understand your SLAs** - what matters more to your business?
5. **Test partition scenarios** - use chaos engineering

[WARNING]
## CAP Misconceptions

1. **"Pick 2 of 3 at design time"** - No! You pick during partition
2. **"CA systems exist"** - In distributed systems, P is mandatory
3. **"Consistency = ACID"** - CAP consistency is about replicas
4. **"It's binary"** - Actually a spectrum (see PACELC)

[REAL-WORLD]
## How Companies Navigate CAP

**Google Spanner:** Claims CA by minimizing partition likelihood
**Amazon DynamoDB:** AP by default, strong consistency optional
**Apache Cassandra:** Tunable consistency (AP to CP per query)
**MongoDB:** CP with replica sets, configurable concern levels

The pattern: **Choose based on business requirements, not technical preference.**`,

            'hld_22': `## Consistency Models

[STORY]
## The Missing Like

Sarah liked her friend's photo on Instagram. She saw the like count go from 99 to 100. Refreshing the page, it showed 99 again. She liked it again‚Äînow it showed 101. But her friend saw 100 the whole time.

This isn't a bug‚Äîit's **eventual consistency** in action. Instagram trades perfect consistency for global availability. And for likes, that's the right trade-off.

> "Strong consistency is expensive. Eventual consistency is confusing. The skill is knowing which to use where." - Distributed systems wisdom

## The Consistency Spectrum

\`\`\`
STRONG ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ EVENTUAL

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Linearizable   Sequential   Causal    Eventual         ‚îÇ
‚îÇ      ‚îÇ              ‚îÇ          ‚îÇ          ‚îÇ              ‚îÇ
‚îÇ  "One true       "Order     "Cause    "Eventually      ‚îÇ
‚îÇ   timeline"      matters"   before     same"           ‚îÇ
‚îÇ                             effect"                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚óÑ‚îÄ‚îÄ‚îÄ Slower, More Consistent ‚îÄ‚îÄ Faster, Less ‚îÄ‚îÄ‚îÄ‚ñ∫      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Consistency Models Explained

### 1. Strong Consistency (Linearizability)

\`\`\`
Every read returns the most recent write. Period.

Timeline:
Client A:  WRITE x=1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                      ‚îÇ
Client B:        READ x ‚îÄ‚îÄ‚ñ∫ returns 1 (always!)
                      ‚îÇ
Client C:             ‚îÇ    READ x ‚îÄ‚îÄ‚ñ∫ returns 1 (always!)

All operations appear in a single global order.
As if there's only one copy of the data.

Cost: Every write must propagate to all replicas
      before acknowledging. SLOW.
\`\`\`

### 2. Eventual Consistency

\`\`\`
If no new updates, all replicas will eventually converge.

Timeline:
Write x=1 at Node A
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚ñ∫ Node A: x=1 (immediate)
         ‚îÇ
         ‚îú‚îÄ ‚îÄ ‚îÄ‚ñ∫ Node B: x=1 (after delay)
         ‚îÇ
         ‚îî‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ‚ñ∫ Node C: x=1 (after longer delay)

During propagation:
  Read from A: x=1 ‚úì
  Read from B: x=0 ‚úó (stale)
  Read from C: x=0 ‚úó (stale)

Eventually (seconds to minutes):
  All nodes: x=1 ‚úì
\`\`\`

### 3. Causal Consistency

\`\`\`
Causally related operations seen in order.
Concurrent operations may be seen in any order.

Example: Comment thread

User A: Posts "Hello!"           (Event 1)
User B: Replies "Hi there!"      (Event 2, caused by 1)
User C: Replies "Welcome!"       (Event 3, caused by 1)

Causal consistency guarantees:
‚úì Everyone sees Event 1 before Events 2 and 3
‚úó Events 2 and 3 may appear in different orders

You'll never see a reply before its parent post!
\`\`\`

### 4. Read-Your-Writes

\`\`\`
A user always sees their own writes.

User updates profile name to "John":

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  User's perspective:                                    ‚îÇ
‚îÇ  1. Update name ‚Üí "John"                                ‚îÇ
‚îÇ  2. Refresh page ‚Üí Sees "John" ‚úì                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Other users' perspective:                              ‚îÇ
‚îÇ  1. View profile ‚Üí Might still see old name             ‚îÇ
‚îÇ  2. Eventually ‚Üí Sees "John"                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Implementation: Route user's reads to same replica
               that received their writes.
\`\`\`

### 5. Monotonic Reads

\`\`\`
Once you've seen a value, you never see an older value.

BAD (non-monotonic):
  Read 1: x = 10
  Read 2: x = 5   ‚Üê Went backwards! Confusing!

GOOD (monotonic):
  Read 1: x = 10
  Read 2: x = 10  ‚Üê Same (OK)
  Read 3: x = 15  ‚Üê Newer (OK)

Implementation: Pin user to same replica or track version.
\`\`\`

## Choosing a Consistency Model

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Use Case               ‚îÇ  Recommended Model            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Bank balance           ‚îÇ  Strong (linearizable)        ‚îÇ
‚îÇ  Stock trading          ‚îÇ  Strong (linearizable)        ‚îÇ
‚îÇ  Inventory count        ‚îÇ  Strong or sequential         ‚îÇ
‚îÇ  User session           ‚îÇ  Read-your-writes             ‚îÇ
‚îÇ  Social media likes     ‚îÇ  Eventual                     ‚îÇ
‚îÇ  Comment threads        ‚îÇ  Causal                       ‚îÇ
‚îÇ  Analytics counters     ‚îÇ  Eventual                     ‚îÇ
‚îÇ  Configuration          ‚îÇ  Strong or sequential         ‚îÇ
‚îÇ  Shopping cart          ‚îÇ  Read-your-writes + eventual  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Consistency in Popular Databases

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Database         ‚îÇ  Default          ‚îÇ  Options        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  PostgreSQL       ‚îÇ  Strong (single)  ‚îÇ  -              ‚îÇ
‚îÇ  MySQL            ‚îÇ  Strong (single)  ‚îÇ  -              ‚îÇ
‚îÇ  MongoDB          ‚îÇ  Eventual         ‚îÇ  Strong (w:maj) ‚îÇ
‚îÇ  Cassandra        ‚îÇ  Eventual         ‚îÇ  Tunable (CL)   ‚îÇ
‚îÇ  DynamoDB         ‚îÇ  Eventual         ‚îÇ  Strong (option)‚îÇ
‚îÇ  Redis Cluster    ‚îÇ  Eventual         ‚îÇ  WAIT command   ‚îÇ
‚îÇ  CockroachDB      ‚îÇ  Serializable     ‚îÇ  -              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[EXAMPLE]
## Implementing Read-Your-Writes

\`\`\`python
# Strategy: Session stickiness + version tracking

class ConsistentReader:
    def __init__(self):
        self.last_write_version = {}  # user_id ‚Üí version
    
    def write(self, user_id, key, value):
        version = database.write(key, value)
        self.last_write_version[user_id] = version
        return version
    
    def read(self, user_id, key):
        min_version = self.last_write_version.get(user_id, 0)
        
        # Wait until replica has caught up
        replica = find_replica_with_version(min_version)
        return replica.read(key)

# User always sees their own writes!
\`\`\`

[TIP]
## Consistency Best Practices

1. **Default to eventual** - Strong consistency is expensive
2. **Use read-your-writes for user-facing data** - Users expect it
3. **Causal for messaging/comments** - Order matters
4. **Strong only where legally/financially required**
5. **Document your consistency guarantees** - Developers need to know

[WARNING]
## Common Consistency Mistakes

1. **Assuming strong consistency** - Most distributed DBs are eventual
2. **Ignoring replication lag** - Reads from replicas can be stale
3. **Not testing eventual consistency** - Works in dev, breaks in prod
4. **Mixing models without understanding** - Leads to data anomalies

[REAL-WORLD]
## How Companies Handle Consistency

**Facebook:** Timeline is eventually consistent, privacy checks are strong
**Amazon:** Cart is eventual, checkout is strong
**Google Docs:** Operational transformation (causal + eventual)
**Slack:** Messages are causal, read receipts are eventual

The pattern: **Use the weakest consistency model that meets your requirements.**`,

            'hld_23': `## Consensus Algorithms: Paxos and Raft

[STORY]
## The Split-Brain Disaster

In 2017, a major cloud provider experienced a split-brain scenario. Two nodes both believed they were the leader. Both accepted writes. When the network healed, they had conflicting data that took weeks to reconcile.

The solution? **Consensus algorithms**‚Äîmathematical proofs that ensure distributed systems agree, even when things go wrong.

> "A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable." - Leslie Lamport (Paxos inventor)

## The Consensus Problem

\`\`\`
Problem: Multiple nodes must agree on a single value

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Scenario: Electing a leader                            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Node A thinks: "I should be leader"                    ‚îÇ
‚îÇ  Node B thinks: "I should be leader"                    ‚îÇ
‚îÇ  Node C thinks: "I should be leader"                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Without consensus: Split brain, chaos                  ‚îÇ
‚îÇ  With consensus: All agree on exactly one leader        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Requirements:
1. Agreement: All nodes decide same value
2. Validity: Decided value was proposed by some node
3. Termination: All nodes eventually decide
4. Fault tolerance: Works despite failures
\`\`\`

## Paxos Algorithm

\`\`\`
Invented by Leslie Lamport in 1989.
Provably correct but notoriously complex.

Roles:
- Proposers: Suggest values
- Acceptors: Vote on proposals
- Learners: Learn the decided value

Phase 1: Prepare
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Proposer ‚Üí Acceptors: "Prepare proposal #N"            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Acceptor rules:                                        ‚îÇ
‚îÇ  - If N > any previous proposal: Promise to ignore < N  ‚îÇ
‚îÇ  - Reply with any value already accepted                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 2: Accept
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  If majority promised:                                  ‚îÇ
‚îÇ  Proposer ‚Üí Acceptors: "Accept value V for proposal #N" ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Acceptor rules:                                        ‚îÇ
‚îÇ  - If no higher proposal promised: Accept               ‚îÇ
‚îÇ  - If majority accepts: Value is CHOSEN                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Why it's hard:
- Multiple proposers can conflict
- Network delays cause confusion
- Edge cases require careful handling
\`\`\`

## Raft Algorithm

\`\`\`
Designed for understandability (2014).
Equivalent to Paxos but easier to implement.

Key insight: Decompose into sub-problems
1. Leader election
2. Log replication
3. Safety

States:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    timeout    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ Follower ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Candidate ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ       ‚ñ≤                          ‚îÇ                       ‚îÇ
‚îÇ       ‚îÇ                          ‚îÇ wins election         ‚îÇ
‚îÇ       ‚îÇ     discover leader      ‚ñº                       ‚îÇ
‚îÇ       ‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Leader  ‚îÇ                  ‚îÇ
‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Raft Leader Election

\`\`\`
Term: Logical clock, increments each election

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Time ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Term 1        ‚îÇ Term 2        ‚îÇ Term 3                 ‚îÇ
‚îÇ  Leader: A     ‚îÇ (election)    ‚îÇ Leader: C              ‚îÇ
‚îÇ                ‚îÇ A fails       ‚îÇ                         ‚îÇ
‚îÇ                ‚îÇ B,C candidate ‚îÇ                         ‚îÇ
‚îÇ                ‚îÇ C wins        ‚îÇ                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Election process:
1. Follower times out (no heartbeat from leader)
2. Converts to Candidate, increments term
3. Votes for self, requests votes from others
4. If majority votes received: Become leader
5. If another leader discovered: Become follower
6. If timeout: Start new election
\`\`\`

## Raft Log Replication

\`\`\`
Leader replicates log entries to followers:

Client ‚îÄ‚îÄWrite‚îÄ‚îÄ‚ñ∫ Leader
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº         ‚ñº         ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇLog    ‚îÇ ‚îÇLog    ‚îÇ ‚îÇLog    ‚îÇ
     ‚îÇ[1,2,3]‚îÇ ‚îÇ[1,2,3]‚îÇ ‚îÇ[1,2,3]‚îÇ
     ‚îÇ       ‚îÇ ‚îÇ       ‚îÇ ‚îÇ       ‚îÇ
     ‚îÇFollower‚îÇFollower‚îÇ ‚îÇFollower‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Steps:
1. Leader appends entry to local log
2. Leader sends AppendEntries to followers
3. Followers append and acknowledge
4. When majority ACK: Entry is COMMITTED
5. Leader notifies followers of commit
6. Leader responds to client
\`\`\`

## Comparison: Paxos vs Raft

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Aspect         ‚îÇ  Paxos          ‚îÇ  Raft              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Complexity     ‚îÇ  Very high      ‚îÇ  Moderate          ‚îÇ
‚îÇ  Leader         ‚îÇ  Optional       ‚îÇ  Required          ‚îÇ
‚îÇ  Correctness    ‚îÇ  Proven         ‚îÇ  Proven            ‚îÇ
‚îÇ  Implementation ‚îÇ  Hard           ‚îÇ  Easier            ‚îÇ
‚îÇ  Adoption       ‚îÇ  Google, etc    ‚îÇ  etcd, Consul      ‚îÇ
‚îÇ  Multi-decree   ‚îÇ  Separate algo  ‚îÇ  Built-in (log)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Rule of thumb: Use Raft unless you have specific Paxos needs.
Better yet: Use existing implementations (etcd, Consul).
\`\`\`

## When to Use Consensus

\`\`\`
USE consensus for:
‚úì Leader election
‚úì Distributed locks
‚úì Configuration management
‚úì Membership changes
‚úì Transaction commit decisions

DON'T use consensus for:
‚úó Every write (too slow)
‚úó Non-critical data
‚úó High-throughput use cases
‚úó When eventual consistency is acceptable

Consensus is expensive: Use sparingly!
\`\`\`

[EXAMPLE]
## etcd: Raft in Practice

\`\`\`
etcd is Kubernetes' backing store, uses Raft.

Architecture:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Kubernetes Cluster                                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  etcd 1  ‚îÇ‚óÑ‚îÄ‚î§  etcd 2  ‚îÇ‚óÑ‚îÄ‚î§  etcd 3  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ (leader) ‚îÇ  ‚îÇ(follower)‚îÇ  ‚îÇ(follower)‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ       ‚îÇ                                                 ‚îÇ
‚îÇ       ‚îÇ  All writes go to leader                        ‚îÇ
‚îÇ       ‚îÇ  Leader replicates to followers                 ‚îÇ
‚îÇ       ‚îÇ  Commit when majority ACK                       ‚îÇ
‚îÇ       ‚ñº                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ
‚îÇ  ‚îÇ   API    ‚îÇ  "Write pod spec to etcd"                ‚îÇ
‚îÇ  ‚îÇ  Server  ‚îÇ                                          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Consensus Best Practices

1. **Use odd number of nodes** - 3, 5, 7 (avoids ties)
2. **Tolerate N/2 - 1 failures** - 3 nodes tolerates 1 failure
3. **Keep nodes close** - Latency kills performance
4. **Don't roll your own** - Use etcd, Consul, ZooKeeper
5. **Monitor leader elections** - Frequent elections = problem

[WARNING]
## Consensus Pitfalls

1. **Using for every write** - Consensus is slow
2. **Too many nodes** - 5 is usually optimal
3. **Geographic distribution** - Latency across continents is brutal
4. **Ignoring split-brain scenarios** - Test failure modes
5. **Implementing yourself** - Very hard to get right

[REAL-WORLD]
## Who Uses Consensus

**Google:** Chubby (Paxos) for lock service
**Kubernetes:** etcd (Raft) for state storage
**HashiCorp:** Consul (Raft) for service discovery
**Apache:** ZooKeeper (ZAB, Paxos-like) for coordination
**CockroachDB:** Raft for distributed SQL

The pattern: **Use battle-tested implementations, consensus is hard.**`,

            // ==================== DAY 9: Distributed Systems Principles ====================
            'hld_24': `## Distributed Transactions and 2PC

[STORY]
## The $10 Million Bug

A major bank discovered a terrifying bug: money was disappearing. A transfer from Account A to Account B would debit A, but a network glitch would prevent crediting B. The money vanished into the void.

The fix required **distributed transactions**‚Äîensuring that either BOTH operations complete, or NEITHER does. This is harder than it sounds when databases are on different servers.

> "Distributed transactions are like a wedding: both parties must say 'I do' or the whole thing is off." - Database engineer

## The Problem

\`\`\`
Money transfer: $100 from Account A to Account B

Scenario without distributed transaction:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Debit A: $1000 ‚Üí $900  ‚úì                           ‚îÇ
‚îÇ  2. Network failure...                                  ‚îÇ
‚îÇ  3. Credit B: FAILED ‚úó                                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Result: $100 vanished!                                 ‚îÇ
‚îÇ  A has $900, B unchanged, $100 lost forever            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

We need ATOMICITY: All or nothing!
\`\`\`

## Two-Phase Commit (2PC)

\`\`\`
Coordinator orchestrates distributed commit:

Phase 1: PREPARE (Voting)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Coordinator ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ "Prepare to commit"                 ‚îÇ
‚îÇ       ‚îÇ                                                  ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Participant A: "Can you commit?"      ‚îÇ
‚îÇ       ‚îÇ                  ‚îÇ                               ‚îÇ
‚îÇ       ‚îÇ           ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò "Yes, I can" (PREPARED)      ‚îÇ
‚îÇ       ‚îÇ                                                  ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Participant B: "Can you commit?"      ‚îÇ
‚îÇ                          ‚îÇ                               ‚îÇ
‚îÇ                   ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò "Yes, I can" (PREPARED)      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  All participants hold locks, ready to commit           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 2: COMMIT (Decision)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  If ALL said "Yes":                                     ‚îÇ
‚îÇ  Coordinator ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ "COMMIT"                            ‚îÇ
‚îÇ       ‚îÇ                                                  ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Participant A: Commits, releases lock ‚îÇ
‚îÇ       ‚îÇ                                                  ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Participant B: Commits, releases lock ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  If ANY said "No":                                      ‚îÇ
‚îÇ  Coordinator ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ "ABORT"                             ‚îÇ
‚îÇ       ‚îÇ                                                  ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ All participants: Rollback            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## 2PC Problems

\`\`\`
Problem 1: BLOCKING
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Coordinator sends PREPARE                              ‚îÇ
‚îÇ  Participant A responds: PREPARED (holding locks!)      ‚îÇ
‚îÇ  Coordinator crashes...                                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Participant A is STUCK:                               ‚îÇ
‚îÇ  - Can't commit (no COMMIT message)                     ‚îÇ
‚îÇ  - Can't abort (coordinator might send COMMIT later)    ‚îÇ
‚îÇ  - Holds locks indefinitely (blocking other txns)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Problem 2: COORDINATOR SPOF
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Coordinator is single point of failure                 ‚îÇ
‚îÇ  If it crashes after PREPARE but before COMMIT:         ‚îÇ
‚îÇ  - Participants are in limbo                            ‚îÇ
‚îÇ  - Need coordinator recovery or manual intervention     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Problem 3: LATENCY
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Minimum 4 network round trips:                         ‚îÇ
‚îÇ  1. Coordinator ‚Üí Participants: PREPARE                 ‚îÇ
‚îÇ  2. Participants ‚Üí Coordinator: PREPARED                ‚îÇ
‚îÇ  3. Coordinator ‚Üí Participants: COMMIT                  ‚îÇ
‚îÇ  4. Participants ‚Üí Coordinator: COMMITTED               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  With network latency: SLOW                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Saga Pattern: The Alternative

\`\`\`
Instead of distributed lock, use compensating transactions:

Forward transactions:
1. Debit Account A: $100  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Success
2. Credit Account B: $100 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ FAILED!

Compensating transactions (undo):
1. Credit Account A: $100 ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Compensate!

Result: Both accounts back to original state.
\`\`\`

### Saga: Choreography

\`\`\`
Services communicate via events:

Order ‚îÄ‚îÄ‚ñ∫ Payment ‚îÄ‚îÄ‚ñ∫ Inventory ‚îÄ‚îÄ‚ñ∫ Shipping
  ‚îÇ          ‚îÇ           ‚îÇ            ‚îÇ
  ‚îÇ     OrderCreated     ‚îÇ            ‚îÇ
  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ            ‚îÇ
  ‚îÇ          ‚îÇ      PaymentCompleted  ‚îÇ
  ‚îÇ          ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
  ‚îÇ          ‚îÇ           ‚îÇ      InventoryReserved
  ‚îÇ          ‚îÇ           ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫

If Inventory fails:
  ‚îÇ          ‚îÇ           ‚îÇ
  ‚îÇ          ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ InventoryFailed
  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ RefundPayment
\`\`\`

### Saga: Orchestration

\`\`\`
Central orchestrator coordinates:

        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Orchestrator  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ            ‚îÇ            ‚îÇ
    ‚ñº            ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇPayment‚îÇ   ‚îÇInventory‚îÇ  ‚îÇ Shipping ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Orchestrator:
1. Call Payment.charge()
2. If success: Call Inventory.reserve()
3. If success: Call Shipping.schedule()
4. If ANY fails: Call compensating actions
\`\`\`

## When to Use What

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Use 2PC when:                                          ‚îÇ
‚îÇ  ‚úì Strong consistency absolutely required               ‚îÇ
‚îÇ  ‚úì Transactions are short-lived                         ‚îÇ
‚îÇ  ‚úì Participants are reliable (same datacenter)          ‚îÇ
‚îÇ  ‚úì Acceptable to block on failures                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Use Saga when:                                         ‚îÇ
‚îÇ  ‚úì Long-running transactions                            ‚îÇ
‚îÇ  ‚úì Microservices across networks                        ‚îÇ
‚îÇ  ‚úì Eventual consistency acceptable                      ‚îÇ
‚îÇ  ‚úì Need high availability                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Transaction Best Practices

1. **Prefer Saga over 2PC** for microservices
2. **Keep transactions short** - Minimize lock duration
3. **Design for idempotency** - Retries should be safe
4. **Plan compensation logic** - What's the "undo"?
5. **Use existing frameworks** - Temporal, Conductor, Camunda

[WARNING]
## Transaction Anti-Patterns

1. **2PC across internet** - Latency and failures make it impractical
2. **Long-running 2PC** - Locks held too long
3. **Ignoring compensation failures** - What if undo fails?
4. **No timeout handling** - Transactions stuck forever

[REAL-WORLD]
## How Companies Handle Distributed Transactions

**Amazon:** Saga pattern for order processing
**Uber:** Custom orchestration for ride booking
**Netflix:** Compensating transactions for subscriptions
**Stripe:** Idempotency keys + eventual consistency

The pattern: **Saga for microservices, 2PC only within trusted boundaries.**`,

            'hld_25': `## Eventual Consistency Patterns

[STORY]
## The Amazon Shopping Cart

In 2007, Amazon engineers discovered something counterintuitive: their shopping cart was MORE reliable when they accepted inconsistency. By allowing carts to temporarily diverge across regions, they achieved 99.9999% availability.

When a customer added items from different regions, the carts would merge later. Occasionally an item appeared twice‚Äîbut customers preferred that over "Cart Unavailable."

> "It's better to have slightly wrong data than no data at all." - Amazon engineering principle

## Understanding Eventual Consistency

\`\`\`
Strong Consistency:
Write ‚îÄ‚îÄ‚ñ∫ All replicas updated ‚îÄ‚îÄ‚ñ∫ ACK to client
         (synchronous, slow)

Eventual Consistency:
Write ‚îÄ‚îÄ‚ñ∫ Primary updated ‚îÄ‚îÄ‚ñ∫ ACK to client
                ‚îÇ
                ‚îî‚îÄ‚îÄ‚ñ∫ Replicas updated later (async)

Timeline:
T0: Write x=10 to Primary
T1: Primary ACKs client (x=10 on Primary)
T2: Read from Replica (might return x=5, old value!)
T3: Replication completes
T4: Read from Replica (returns x=10, consistent!)

"Eventually" = Usually milliseconds to seconds
\`\`\`

## Conflict Resolution Strategies

### 1. Last-Write-Wins (LWW)

\`\`\`
Simplest strategy: Latest timestamp wins.

Node A: x = "apple" at T1
Node B: x = "banana" at T2 (T2 > T1)

After merge: x = "banana" (T2 wins)

Problems:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Clock skew: What if T2 is wrong?                       ‚îÇ
‚îÇ  Data loss: "apple" is silently discarded               ‚îÇ
‚îÇ  Concurrent writes: Both might be valid                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Use when: Overwrites are acceptable (like status updates)
\`\`\`

### 2. Vector Clocks

\`\`\`
Track causality, detect conflicts:

Initial: x = {value: 1, clock: {A:0, B:0, C:0}}

Node A writes: x = {value: 10, clock: {A:1, B:0, C:0}}
Node B writes: x = {value: 20, clock: {A:0, B:1, C:0}}

Merge detects CONFLICT (neither clock dominates):
{A:1, B:0} vs {A:0, B:1} = CONCURRENT

Resolution options:
- Keep both (client resolves)
- Merge values (if possible)
- Use application-specific logic
\`\`\`

### 3. CRDTs (Conflict-free Replicated Data Types)

\`\`\`
Data structures that automatically merge without conflicts!

G-Counter (Grow-only counter):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Each node has own counter:                             ‚îÇ
‚îÇ  Node A: 5                                              ‚îÇ
‚îÇ  Node B: 3                                              ‚îÇ
‚îÇ  Node C: 2                                              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Total = sum of all = 10                                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Merge: Take max of each node's counter                 ‚îÇ
‚îÇ  No conflicts possible!                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

G-Set (Grow-only set):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Node A: {apple, banana}                                ‚îÇ
‚îÇ  Node B: {banana, cherry}                               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Merge: Union = {apple, banana, cherry}                 ‚îÇ
‚îÇ  No conflicts!                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

LWW-Element-Set (with removes):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Add set: {(apple, T1), (banana, T2)}                   ‚îÇ
‚îÇ  Remove set: {(apple, T3)}                              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Element in set if: add timestamp > remove timestamp    ‚îÇ
‚îÇ  apple: T1 < T3, so REMOVED                             ‚îÇ
‚îÇ  banana: T2, no remove, so PRESENT                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Application Patterns

### Shopping Cart (Merge on Read)

\`\`\`python
class EventualCart:
    def add_item(self, item, quantity):
        # Each add is a separate event with timestamp
        event = CartEvent(
            type="ADD",
            item=item,
            quantity=quantity,
            timestamp=now(),
            node_id=self.node_id
        )
        self.events.append(event)
    
    def remove_item(self, item):
        event = CartEvent(
            type="REMOVE",
            item=item,
            timestamp=now(),
            node_id=self.node_id
        )
        self.events.append(event)
    
    def get_cart(self):
        # Merge all events to compute current state
        cart = {}
        for event in sorted(self.events, key=lambda e: e.timestamp):
            if event.type == "ADD":
                cart[event.item] = cart.get(event.item, 0) + event.quantity
            elif event.type == "REMOVE":
                cart.pop(event.item, None)
        return cart
\`\`\`

### Like Counter (CRDT)

\`\`\`python
class LikeCounter:
    def __init__(self, num_nodes):
        # Each node has its own counter
        self.counters = {i: 0 for i in range(num_nodes)}
    
    def like(self, node_id):
        self.counters[node_id] += 1
    
    def get_count(self):
        return sum(self.counters.values())
    
    def merge(self, other):
        # Take max of each node's counter
        for node_id in self.counters:
            self.counters[node_id] = max(
                self.counters[node_id],
                other.counters.get(node_id, 0)
            )
\`\`\`

## Read Repair

\`\`\`
Fix inconsistencies during reads:

Client reads from 3 replicas:
  Replica A: x = 10, version 5
  Replica B: x = 10, version 5
  Replica C: x = 8,  version 4  ‚Üê STALE!

Read repair:
  1. Return x = 10 to client (majority)
  2. Background: Update Replica C to version 5
  
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  After repair, all replicas consistent:                 ‚îÇ
‚îÇ  Replica A: x = 10, version 5                          ‚îÇ
‚îÇ  Replica B: x = 10, version 5                          ‚îÇ
‚îÇ  Replica C: x = 10, version 5  ‚Üê Fixed!                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Eventual Consistency Best Practices

1. **Design for conflicts** - They will happen
2. **Make operations commutative** - Order shouldn't matter
3. **Use CRDTs where possible** - Automatic conflict resolution
4. **Implement read repair** - Self-healing consistency
5. **Monitor convergence time** - Know your "eventual" window

[WARNING]
## Eventual Consistency Pitfalls

1. **Assuming immediate consistency** - Reads may be stale
2. **LWW without careful timestamps** - Clock skew causes data loss
3. **Not handling merge conflicts** - Corrupted data
4. **Testing only happy path** - Consistency bugs are subtle

[REAL-WORLD]
## How Companies Handle Eventual Consistency

**Amazon DynamoDB:** Vector clocks, last-write-wins configurable
**Apache Cassandra:** LWW with tunable consistency
**Riak:** CRDTs for counters, sets, maps
**Redis:** CRDT-based for geo-distributed deployments

The pattern: **Embrace eventual consistency, design for conflict resolution.**`,

            'hld_26': `## Distributed Locking

[STORY]
## The Double Booking Disaster

An airline's reservation system allowed two customers to book the same seat. The culprit: two servers checking seat availability simultaneously, both seeing it as available, both confirming the booking.

They needed a **distributed lock**‚Äîa way to ensure only one server could book a seat at a time, even across multiple machines.

> "A lock is a pessimistic assumption that concurrent access will cause problems. In distributed systems, that assumption is usually correct." - Distributed systems engineer

## The Problem

\`\`\`
Without distributed lock:

Server A                     Server B
    ‚îÇ                            ‚îÇ
    ‚îú‚îÄ‚îÄ Check: Seat 15A free? ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ          ‚óÑ‚îÄ‚îÄ‚îÄ Yes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ                            ‚îÇ
    ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Check: Seat 15A free?
    ‚îÇ   ‚îÇ              ‚óÑ‚îÄ‚îÄ‚îÄ Yes
    ‚îÇ   ‚îÇ
    ‚îú‚îÄ‚îÄ Book Seat 15A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ                            ‚îÇ
    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Book Seat 15A ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ                            ‚îÇ
    ‚ñº                            ‚ñº
  BOTH BOOKED THE SAME SEAT!

With distributed lock:

Server A                     Server B
    ‚îÇ                            ‚îÇ
    ‚îú‚îÄ‚îÄ Acquire lock ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ          ‚óÑ‚îÄ‚îÄ‚îÄ OK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ                            ‚îÇ
    ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Acquire lock ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ   ‚îÇ              ‚óÑ‚îÄ‚îÄ‚îÄ WAIT ‚îÇ
    ‚îÇ   ‚îÇ                        ‚îÇ
    ‚îú‚îÄ‚îÄ Check: Seat 15A free? ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ          ‚óÑ‚îÄ‚îÄ‚îÄ Yes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îú‚îÄ‚îÄ Book Seat 15A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îú‚îÄ‚îÄ Release lock ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ                            ‚îÇ
    ‚îÇ   ‚îî‚îÄ‚îÄ Lock acquired ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ       Check: Seat 15A free?‚îÇ
    ‚îÇ              ‚óÑ‚îÄ‚îÄ‚îÄ No ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
\`\`\`

## Implementing Distributed Locks

### 1. Database Lock

\`\`\`sql
-- Simple but limited

-- Acquire lock
INSERT INTO locks (resource_id, owner, expires_at)
VALUES ('seat-15A', 'server-1', NOW() + INTERVAL 30 SECOND)
ON DUPLICATE KEY UPDATE -- Handle race condition
    owner = IF(expires_at < NOW(), VALUES(owner), owner),
    expires_at = IF(expires_at < NOW(), VALUES(expires_at), expires_at);

-- Check if we got the lock
SELECT owner FROM locks WHERE resource_id = 'seat-15A';

-- Release lock
DELETE FROM locks WHERE resource_id = 'seat-15A' AND owner = 'server-1';
\`\`\`

### 2. Redis Lock (SETNX)

\`\`\`python
import redis
import uuid

def acquire_lock(redis_client, resource, ttl_seconds=30):
    lock_id = str(uuid.uuid4())
    acquired = redis_client.set(
        f"lock:{resource}",
        lock_id,
        nx=True,  # Only set if not exists
        ex=ttl_seconds  # Auto-expire
    )
    return lock_id if acquired else None

def release_lock(redis_client, resource, lock_id):
    # Lua script for atomic check-and-delete
    script = """
    if redis.call("get", KEYS[1]) == ARGV[1] then
        return redis.call("del", KEYS[1])
    else
        return 0
    end
    """
    redis_client.eval(script, 1, f"lock:{resource}", lock_id)
\`\`\`

### 3. Redlock Algorithm

\`\`\`
For higher reliability, use multiple Redis instances:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Redlock: Distributed lock across N Redis instances     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  1. Get current time                                    ‚îÇ
‚îÇ  2. Try to acquire lock on ALL N instances              ‚îÇ
‚îÇ  3. Lock acquired if:                                   ‚îÇ
‚îÇ     - Majority (N/2 + 1) instances granted lock         ‚îÇ
‚îÇ     - Total time < lock TTL                             ‚îÇ
‚îÇ  4. If failed, unlock all instances                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚îÇRedis 1 ‚îÇ  ‚îÇRedis 2 ‚îÇ  ‚îÇRedis 3 ‚îÇ  ‚îÇRedis 4 ‚îÇ  ‚îÇRedis 5 ‚îÇ
‚îÇ  ‚îÇ  ‚úì     ‚îÇ  ‚îÇ  ‚úì     ‚îÇ  ‚îÇ  ‚úì     ‚îÇ  ‚îÇ  ‚úó     ‚îÇ  ‚îÇ  ‚úó     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ                                                          ‚îÇ
‚îÇ  3 out of 5 = Majority = Lock acquired!                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### 4. ZooKeeper/etcd Lock

\`\`\`
Using consensus-based systems (most reliable):

ZooKeeper ephemeral sequential nodes:

/locks/seat-15A/
  ‚îú‚îÄ‚îÄ lock-0000000001  (Server A)
  ‚îú‚îÄ‚îÄ lock-0000000002  (Server B)
  ‚îî‚îÄ‚îÄ lock-0000000003  (Server C)

Rules:
1. Create sequential ephemeral node
2. List all children, sort by sequence
3. If your node is first: You have the lock
4. Otherwise: Watch the node before yours
5. When it's deleted: Check if you're now first
6. Lock released when session ends (automatic!)
\`\`\`

## Lock Safety Properties

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SAFETY: Only one client holds lock at a time          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÇ
‚îÇ  If two clients both think they have the lock,         ‚îÇ
‚îÇ  data corruption can occur.                            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  LIVENESS: Lock is eventually released                  ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                ‚îÇ
‚îÇ  Client crashes while holding lock?                     ‚îÇ
‚îÇ  TTL/expiry ensures lock is eventually freed.          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  FAULT TOLERANCE: Lock survives node failures          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÇ
‚îÇ  Redis dies? Redlock uses majority.                     ‚îÇ
‚îÇ  ZooKeeper node dies? Others maintain state.           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Fencing Tokens

\`\`\`
Problem: Lock expires while client is still working

T0: Client A acquires lock, token = 1
T1: Client A pauses (GC, network delay)
T2: Lock expires!
T3: Client B acquires lock, token = 2
T4: Client A resumes, writes to resource
T5: Client B writes to resource

Both wrote! Data corrupted.

Solution: Fencing tokens

Resource accepts writes only if token >= last seen token

T4: Client A writes with token 1
    Resource: "Token 1 < 2, REJECT"
T5: Client B writes with token 2
    Resource: "Token 2 >= 2, ACCEPT"

Safety restored!
\`\`\`

[EXAMPLE]
## Lock Pattern in Practice

\`\`\`python
from contextlib import contextmanager

@contextmanager
def distributed_lock(resource, timeout=30):
    lock_id = acquire_lock(redis_client, resource, timeout)
    if not lock_id:
        raise LockNotAcquiredError(f"Could not acquire lock on {resource}")
    
    try:
        yield lock_id
    finally:
        release_lock(redis_client, resource, lock_id)

# Usage
try:
    with distributed_lock("seat-15A", timeout=10):
        # Only one process executes this at a time
        if is_seat_available("15A"):
            book_seat("15A", customer_id)
except LockNotAcquiredError:
    # Handle contention
    return "Seat being booked, please retry"
\`\`\`

[TIP]
## Distributed Lock Best Practices

1. **Always set TTL** - Prevent deadlocks from crashed clients
2. **Use fencing tokens** - Protect against split-brain
3. **Keep critical section short** - Minimize lock hold time
4. **Handle lock acquisition failure** - Have a fallback
5. **Prefer idempotency over locks** - When possible

[WARNING]
## Distributed Lock Pitfalls

1. **Clock skew** - TTL-based locks can fail with unsynchronized clocks
2. **Long GC pauses** - Lock expires while client thinks it holds it
3. **Network partitions** - Client isolated but holds lock
4. **Forgetting to release** - Always use try/finally
5. **Over-reliance on locks** - They're expensive, use sparingly

[REAL-WORLD]
## How Companies Handle Distributed Locks

**Google:** Chubby (Paxos-based lock service)
**Apache:** ZooKeeper for coordination
**AWS:** DynamoDB conditional writes as locks
**Redis:** Redlock for distributed scenarios

The pattern: **Prefer idempotent operations; use locks only when necessary.**`,

            // ==================== DAY 10: Messaging & Event-Driven Architecture ====================
            'hld_27': `## Message Queues Deep Dive

[STORY]
## The Black Friday Meltdown

In 2016, an e-commerce company's checkout system collapsed on Black Friday. Their synchronous architecture meant every order waited for payment processing, inventory checks, and email confirmations‚Äîall inline. When traffic spiked 10x, everything timed out.

The next year, they introduced **message queues**. Orders were accepted instantly and processed asynchronously. They handled 50x the traffic with the same infrastructure.

> "A queue is a shock absorber for your system‚Äîit smooths out the bumps of variable load." - System architect

## Why Message Queues?

\`\`\`
Synchronous (without queue):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  User ‚îÄ‚îÄ‚ñ∫ Order Service ‚îÄ‚îÄ‚ñ∫ Payment ‚îÄ‚îÄ‚ñ∫ Inventory ‚îÄ‚îÄ‚ñ∫  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Total time: 100ms + 500ms + 200ms = 800ms             ‚îÇ
‚îÇ  If Payment slow: User waits                            ‚îÇ
‚îÇ  If Payment down: Order fails                           ‚îÇ
‚îÇ  Coupling: Order knows about Payment, Inventory        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Asynchronous (with queue):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  User ‚îÄ‚îÄ‚ñ∫ Order Service ‚îÄ‚îÄ‚ñ∫ Queue                      ‚îÇ
‚îÇ                  ‚îÇ            ‚îÇ                         ‚îÇ
‚îÇ                  ‚îÇ            ‚îú‚îÄ‚îÄ‚ñ∫ Payment Consumer     ‚îÇ
‚îÇ                  ‚îÇ            ‚îî‚îÄ‚îÄ‚ñ∫ Inventory Consumer   ‚îÇ
‚îÇ                  ‚îÇ                                       ‚îÇ
‚îÇ  Response time: 100ms (just queue the order)           ‚îÇ
‚îÇ  If Payment slow: Processes in background               ‚îÇ
‚îÇ  If Payment down: Messages wait in queue               ‚îÇ
‚îÇ  Decoupling: Order doesn't know consumers              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Queue Models

### Point-to-Point (Queue)

\`\`\`
One message consumed by ONE consumer.

Producer ‚îÄ‚îÄ‚ñ∫ [Queue] ‚îÄ‚îÄ‚ñ∫ Consumer A
                         Consumer B (waiting)
                         Consumer C (waiting)

Use case: Task distribution (each task done once)
Example: Order processing, job scheduling
\`\`\`

### Publish-Subscribe (Topic)

\`\`\`
One message consumed by ALL subscribers.

                    ‚îå‚îÄ‚îÄ‚ñ∫ Subscriber A (gets copy)
Producer ‚îÄ‚îÄ‚ñ∫ [Topic]‚îú‚îÄ‚îÄ‚ñ∫ Subscriber B (gets copy)
                    ‚îî‚îÄ‚îÄ‚ñ∫ Subscriber C (gets copy)

Use case: Event broadcasting
Example: User signup ‚Üí Email, Analytics, Welcome bonus
\`\`\`

## Delivery Guarantees

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AT-MOST-ONCE                                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                          ‚îÇ
‚îÇ  Producer sends, doesn't wait for ACK                   ‚îÇ
‚îÇ  Message may be lost if broker fails                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Use: Metrics, logs (some loss acceptable)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AT-LEAST-ONCE                                          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                          ‚îÇ
‚îÇ  Producer retries until ACK received                    ‚îÇ
‚îÇ  Message may be delivered multiple times                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Use: Most applications (with idempotent consumers)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  EXACTLY-ONCE                                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                            ‚îÇ
‚îÇ  Very hard to achieve in practice                       ‚îÇ
‚îÇ  Usually "effectively once" via idempotency             ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Use: Financial transactions (rare, expensive)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Message Queue Comparison

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Queue     ‚îÇ  Characteristics                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  RabbitMQ  ‚îÇ  Traditional broker, AMQP protocol          ‚îÇ
‚îÇ            ‚îÇ  Complex routing, exchanges, bindings       ‚îÇ
‚îÇ            ‚îÇ  Good for: Task queues, RPC                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  AWS SQS   ‚îÇ  Fully managed, highly available            ‚îÇ
‚îÇ            ‚îÇ  Simple API, auto-scaling                   ‚îÇ
‚îÇ            ‚îÇ  Good for: AWS ecosystem, simplicity        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Kafka     ‚îÇ  Distributed log, high throughput           ‚îÇ
‚îÇ            ‚îÇ  Message retention, replay capability       ‚îÇ
‚îÇ            ‚îÇ  Good for: Event streaming, analytics       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Redis     ‚îÇ  In-memory, very fast                       ‚îÇ
‚îÇ            ‚îÇ  Simple pub/sub, lists as queues            ‚îÇ
‚îÇ            ‚îÇ  Good for: Real-time, low-latency           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Handling Failures

### Dead Letter Queue (DLQ)

\`\`\`
When messages repeatedly fail processing:

Main Queue ‚îÄ‚îÄ‚ñ∫ Consumer ‚îÄ‚îÄ‚ñ∫ FAIL (3 times)
                   ‚îÇ
                   ‚îî‚îÄ‚îÄ‚ñ∫ Dead Letter Queue
                              ‚îÇ
                              ‚ñº
                        Manual review
                        or automated fix
\`\`\`

### Idempotent Consumers

\`\`\`python
def process_order(message):
    order_id = message['order_id']
    
    # Check if already processed
    if redis.get(f"processed:{order_id}"):
        logger.info(f"Order {order_id} already processed, skipping")
        return
    
    # Process the order
    create_order(message)
    
    # Mark as processed
    redis.set(f"processed:{order_id}", "1", ex=86400)  # 24h TTL

# Now safe to receive duplicates!
\`\`\`

### Poison Message Handling

\`\`\`
Messages that can never be processed:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Set max retry count (e.g., 3)                       ‚îÇ
‚îÇ  2. Track retry count in message metadata               ‚îÇ
‚îÇ  3. After max retries: Move to DLQ                      ‚îÇ
‚îÇ  4. Alert operations team                               ‚îÇ
‚îÇ  5. Fix bug or data issue                               ‚îÇ
‚îÇ  6. Replay messages from DLQ                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Message Queue Best Practices

1. **Make consumers idempotent** - Duplicates will happen
2. **Set message TTL** - Don't let queues grow forever
3. **Use DLQ** - Don't lose failed messages
4. **Monitor queue depth** - Growing queue = problem
5. **Keep messages small** - Store large data elsewhere, reference it

[WARNING]
## Message Queue Pitfalls

1. **Queue as database** - Queues are for transit, not storage
2. **Ignoring order** - Many queues don't guarantee order
3. **Large messages** - Kill performance, use references
4. **No monitoring** - Silent failures
5. **Tight coupling via message format** - Use schemas

[REAL-WORLD]
## How Companies Use Queues

**Amazon:** SQS for decoupling services (billions/day)
**Uber:** Kafka for event streaming, RabbitMQ for tasks
**Slack:** Redis for real-time, Kafka for persistence
**Netflix:** Kafka for everything (700B+ events/day)

The pattern: **Queues for decoupling and resilience, not as primary storage.**`,

            'hld_28': `## Event Streaming with Kafka

[STORY]
## LinkedIn's Data Problem

In 2011, LinkedIn was drowning in data pipelines. They had dozens of point-to-point connections: databases to data warehouses, services to analytics, logs to monitoring. Each new connection added complexity exponentially.

They built **Apache Kafka**‚Äîa distributed streaming platform that became the single source of truth for all data movement. Today, Kafka powers trillions of messages daily across thousands of companies.

> "Kafka is not just a message queue‚Äîit's a distributed commit log that fundamentally changes how you think about data." - Jay Kreps, Kafka co-creator

## Kafka Architecture

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    KAFKA CLUSTER                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  Producers ‚îÄ‚îÄ‚ñ∫ TOPIC: user-events                       ‚îÇ
‚îÇ                    ‚îÇ                                     ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ    ‚îÇ               ‚îÇ               ‚îÇ                    ‚îÇ
‚îÇ    ‚ñº               ‚ñº               ‚ñº                    ‚îÇ
‚îÇ Partition 0   Partition 1   Partition 2                 ‚îÇ
‚îÇ [0,1,2,3]     [0,1,2,3]     [0,1,2,3]                  ‚îÇ
‚îÇ    ‚îÇ               ‚îÇ               ‚îÇ                    ‚îÇ
‚îÇ    ‚ñº               ‚ñº               ‚ñº                    ‚îÇ
‚îÇ Consumer Group A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                   ‚îÇ
‚îÇ    ‚îÇ               ‚îÇ               ‚îÇ                    ‚îÇ
‚îÇ Consumer 1    Consumer 2    Consumer 3                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Key Concepts:
- Topic: Category/feed of messages
- Partition: Ordered, immutable log
- Offset: Position in partition
- Consumer Group: Coordinated consumers
\`\`\`

## Why Kafka is Different

\`\`\`
Traditional Queue:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Producer ‚îÄ‚îÄ‚ñ∫ [Queue] ‚îÄ‚îÄ‚ñ∫ Consumer                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Message consumed = Message deleted                     ‚îÇ
‚îÇ  Can't replay, can't have multiple consumers           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Kafka (Distributed Log):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Producer ‚îÄ‚îÄ‚ñ∫ [Log: 0,1,2,3,4,5,6,7,8,9,...]          ‚îÇ
‚îÇ                    ‚ñ≤     ‚ñ≤        ‚ñ≤                     ‚îÇ
‚îÇ                    ‚îÇ     ‚îÇ        ‚îÇ                     ‚îÇ
‚îÇ               Consumer A ‚îÇ   Consumer C                 ‚îÇ
‚îÇ               (offset 2) ‚îÇ   (offset 7)                 ‚îÇ
‚îÇ                    Consumer B                           ‚îÇ
‚îÇ                    (offset 5)                           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Messages RETAINED (configurable: hours, days, forever)‚îÇ
‚îÇ  Multiple consumers at different positions             ‚îÇ
‚îÇ  Can REPLAY from any offset                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Partitions and Ordering

\`\`\`
Ordering guarantee: WITHIN a partition, never across

Topic: orders
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Partition 0: [Order1, Order4, Order7]  ‚Üê Ordered!     ‚îÇ
‚îÇ  Partition 1: [Order2, Order5, Order8]  ‚Üê Ordered!     ‚îÇ
‚îÇ  Partition 2: [Order3, Order6, Order9]  ‚Üê Ordered!     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Partition assignment (by key):
order.user_id = "user123" ‚Üí hash("user123") % 3 = 1 ‚Üí Partition 1

All orders for user123 go to same partition = Ordered!
Different users may be on different partitions = Parallel!
\`\`\`

## Consumer Groups

\`\`\`
Consumer Group: Consumers that share the load

Topic with 6 partitions:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  P0  P1  P2  P3  P4  P5                               ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ                                ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ
‚îÇ       ‚îÇ           ‚îÇ                                     ‚îÇ
‚îÇ   Consumer 1  Consumer 2     (Group: analytics)        ‚îÇ
‚îÇ   (handles    (handles                                 ‚îÇ
‚îÇ    P0,P1,P2)   P3,P4,P5)                               ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ALSO:                                                 ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ
‚îÇ           ‚îÇ                                             ‚îÇ
‚îÇ       Consumer A             (Group: search-indexer)   ‚îÇ
‚îÇ       (handles ALL)                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Each group gets ALL messages independently!
Within a group, partitions are distributed.
\`\`\`

## Kafka Guarantees

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DURABILITY                                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ  Messages replicated across brokers                     ‚îÇ
‚îÇ  Configurable replication factor (usually 3)            ‚îÇ
‚îÇ  Survives broker failures                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ORDERING                                               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                               ‚îÇ
‚îÇ  Guaranteed within partition                            ‚îÇ
‚îÇ  Use partition key for related messages                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  DELIVERY                                               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                               ‚îÇ
‚îÇ  At-least-once by default                               ‚îÇ
‚îÇ  Exactly-once with transactions (Kafka Streams)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  RETENTION                                              ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                              ‚îÇ
‚îÇ  Time-based (e.g., 7 days)                              ‚îÇ
‚îÇ  Size-based (e.g., 1TB per partition)                   ‚îÇ
‚îÇ  Compacted (keep latest per key)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Producer Example

\`\`\`python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['kafka:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    acks='all',  # Wait for all replicas
    retries=3
)

# Send with key (ensures ordering per user)
producer.send(
    topic='user-events',
    key=b'user123',  # Partition key
    value={
        'event': 'purchase',
        'user_id': 'user123',
        'amount': 99.99,
        'timestamp': '2024-01-15T10:30:00Z'
    }
)

producer.flush()  # Wait for all messages to be sent
\`\`\`

## Consumer Example

\`\`\`python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'user-events',
    bootstrap_servers=['kafka:9092'],
    group_id='analytics-processor',
    auto_offset_reset='earliest',  # Start from beginning
    enable_auto_commit=False,  # Manual commit for reliability
    value_deserializer=lambda v: json.loads(v.decode('utf-8'))
)

for message in consumer:
    try:
        process_event(message.value)
        consumer.commit()  # Commit after successful processing
    except Exception as e:
        logger.error(f"Failed to process: {e}")
        # Don't commit, will retry on restart
\`\`\`

[TIP]
## Kafka Best Practices

1. **Choose partition keys wisely** - Related events same partition
2. **Set appropriate retention** - Balance storage vs replay needs
3. **Use consumer groups** - For scalable processing
4. **Monitor lag** - Consumer lag = falling behind
5. **Compact topics for state** - Keep latest value per key

[WARNING]
## Kafka Pitfalls

1. **Too few partitions** - Can't parallelize enough
2. **Too many partitions** - Overhead, leader elections slow
3. **Large messages** - Use references, not inline data
4. **Ignoring consumer lag** - Data piles up silently
5. **No schema management** - Use Avro/Protobuf + Schema Registry

[REAL-WORLD]
## How Companies Use Kafka

**LinkedIn:** 7T+ messages/day, 100+ clusters
**Netflix:** 700B+ events/day for recommendations
**Uber:** Real-time pricing, surge detection
**Spotify:** User activity, playlist updates

The pattern: **Kafka as central nervous system for data.**`,

            'hld_29': `## Event-Driven Architecture Patterns

[STORY]
## Netflix's Microservices Evolution

Netflix's monolith became impossible to maintain. Changes in one area broke others. Deployments were nightmares. They moved to microservices, but discovered a new problem: services were tightly coupled through synchronous HTTP calls.

Their solution? **Event-driven architecture**. Services communicate through events, not direct calls. A user action triggers events that ripple through the system, each service reacting independently.

> "In event-driven systems, you don't call services‚Äîyou announce what happened and let interested parties react." - Netflix engineer

## Event-Driven vs Request-Driven

\`\`\`
Request-Driven (Imperative):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Order Service says:                                    ‚îÇ
‚îÇ  "Hey Payment, charge this customer"                    ‚îÇ
‚îÇ  "Hey Inventory, reserve these items"                   ‚îÇ
‚îÇ  "Hey Shipping, schedule delivery"                      ‚îÇ
‚îÇ  "Hey Email, send confirmation"                         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Order knows about everyone. Tightly coupled.          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Event-Driven (Reactive):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Order Service announces:                               ‚îÇ
‚îÇ  "OrderCreated" event                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Payment hears it: Charges customer                     ‚îÇ
‚îÇ  Inventory hears it: Reserves items                     ‚îÇ
‚îÇ  Shipping hears it: Schedules delivery                  ‚îÇ
‚îÇ  Email hears it: Sends confirmation                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Order doesn't know who's listening. Loosely coupled.  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Key Patterns

### 1. Event Sourcing

\`\`\`
Store events, not state. Derive state from events.

Traditional:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Account Table                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                ‚îÇ
‚îÇ  ‚îÇ account  ‚îÇ balance ‚îÇ                                ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                                ‚îÇ
‚îÇ  ‚îÇ A123     ‚îÇ $500    ‚îÇ  ‚Üê Current state only          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Event Sourced:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Event Log                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ 1. AccountCreated(A123)                   ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ 2. MoneyDeposited(A123, $1000)            ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ 3. MoneyWithdrawn(A123, $300)             ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ 4. MoneyWithdrawn(A123, $200)             ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Current balance = 0 + 1000 - 300 - 200 = $500         ‚îÇ
‚îÇ  Complete history preserved!                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
‚úì Complete audit trail
‚úì Can replay to any point in time
‚úì Can derive new views from events
‚úì Natural fit for event-driven systems
\`\`\`

### 2. CQRS (Command Query Responsibility Segregation)

\`\`\`
Separate read and write models:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Commands (Write)              Queries (Read)           ‚îÇ
‚îÇ       ‚îÇ                              ‚îÇ                   ‚îÇ
‚îÇ       ‚ñº                              ‚ñº                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ Command ‚îÇ                   ‚îÇ Query Model ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ Handler ‚îÇ                   ‚îÇ (Optimized  ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ  for reads) ‚îÇ          ‚îÇ
‚îÇ       ‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ       ‚ñº                              ‚ñ≤                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Events        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ Event   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Projector ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ Store   ‚îÇ                   ‚îÇ (Updates  ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ  read DB) ‚îÇ           ‚îÇ
‚îÇ                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Write: Optimized for consistency (normalized)
Read: Optimized for queries (denormalized, cached)
\`\`\`

### 3. Choreography vs Orchestration

\`\`\`
CHOREOGRAPHY: Services react to events independently

OrderCreated ‚îÄ‚îÄ‚ñ∫ Payment listens ‚îÄ‚îÄ‚ñ∫ PaymentCompleted
                                           ‚îÇ
                 Inventory listens ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
                 InventoryReserved ‚îÄ‚îÄ‚ñ∫ Shipping listens
                                              ‚îÇ
                                              ‚ñº
                                        ShipmentScheduled

Pros: Loose coupling, services are independent
Cons: Hard to see full flow, debugging difficult


ORCHESTRATION: Central coordinator directs flow

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Order Saga                           ‚îÇ
‚îÇ                   (Orchestrator)                        ‚îÇ
‚îÇ                        ‚îÇ                                ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ    ‚îÇ                   ‚îÇ                   ‚îÇ           ‚îÇ
‚îÇ    ‚ñº                   ‚ñº                   ‚ñº           ‚îÇ
‚îÇ Payment            Inventory           Shipping        ‚îÇ
‚îÇ Service            Service             Service         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ Orchestrator knows the flow, handles failures          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Pros: Clear flow, easier to monitor and debug
Cons: Single point of failure, more coupling
\`\`\`

## Saga Pattern for Distributed Transactions

\`\`\`
Order Saga (Orchestration style):

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Happy Path:                                            ‚îÇ
‚îÇ  1. Create Order (pending)                              ‚îÇ
‚îÇ  2. Reserve Inventory ‚úì                                 ‚îÇ
‚îÇ  3. Charge Payment ‚úì                                    ‚îÇ
‚îÇ  4. Confirm Order                                       ‚îÇ
‚îÇ  5. Schedule Shipping                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Failure at step 3 (Payment fails):                     ‚îÇ
‚îÇ  1. Create Order (pending) ‚úì                            ‚îÇ
‚îÇ  2. Reserve Inventory ‚úì                                 ‚îÇ
‚îÇ  3. Charge Payment ‚úó FAILED                             ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Compensating transactions:                             ‚îÇ
‚îÇ  2c. Release Inventory (undo step 2)                    ‚îÇ
‚îÇ  1c. Cancel Order (undo step 1)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Event Schema Evolution

\`\`\`
Events are forever‚Äîhandle schema changes carefully!

Version 1:
{
  "event": "UserCreated",
  "userId": "123",
  "name": "John"
}

Version 2 (additive‚ÄîSAFE):
{
  "event": "UserCreated",
  "userId": "123",
  "name": "John",
  "email": "john@example.com"  // NEW field, optional
}

Version 3 (breaking‚ÄîDANGEROUS):
{
  "event": "UserCreated",
  "userId": "123",
  "fullName": "John Doe"  // RENAMED field!
  // Old consumers break!
}

Rule: Add fields, never remove or rename.
Use Schema Registry to enforce compatibility.
\`\`\`

[TIP]
## Event-Driven Best Practices

1. **Design events as facts** - "OrderCreated", not "CreateOrder"
2. **Make events immutable** - Never modify, only append new ones
3. **Include enough context** - Consumer shouldn't need to query back
4. **Version your events** - Schema will evolve
5. **Idempotent consumers** - Events may be delivered multiple times

[WARNING]
## Event-Driven Pitfalls

1. **Event soup** - Too many fine-grained events
2. **Tight coupling via events** - Consumer depends on producer internals
3. **No event schema** - Leads to integration nightmares
4. **Ignoring event ordering** - Causality matters
5. **Synchronous mindset** - Expecting immediate consistency

[REAL-WORLD]
## How Companies Use Event-Driven

**Netflix:** Microservices communicate via events
**Uber:** Real-time pricing, trip matching
**LinkedIn:** Activity feed, notifications
**Shopify:** Order processing, inventory management

The pattern: **Events as first-class citizens, choreography + orchestration hybrid.**`,

            // ==================== DAY 11: Reliability & Resilience ====================
            'hld_30': `## Designing for Failure

[STORY]
## The Chaos Monkey Philosophy

In 2011, Netflix did something crazy: they built a tool that randomly kills their production servers. **Chaos Monkey** runs during business hours, terminating instances without warning.

Why? Because they learned that the only way to build resilient systems is to constantly test failure. "The best way to avoid failure is to fail constantly."

> "Everything fails, all the time." - Werner Vogels, Amazon CTO

## Failure is Inevitable

\`\`\`
Murphy's Law of Distributed Systems:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  If you have 99.9% reliable components:                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  1 component:   99.9% uptime                            ‚îÇ
‚îÇ  10 components: 99.0% uptime (99.9^10)                  ‚îÇ
‚îÇ  100 components: 90.5% uptime                           ‚îÇ
‚îÇ  1000 components: 36.8% uptime                          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  More components = More failures = Design for it!      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Types of Failures

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CRASH FAILURES                                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                        ‚îÇ
‚îÇ  Server dies, process crashes                           ‚îÇ
‚îÇ  Solution: Redundancy, health checks, auto-restart     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  OMISSION FAILURES                                      ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                      ‚îÇ
‚îÇ  Messages lost, timeouts                                ‚îÇ
‚îÇ  Solution: Retries, acknowledgments, idempotency       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  TIMING FAILURES                                        ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                        ‚îÇ
‚îÇ  Response too slow, clock skew                          ‚îÇ
‚îÇ  Solution: Timeouts, async processing                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  BYZANTINE FAILURES                                     ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                     ‚îÇ
‚îÇ  Malicious or buggy nodes send wrong data              ‚îÇ
‚îÇ  Solution: Consensus protocols, checksums              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Resilience Patterns

### 1. Redundancy

\`\`\`
Don't depend on single instances:

Single Point of Failure:
User ‚îÄ‚îÄ‚ñ∫ [Single Server] ‚îÄ‚îÄ‚ñ∫ Crash = OUTAGE

Redundant:
                ‚îå‚îÄ‚îÄ‚ñ∫ Server 1 ‚îÄ‚îÄ‚îê
User ‚îÄ‚îÄ‚ñ∫ [LB] ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ Server 2 ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ Database Cluster
                ‚îî‚îÄ‚îÄ‚ñ∫ Server 3 ‚îÄ‚îÄ‚îò
                      (Primary + Replicas)

One server dies? Others handle traffic.
\`\`\`

### 2. Failover

\`\`\`
Automatic switch to backup:

Active-Passive:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Primary (Active)                                       ‚îÇ
‚îÇ       ‚îÇ                                                  ‚îÇ
‚îÇ       ‚îÇ heartbeat                                       ‚îÇ
‚îÇ       ‚ñº                                                  ‚îÇ
‚îÇ  Secondary (Passive/Standby)                           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Primary fails ‚Üí Secondary becomes Active               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Active-Active:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Server A (Active) ‚óÑ‚îÄ‚îÄ‚ñ∫ Server B (Active)              ‚îÇ
‚îÇ       ‚îÇ                        ‚îÇ                        ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Load Balancer ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Either can handle requests. One fails? Other continues.‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### 3. Graceful Degradation

\`\`\`
Partial functionality > Complete failure

Netflix example:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Normal: Personalized recommendations                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Recommendation service down:                           ‚îÇ
‚îÇ  ‚Üí Show trending/popular content instead                ‚îÇ
‚îÇ  ‚Üí User still gets a usable experience                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Profile service slow:                                  ‚îÇ
‚îÇ  ‚Üí Show cached profile data                             ‚îÇ
‚îÇ  ‚Üí Allow watching, queue profile updates                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### 4. Bulkhead Pattern

\`\`\`
Isolate failures, prevent cascade:

Without bulkhead:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Shared Thread Pool (100 threads)                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Service A (slow) uses 95 threads                       ‚îÇ
‚îÇ  Service B (healthy) gets 5 threads                     ‚îÇ
‚îÇ  Service C (healthy) gets 0 threads ‚Üê STARVED!         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

With bulkhead:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Service A Pool (30 threads) ‚Üê Isolated!               ‚îÇ
‚îÇ  Service B Pool (30 threads) ‚Üê Protected               ‚îÇ
‚îÇ  Service C Pool (30 threads) ‚Üê Protected               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Service A slow? Only Service A suffers.               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### 5. Health Checks

\`\`\`python
# Liveness: Is the process running?
@app.route('/health/live')
def liveness():
    return {'status': 'alive'}, 200

# Readiness: Can it handle requests?
@app.route('/health/ready')
def readiness():
    if not database.is_connected():
        return {'status': 'not ready', 'reason': 'db down'}, 503
    if not cache.is_connected():
        return {'status': 'not ready', 'reason': 'cache down'}, 503
    return {'status': 'ready'}, 200
\`\`\`

## Chaos Engineering

\`\`\`
Principles:

1. Define steady state (normal behavior)
2. Hypothesize: System handles failure gracefully
3. Introduce failure: Kill server, add latency, corrupt data
4. Observe: Did it match hypothesis?
5. Fix: If not, improve resilience

Netflix Chaos Tools:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Chaos Monkey: Kills random instances                  ‚îÇ
‚îÇ  Chaos Kong: Kills entire AWS region                   ‚îÇ
‚îÇ  Latency Monkey: Adds network delays                   ‚îÇ
‚îÇ  Chaos Gorilla: Kills entire availability zone         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Design for Failure Best Practices

1. **Assume everything fails** - Design with failure in mind
2. **Fail fast** - Quick timeouts, don't wait forever
3. **Fail gracefully** - Partial function > no function
4. **Practice failure** - Regular chaos engineering
5. **Monitor everything** - Can't fix what you can't see

[WARNING]
## Failure Handling Anti-Patterns

1. **Ignoring errors** - Silent failures are the worst
2. **Infinite retries** - Can cascade failures
3. **No timeouts** - Threads stuck forever
4. **Single points of failure** - One thing dies, everything dies
5. **Only testing happy path** - Production finds your bugs

[REAL-WORLD]
## How Companies Handle Failures

**Netflix:** Chaos engineering pioneers
**Amazon:** Cell-based architecture for blast radius
**Google:** SRE practices, error budgets
**Spotify:** Squad-based ownership, post-mortems

The pattern: **Expect failure, design for it, practice it.**`,

            'hld_31': `## Circuit Breaker Pattern

[STORY]
## The Cascade That Brought Down Amazon

In 2012, a single overloaded service at Amazon created a cascading failure. Service A called B, B called C, C was slow. A kept retrying, exhausting its thread pool. A became slow, affecting upstream services. Within minutes, dozens of services were down.

The fix? **Circuit breakers**‚Äîlike electrical circuit breakers that trip to prevent fires, software circuit breakers prevent cascading failures.

> "The circuit breaker pattern is the immune system for distributed systems." - Michael Nygard

## The Problem: Cascading Failures

\`\`\`
Without circuit breaker:

Service A ‚îÄ‚îÄ‚ñ∫ Service B ‚îÄ‚îÄ‚ñ∫ Service C (SLOW!)
    ‚îÇ              ‚îÇ              ‚îÇ
    ‚îÇ  Waiting...  ‚îÇ  Waiting...  ‚îÇ  Slow...
    ‚îÇ              ‚îÇ              ‚îÇ
    ‚îÇ  Threads     ‚îÇ  Threads     ‚îÇ
    ‚îÇ  exhausted   ‚îÇ  exhausted   ‚îÇ
    ‚îÇ              ‚îÇ              ‚îÇ
    ‚ñº              ‚ñº              ‚ñº
  DEAD           DEAD          SLOW

One slow service kills everything!
\`\`\`

## Circuit Breaker States

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    failures     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ   ‚îÇ  CLOSED  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   OPEN   ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ (Normal) ‚îÇ                 ‚îÇ (Failing)‚îÇ            ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ        ‚ñ≤                            ‚îÇ                   ‚îÇ
‚îÇ        ‚îÇ                       timeout                  ‚îÇ
‚îÇ        ‚îÇ                            ‚îÇ                   ‚îÇ
‚îÇ        ‚îÇ                            ‚ñº                   ‚îÇ
‚îÇ        ‚îÇ      success        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ HALF-OPEN  ‚îÇ            ‚îÇ
‚îÇ                              ‚îÇ  (Testing) ‚îÇ            ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                    ‚îÇ                    ‚îÇ
‚îÇ                               failure                   ‚îÇ
‚îÇ                                    ‚îÇ                    ‚îÇ
‚îÇ                                    ‚ñº                    ‚îÇ
‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ                              ‚îÇ   OPEN   ‚îÇ              ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

CLOSED: Requests pass through normally
OPEN: Requests fail immediately (fast fail)
HALF-OPEN: Limited requests to test if service recovered
\`\`\`

## How It Works

\`\`\`
Timeline:

T0: Circuit CLOSED, requests flowing
T1: Service B starts failing
T2: 5 failures in 10 seconds ‚Üí Trip threshold!
T3: Circuit OPENS
    ‚îÇ
    ‚îÇ All requests to B fail IMMEDIATELY
    ‚îÇ (No waiting, no thread exhaustion)
    ‚îÇ
T4: After 30 seconds, circuit enters HALF-OPEN
T5: Send 1 test request to B
    ‚îÇ
    ‚îú‚îÄ‚ñ∫ B responds OK ‚Üí Circuit CLOSES (back to normal)
    ‚îÇ
    ‚îî‚îÄ‚ñ∫ B still failing ‚Üí Circuit OPENS (wait more)
\`\`\`

## Implementation

\`\`\`python
from enum import Enum
from datetime import datetime, timedelta
import threading

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    def __init__(
        self,
        failure_threshold=5,
        recovery_timeout=30,
        half_open_requests=1
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.half_open_requests = half_open_requests
        
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.last_failure_time = None
        self.half_open_successes = 0
        self.lock = threading.Lock()
    
    def call(self, func, *args, **kwargs):
        with self.lock:
            if self.state == CircuitState.OPEN:
                if self._should_try_reset():
                    self.state = CircuitState.HALF_OPEN
                    self.half_open_successes = 0
                else:
                    raise CircuitOpenError("Circuit is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise
    
    def _should_try_reset(self):
        return (
            datetime.now() - self.last_failure_time
            > timedelta(seconds=self.recovery_timeout)
        )
    
    def _on_success(self):
        with self.lock:
            if self.state == CircuitState.HALF_OPEN:
                self.half_open_successes += 1
                if self.half_open_successes >= self.half_open_requests:
                    self.state = CircuitState.CLOSED
                    self.failure_count = 0
    
    def _on_failure(self):
        with self.lock:
            self.failure_count += 1
            self.last_failure_time = datetime.now()
            
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.OPEN
            elif self.failure_count >= self.failure_threshold:
                self.state = CircuitState.OPEN
\`\`\`

## Using Circuit Breakers

\`\`\`python
# Create circuit breaker for each dependency
payment_cb = CircuitBreaker(failure_threshold=5, recovery_timeout=30)
inventory_cb = CircuitBreaker(failure_threshold=3, recovery_timeout=60)

def process_order(order):
    try:
        # Circuit breaker wraps the call
        payment_result = payment_cb.call(
            payment_service.charge,
            order.customer_id,
            order.amount
        )
    except CircuitOpenError:
        # Fast fail - don't even try
        return {"status": "payment_unavailable", "retry_after": 30}
    except PaymentError as e:
        # Actual payment failure
        return {"status": "payment_failed", "error": str(e)}
    
    try:
        inventory_result = inventory_cb.call(
            inventory_service.reserve,
            order.items
        )
    except CircuitOpenError:
        # Compensate - refund payment
        payment_service.refund(payment_result.transaction_id)
        return {"status": "inventory_unavailable"}
\`\`\`

## Circuit Breaker Libraries

\`\`\`
Popular implementations:

Python:
  - pybreaker
  - circuitbreaker
  - resilience4j (via Py4J)

Java:
  - Resilience4j (recommended)
  - Hystrix (Netflix, deprecated)

Node.js:
  - opossum
  - brakes

Go:
  - sony/gobreaker
  - hystrix-go
\`\`\`

[TIP]
## Circuit Breaker Best Practices

1. **Tune thresholds** - Too sensitive = flapping, too lenient = slow detection
2. **Monitor state changes** - Alert on circuit opens
3. **Fallback behavior** - What happens when circuit opens?
4. **Different breakers per dependency** - Isolate failures
5. **Combine with timeouts** - Circuit breaker + timeout = comprehensive protection

[WARNING]
## Circuit Breaker Pitfalls

1. **No fallback** - Circuit opens, user sees error
2. **Shared circuit** - One bad endpoint trips all
3. **Too short timeout** - Never recovers
4. **Not monitoring** - Silent failures
5. **Testing only happy path** - Circuit logic untested

[REAL-WORLD]
## How Companies Use Circuit Breakers

**Netflix:** Hystrix (now Resilience4j) everywhere
**Amazon:** Built into their service mesh
**Uber:** Per-endpoint circuit breakers
**Spotify:** Combined with retry policies

The pattern: **Fail fast, recover gracefully, protect the system.**`,

            'hld_32': `## Rate Limiting & Throttling

[STORY]
## The Twitter Fail Whale

In Twitter's early days, viral events would bring down the entire site. The famous "Fail Whale" error page became a symbol of system overload. The solution? **Rate limiting**‚Äîprotecting the system by limiting how many requests each user could make.

Today, rate limiting is essential for APIs, preventing abuse, ensuring fair usage, and keeping systems stable.

> "Rate limiting is the bouncer at your API's door‚Äîit decides who gets in and how often." - API engineer

## Why Rate Limiting?

\`\`\`
Without rate limiting:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Malicious user sends 10,000 requests/second           ‚îÇ
‚îÇ  Bot scrapes your entire database                       ‚îÇ
‚îÇ  Viral event causes 100x traffic spike                  ‚îÇ
‚îÇ  One customer consumes all resources                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Result: System crashes, everyone suffers               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

With rate limiting:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Malicious user: BLOCKED after 100 requests            ‚îÇ
‚îÇ  Bot: SLOWED DOWN to 1 request/second                  ‚îÇ
‚îÇ  Traffic spike: QUEUED, processed gradually            ‚îÇ
‚îÇ  Greedy customer: Limited to fair share                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Result: System stays healthy for everyone             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Rate Limiting Algorithms

### 1. Token Bucket

\`\`\`
Bucket fills with tokens at fixed rate.
Request needs token to proceed.
No token? Request denied.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Bucket capacity: 10 tokens                             ‚îÇ
‚îÇ  Refill rate: 1 token/second                            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  T0: [ü™ôü™ôü™ôü™ôü™ôü™ôü™ôü™ôü™ôü™ô] 10 tokens                    ‚îÇ
‚îÇ  T1: Request! [ü™ôü™ôü™ôü™ôü™ôü™ôü™ôü™ôü™ô] 9 tokens              ‚îÇ
‚îÇ  T2: Request! [ü™ôü™ôü™ôü™ôü™ôü™ôü™ôü™ô] 8 tokens               ‚îÇ
‚îÇ  ...                                                     ‚îÇ
‚îÇ  T9: Request! [ü™ô] 1 token                              ‚îÇ
‚îÇ  T10: Request! [] DENIED! (no tokens)                  ‚îÇ
‚îÇ  T11: Refill! [ü™ô] 1 token (can make request)          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Allows bursts (up to bucket size) but limits sustained ‚îÇ
‚îÇ  rate to refill rate.                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### 2. Leaky Bucket

\`\`\`
Requests enter bucket, processed at fixed rate.
Bucket overflow? Request denied.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                         ‚îÇ
‚îÇ  ‚îÇ ‚óã ‚óã ‚óã ‚óã ‚óã  ‚îÇ ‚Üê Requests enter (any rate)            ‚îÇ
‚îÇ  ‚îÇ ‚óã ‚óã ‚óã ‚óã    ‚îÇ                                         ‚îÇ
‚îÇ  ‚îÇ ‚óã ‚óã ‚óã      ‚îÇ                                         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                         ‚îÇ
‚îÇ        ‚îÇ leak (fixed rate: 1/second)                    ‚îÇ
‚îÇ        ‚ñº                                                 ‚îÇ
‚îÇ     Process                                             ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Smooths out bursts‚Äîoutput is always at fixed rate     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### 3. Fixed Window Counter

\`\`\`
Count requests in fixed time windows.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Window: 1 minute, Limit: 100 requests                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  12:00:00-12:00:59: [|||||||||...] 85 requests ‚úì       ‚îÇ
‚îÇ  12:01:00-12:01:59: [||||||||||...] 100 requests ‚úì     ‚îÇ
‚îÇ  12:02:00-12:02:59: [|||] 3 requests ‚úì                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Problem: Burst at window boundary                      ‚îÇ
‚îÇ  12:00:59: 100 requests                                 ‚îÇ
‚îÇ  12:01:00: 100 requests (new window!)                   ‚îÇ
‚îÇ  = 200 requests in 2 seconds üò±                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### 4. Sliding Window Log

\`\`\`
Track timestamp of each request.
Count requests in rolling window.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Window: 1 minute, Limit: 100                           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Log: [12:00:05, 12:00:10, 12:00:45, 12:01:02, ...]    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  At 12:01:30, count requests from 12:00:30 to 12:01:30 ‚îÇ
‚îÇ  Remove old entries, count remaining                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Precise but memory-intensive (stores all timestamps)   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### 5. Sliding Window Counter

\`\`\`
Combines fixed window + weighted average.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Previous window: 70 requests                           ‚îÇ
‚îÇ  Current window: 30 requests (40% into window)         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Weighted count = 70 * 0.6 + 30 = 72                   ‚îÇ
‚îÇ  (60% of previous + 100% of current)                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Good balance of accuracy and memory efficiency         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Implementation with Redis

\`\`\`python
import redis
import time

class RateLimiter:
    def __init__(self, redis_client, limit=100, window=60):
        self.redis = redis_client
        self.limit = limit
        self.window = window
    
    def is_allowed(self, user_id):
        """Token bucket using Redis"""
        key = f"rate_limit:{user_id}"
        now = time.time()
        
        # Lua script for atomic operations
        script = """
        local tokens_key = KEYS[1]
        local timestamp_key = KEYS[2]
        local rate = tonumber(ARGV[1])
        local capacity = tonumber(ARGV[2])
        local now = tonumber(ARGV[3])
        local requested = tonumber(ARGV[4])
        
        local last_time = tonumber(redis.call('get', timestamp_key) or now)
        local tokens = tonumber(redis.call('get', tokens_key) or capacity)
        
        local delta = math.max(0, now - last_time)
        local new_tokens = math.min(capacity, tokens + delta * rate)
        
        if new_tokens >= requested then
            new_tokens = new_tokens - requested
            redis.call('set', tokens_key, new_tokens)
            redis.call('set', timestamp_key, now)
            return 1
        end
        return 0
        """
        
        return self.redis.eval(
            script, 2,
            f"{key}:tokens", f"{key}:ts",
            self.limit / self.window,  # refill rate
            self.limit,  # capacity
            now,
            1  # tokens requested
        )
\`\`\`

## Rate Limiting Headers

\`\`\`
Standard response headers:

HTTP/1.1 200 OK
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 42
X-RateLimit-Reset: 1640000000
Retry-After: 30

HTTP/1.1 429 Too Many Requests
{
  "error": "Rate limit exceeded",
  "limit": 100,
  "remaining": 0,
  "reset": 1640000000
}
\`\`\`

## Rate Limiting Strategies

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  By User/API Key:                                       ‚îÇ
‚îÇ  Free tier: 100 req/hour                                ‚îÇ
‚îÇ  Paid tier: 10,000 req/hour                             ‚îÇ
‚îÇ  Enterprise: 100,000 req/hour                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  By IP Address:                                         ‚îÇ
‚îÇ  Anonymous: 10 req/minute                               ‚îÇ
‚îÇ  (Careful: NAT may have many users behind one IP)       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  By Endpoint:                                           ‚îÇ
‚îÇ  /api/search: 10 req/second (expensive)                ‚îÇ
‚îÇ  /api/users: 100 req/second (cheap)                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Global:                                                ‚îÇ
‚îÇ  Total system capacity: 1M req/second                  ‚îÇ
‚îÇ  Protect against DDoS and viral events                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Rate Limiting Best Practices

1. **Communicate limits clearly** - Document and return headers
2. **Use appropriate algorithm** - Token bucket for API, leaky bucket for queues
3. **Implement at multiple levels** - Edge, gateway, application
4. **Grace period for bursts** - Don't be too strict
5. **Monitor and alert** - Track rate limit hits

[WARNING]
## Rate Limiting Pitfalls

1. **Too strict** - Frustrates legitimate users
2. **Only at application** - DDoS hits before app
3. **No Retry-After header** - Clients retry immediately
4. **Single point of rate limit** - Becomes bottleneck
5. **Forgetting distributed rate limiting** - Each server has separate limits

[REAL-WORLD]
## How Companies Handle Rate Limiting

**Twitter:** Tiered limits by endpoint and user type
**GitHub:** 5000 req/hour authenticated, 60 unauthenticated
**Stripe:** Separate limits for different API operations
**AWS API Gateway:** Built-in rate limiting + throttling

The pattern: **Protect your system, communicate limits, be fair.**`,

            // ==================== DAY 12: Observability ====================
            'hld_33': `## Logging, Metrics, and Tracing

[STORY]
## The Mystery of the Slow Checkout

An e-commerce site had a critical problem: checkout was slow, but only sometimes, only for some users, only on certain products. Traditional debugging was useless‚Äîthe bug was a ghost.

The solution came from **distributed tracing**. By following a single request through 20+ microservices, they found it: a rarely-used discount service was calling an external API that occasionally took 10 seconds. Mystery solved.

> "Observability is not about collecting data‚Äîit's about being able to ask any question about your system and get an answer." - Charity Majors

## The Three Pillars

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 OBSERVABILITY                            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ    ‚îÇ  LOGS    ‚îÇ   ‚îÇ METRICS  ‚îÇ   ‚îÇ TRACES   ‚îÇ          ‚îÇ
‚îÇ    ‚îÇ (Events) ‚îÇ   ‚îÇ (Numbers)‚îÇ   ‚îÇ (Flows)  ‚îÇ          ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ    What        How much/         Where did              ‚îÇ
‚îÇ    happened?   how fast?         the request go?        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Logging

\`\`\`
Discrete events with context:

{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "ERROR",
  "service": "payment-service",
  "trace_id": "abc123",
  "user_id": "user456",
  "message": "Payment failed",
  "error": "Card declined",
  "card_type": "visa",
  "amount": 99.99
}

Log Levels:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DEBUG:   Detailed debugging info (dev only)           ‚îÇ
‚îÇ  INFO:    Normal operations (user login, order placed) ‚îÇ
‚îÇ  WARN:    Unexpected but handled (retry succeeded)     ‚îÇ
‚îÇ  ERROR:   Failure requiring attention (payment failed) ‚îÇ
‚îÇ  FATAL:   System crash, immediate action needed        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Structured Logging

\`\`\`python
import structlog

log = structlog.get_logger()

def process_order(order):
    log.info(
        "processing_order",
        order_id=order.id,
        user_id=order.user_id,
        total=order.total,
        items_count=len(order.items)
    )
    
    try:
        result = payment.charge(order)
        log.info(
            "payment_successful",
            order_id=order.id,
            transaction_id=result.transaction_id
        )
    except PaymentError as e:
        log.error(
            "payment_failed",
            order_id=order.id,
            error=str(e),
            card_type=order.card_type
        )
        raise
\`\`\`

## Metrics

\`\`\`
Numeric measurements over time:

Types:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COUNTER: Only goes up                                 ‚îÇ
‚îÇ    requests_total = 1,234,567                          ‚îÇ
‚îÇ    errors_total = 123                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  GAUGE: Current value, can go up/down                  ‚îÇ
‚îÇ    temperature = 72.5                                   ‚îÇ
‚îÇ    active_connections = 42                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  HISTOGRAM: Distribution of values                     ‚îÇ
‚îÇ    request_duration_seconds                            ‚îÇ
‚îÇ    bucket{le="0.1"} = 1000 (1000 req under 100ms)     ‚îÇ
‚îÇ    bucket{le="0.5"} = 1500 (1500 req under 500ms)     ‚îÇ
‚îÇ    bucket{le="1.0"} = 1550 (1550 req under 1s)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  SUMMARY: Similar to histogram, calculates percentiles ‚îÇ
‚îÇ    request_duration{quantile="0.99"} = 0.8            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Key Metrics (RED Method)

\`\`\`
For request-driven services:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  R - Rate: Requests per second                         ‚îÇ
‚îÇ      How much traffic are we handling?                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  E - Errors: Failed requests per second                ‚îÇ
‚îÇ      What's breaking?                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  D - Duration: Distribution of request times           ‚îÇ
‚îÇ      How long are requests taking?                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Key Metrics (USE Method)

\`\`\`
For resources (CPU, memory, disk):

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  U - Utilization: % time resource is busy              ‚îÇ
‚îÇ      CPU at 80%? Memory at 90%?                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  S - Saturation: Work queued up                        ‚îÇ
‚îÇ      Run queue length, swap usage                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  E - Errors: Error count                               ‚îÇ
‚îÇ      Disk errors, network timeouts                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Distributed Tracing

\`\`\`
Following a request across services:

User Request: "Add to Cart"
Trace ID: abc-123

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Service        ‚îÇ Span          ‚îÇ Duration ‚îÇ Status     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ API Gateway    ‚îÇ ‚ñà‚ñà‚ñà‚ñà          ‚îÇ 250ms    ‚îÇ OK         ‚îÇ
‚îÇ   ‚îî Auth       ‚îÇ  ‚ñà‚ñà           ‚îÇ 50ms     ‚îÇ OK         ‚îÇ
‚îÇ   ‚îî Cart       ‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     ‚îÇ 180ms    ‚îÇ OK         ‚îÇ
‚îÇ     ‚îî User     ‚îÇ   ‚ñà‚ñà          ‚îÇ 30ms     ‚îÇ OK         ‚îÇ
‚îÇ     ‚îî Inventory‚îÇ   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      ‚îÇ 120ms    ‚îÇ OK         ‚îÇ
‚îÇ       ‚îî DB     ‚îÇ    ‚ñà‚ñà‚ñà‚ñà       ‚îÇ 80ms     ‚îÇ SLOW!      ‚îÇ
‚îÇ     ‚îî Cache    ‚îÇ   ‚ñà           ‚îÇ 5ms      ‚îÇ OK         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Found it! Database query in Inventory service is slow.
\`\`\`

### Trace Context

\`\`\`python
# Trace propagation
import opentelemetry.trace as trace

tracer = trace.get_tracer(__name__)

@app.route('/api/cart/add')
def add_to_cart():
    with tracer.start_as_current_span("add_to_cart") as span:
        span.set_attribute("user_id", request.user_id)
        span.set_attribute("product_id", request.product_id)
        
        # Trace propagates to downstream calls
        inventory_result = inventory_service.check(
            product_id=request.product_id,
            # Trace context automatically included in headers
        )
        
        if not inventory_result.available:
            span.set_status(trace.StatusCode.ERROR)
            return {"error": "Out of stock"}, 400
\`\`\`

## Correlation

\`\`\`
Connect logs, metrics, and traces:

Request comes in:
1. Generate trace_id = "abc-123"
2. Include trace_id in all logs
3. Tag all metrics with trace_id
4. Pass trace_id to downstream services

Now you can:
- See metrics ‚Üí drill into traces
- See error log ‚Üí find full trace
- See slow trace ‚Üí find relevant logs
\`\`\`

[TIP]
## Observability Best Practices

1. **Structured logging** - JSON, not plain text
2. **Consistent labels** - Same names across services
3. **High cardinality is OK for traces** - User IDs, request IDs
4. **Sample traces in production** - Don't trace 100% of requests
5. **Correlate everything** - Use trace IDs across all pillars

[WARNING]
## Observability Pitfalls

1. **Too much logging** - Noise drowns signal
2. **Metrics without context** - "Errors up" but which endpoint?
3. **No trace sampling** - Overwhelms storage
4. **Siloed tools** - Logs here, metrics there, no correlation
5. **Alert fatigue** - Too many alerts = ignored alerts

[REAL-WORLD]
## How Companies Handle Observability

**Google:** Dapper (inspired OpenTelemetry)
**Uber:** Jaeger for tracing
**Netflix:** Atlas for metrics
**Twitter:** Zipkin for distributed tracing

The pattern: **Logs for details, metrics for dashboards, traces for debugging.**`,

            'hld_34': `## Alerting and On-Call

[STORY]
## The Alert That Cried Wolf

A team had 500 alerts configured. Their pagers went off constantly‚Äîmostly false positives. When a real outage happened, the on-call engineer ignored it, assuming it was another false alarm. The outage lasted 4 hours.

They rebuilt their alerting from scratch: only 20 alerts, each one actionable. Alert fatigue disappeared, and response times dropped to minutes.

> "Every alert should either wake someone up or it shouldn't exist." - Rob Ewaschuk, Google SRE

## What Makes a Good Alert?

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GOOD ALERTS                                            ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                            ‚îÇ
‚îÇ  ‚úì Actionable: You can do something about it           ‚îÇ
‚îÇ  ‚úì Urgent: Requires immediate attention                ‚îÇ
‚îÇ  ‚úì Symptoms, not causes: "Users can't checkout"        ‚îÇ
‚îÇ  ‚úì Low false-positive rate: Trust the alert            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  BAD ALERTS                                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ  ‚úó Informational: "Deployment completed"               ‚îÇ
‚îÇ  ‚úó Not actionable: "CPU at 80%" (so what?)            ‚îÇ
‚îÇ  ‚úó Too sensitive: Fires on minor blips                ‚îÇ
‚îÇ  ‚úó Cause-based: "Server X down" (but service is fine) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Alert on Symptoms, Not Causes

\`\`\`
BAD (Cause-based):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Alert: "Database server CPU > 90%"                    ‚îÇ
‚îÇ  Problem: CPU might be fine at 90%                     ‚îÇ
‚îÇ  Problem: Multiple alerts for same incident            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

GOOD (Symptom-based):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Alert: "Checkout error rate > 5%"                     ‚îÇ
‚îÇ  Why: Directly impacts users                           ‚îÇ
‚îÇ  Why: One alert per user-facing issue                  ‚îÇ
‚îÇ  Why: Don't care what caused it initially              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## SLO-Based Alerting

\`\`\`
Define Service Level Objectives, alert on burn rate:

SLO: 99.9% availability (error budget: 0.1%)
     43.2 minutes of downtime per month allowed

Burn Rate Alert:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1-hour burn rate > 14.4x                              ‚îÇ
‚îÇ  = Burning 14.4x faster than budget allows             ‚îÇ
‚îÇ  = Will exhaust monthly budget in ~2 days              ‚îÇ
‚îÇ  ‚Üí PAGE immediately                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  6-hour burn rate > 6x                                 ‚îÇ
‚îÇ  = Burning 6x faster than budget allows                ‚îÇ
‚îÇ  = Will exhaust monthly budget in ~5 days              ‚îÇ
‚îÇ  ‚Üí Ticket, investigate soon                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Alert Severity Levels

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Severity  ‚îÇ Response                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ P1/SEV1   ‚îÇ Page immediately, drop everything          ‚îÇ
‚îÇ           ‚îÇ "Site is down, users can't buy"           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ P2/SEV2   ‚îÇ Page if during business hours             ‚îÇ
‚îÇ           ‚îÇ "Checkout slow for 10% of users"          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ P3/SEV3   ‚îÇ Ticket, fix this week                     ‚îÇ
‚îÇ           ‚îÇ "Non-critical feature degraded"           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ P4/SEV4   ‚îÇ Ticket, fix when possible                 ‚îÇ
‚îÇ           ‚îÇ "Minor UI issue, workaround exists"       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## On-Call Best Practices

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ROTATION                                               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                               ‚îÇ
‚îÇ  ‚Ä¢ Rotate weekly (long enough to learn)                 ‚îÇ
‚îÇ  ‚Ä¢ Primary + Secondary on-call                          ‚îÇ
‚îÇ  ‚Ä¢ Follow the sun for global teams                      ‚îÇ
‚îÇ  ‚Ä¢ Compensate fairly (time off, pay)                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  RUNBOOKS                                               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                               ‚îÇ
‚îÇ  ‚Ä¢ Every alert has a runbook link                       ‚îÇ
‚îÇ  ‚Ä¢ Step-by-step diagnosis                               ‚îÇ
‚îÇ  ‚Ä¢ Common fixes and escalation paths                    ‚îÇ
‚îÇ  ‚Ä¢ Keep runbooks updated                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ESCALATION                                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ  ‚Ä¢ Clear escalation path                                ‚îÇ
‚îÇ  ‚Ä¢ Auto-escalate if not ACKed in 5 minutes              ‚îÇ
‚îÇ  ‚Ä¢ Know when to page leadership                         ‚îÇ
‚îÇ  ‚Ä¢ No shame in escalating                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Runbook Example

\`\`\`markdown
# Alert: Checkout Error Rate > 5%

## Severity: P1

## Description
User-facing checkout is failing above acceptable threshold.

## Impact
Users cannot complete purchases. Revenue loss.

## Diagnosis Steps
1. Check Grafana dashboard: [link]
2. Check recent deployments: [link]
3. Check downstream services:
   - Payment Service: [status page]
   - Inventory Service: [status page]
4. Check database health: [dashboard]

## Common Causes & Fixes
1. **Payment provider outage**
   - Check payment provider status
   - If down, enable backup provider: \`feature-flag payment-backup\`

2. **Recent bad deployment**
   - Check deploy timeline
   - Rollback: \`./scripts/rollback.sh checkout-service\`

3. **Database connection exhaustion**
   - Check connection pool metrics
   - Restart service if needed: \`kubectl rollout restart\`

## Escalation
- If not resolved in 15 minutes: Page checkout team lead
- If not resolved in 30 minutes: Page engineering director
\`\`\`

## Post-Incident Review

\`\`\`
Blameless post-mortems:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TIMELINE                                               ‚îÇ
‚îÇ  ‚Ä¢ When did it start?                                   ‚îÇ
‚îÇ  ‚Ä¢ When was it detected?                                ‚îÇ
‚îÇ  ‚Ä¢ When was it resolved?                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  IMPACT                                                 ‚îÇ
‚îÇ  ‚Ä¢ Users affected?                                      ‚îÇ
‚îÇ  ‚Ä¢ Revenue lost?                                        ‚îÇ
‚îÇ  ‚Ä¢ SLO impact?                                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ROOT CAUSE                                             ‚îÇ
‚îÇ  ‚Ä¢ What actually broke?                                 ‚îÇ
‚îÇ  ‚Ä¢ Why wasn't it caught earlier?                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ACTION ITEMS                                           ‚îÇ
‚îÇ  ‚Ä¢ How do we prevent recurrence?                        ‚îÇ
‚îÇ  ‚Ä¢ How do we detect faster?                             ‚îÇ
‚îÇ  ‚Ä¢ How do we recover faster?                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Alerting Best Practices

1. **Start with symptoms** - User impact first
2. **Use SLO-based alerts** - Aligned with business goals
3. **Every alert needs a runbook** - Actionable guidance
4. **Review alerts quarterly** - Delete stale ones
5. **Track alert quality** - False positive rate

[WARNING]
## Alerting Anti-Patterns

1. **Alert on everything** - Creates noise, ignored
2. **No runbooks** - On-call scrambles blindly
3. **Alerting on causes** - CPU/memory without context
4. **Not testing alerts** - Find out they're broken during outage
5. **Blame culture** - People hide problems

[REAL-WORLD]
## How Companies Handle On-Call

**Google:** SLO-based alerting, error budgets
**PagerDuty:** Tiered escalation, auto-acknowledge
**Netflix:** "What broke, not who broke it"
**Datadog:** Anomaly detection + static thresholds

The pattern: **Actionable alerts, runbooks, blameless post-mortems.**`,

            'hld_35': `## APM and Performance Monitoring

[STORY]
## The 100ms That Cost $1 Million

Amazon discovered that every 100ms of latency cost them 1% in sales. Google found that a half-second delay reduced search traffic by 20%. Performance isn't just about user experience‚Äîit's about revenue.

**Application Performance Monitoring (APM)** gives you the visibility to find and fix performance issues before they cost you money.

> "Performance is a feature." - Jeff Atwood

## What is APM?

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  APM = Application Performance Monitoring              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Combines:                                              ‚îÇ
‚îÇ  ‚Ä¢ Distributed tracing (where is it slow?)             ‚îÇ
‚îÇ  ‚Ä¢ Metrics (how slow? how often?)                       ‚îÇ
‚îÇ  ‚Ä¢ Profiling (why is it slow?)                         ‚îÇ
‚îÇ  ‚Ä¢ Error tracking (what's breaking?)                   ‚îÇ
‚îÇ  ‚Ä¢ User experience (real user impact)                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Goal: Understand application performance end-to-end   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Key APM Metrics

\`\`\`
APDEX Score (Application Performance Index):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Threshold T = 500ms (you define "satisfactory")       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Satisfied:   response < T (< 500ms)                   ‚îÇ
‚îÇ  Tolerating:  response < 4T (< 2s)                     ‚îÇ
‚îÇ  Frustrated:  response >= 4T (>= 2s)                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  APDEX = (Satisfied + Tolerating/2) / Total            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Score 0.94 = "Excellent"                              ‚îÇ
‚îÇ  Score 0.85 = "Good"                                   ‚îÇ
‚îÇ  Score 0.70 = "Fair"                                   ‚îÇ
‚îÇ  Score < 0.50 = "Poor"                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Percentiles:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  p50 (median): 50% of requests faster than this        ‚îÇ
‚îÇ  p90: 90% of requests faster than this                 ‚îÇ
‚îÇ  p99: 99% of requests faster than this                 ‚îÇ
‚îÇ  p99.9: 99.9% faster (the long tail)                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Example:                                               ‚îÇ
‚îÇ  p50 = 100ms (most users happy)                        ‚îÇ
‚îÇ  p99 = 2000ms (1% of users waiting 2+ seconds!)        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Don't just look at averages‚Äîlook at percentiles!      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Real User Monitoring (RUM)

\`\`\`
Measure actual user experience:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Browser collects:                                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Navigation Timing:                                     ‚îÇ
‚îÇ  DNS Lookup ‚îÄ‚ñ∫ TCP Connect ‚îÄ‚ñ∫ TLS ‚îÄ‚ñ∫ Request ‚îÄ‚ñ∫ Response‚îÇ
‚îÇ      ‚îÇ              ‚îÇ          ‚îÇ        ‚îÇ         ‚îÇ     ‚îÇ
‚îÇ    50ms           30ms       40ms     100ms     200ms   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Paint Timing:                                          ‚îÇ
‚îÇ  First Contentful Paint (FCP): 800ms                   ‚îÇ
‚îÇ  Largest Contentful Paint (LCP): 1.2s                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Interactivity:                                         ‚îÇ
‚îÇ  First Input Delay (FID): 50ms                         ‚îÇ
‚îÇ  Time to Interactive (TTI): 2.5s                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Layout Stability:                                      ‚îÇ
‚îÇ  Cumulative Layout Shift (CLS): 0.1                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Core Web Vitals

\`\`\`
Google's user experience metrics:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LCP (Largest Contentful Paint)                        ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                          ‚îÇ
‚îÇ  Loading performance                                    ‚îÇ
‚îÇ  Good: < 2.5s | Needs Improvement: 2.5-4s | Poor: > 4s ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  FID (First Input Delay)                               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                ‚îÇ
‚îÇ  Interactivity                                          ‚îÇ
‚îÇ  Good: < 100ms | Needs Improvement: 100-300ms | Poor: > 300ms‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  CLS (Cumulative Layout Shift)                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚îÇ
‚îÇ  Visual stability                                       ‚îÇ
‚îÇ  Good: < 0.1 | Needs Improvement: 0.1-0.25 | Poor: > 0.25‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Profiling

\`\`\`
Find WHERE time is spent:

CPU Profile (flame graph):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà main()        ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà processRequest()               ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà parseJSON()                             ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà validateData()                                ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà queryDatabase()           ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà executeQuery()       ‚Üê HOTSPOT!   ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà processResults()                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Memory Profile:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Object Type       ‚îÇ Count    ‚îÇ Size                   ‚îÇ
‚îÇ  String            ‚îÇ 1.2M     ‚îÇ 45MB                   ‚îÇ
‚îÇ  ArrayList         ‚îÇ 500K     ‚îÇ 30MB   ‚Üê Memory hog?   ‚îÇ
‚îÇ  HashMap$Node      ‚îÇ 2M       ‚îÇ 80MB   ‚Üê Memory leak?  ‚îÇ
‚îÇ  byte[]            ‚îÇ 100K     ‚îÇ 200MB                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## APM Tools

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tool           ‚îÇ Strengths                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Datadog        ‚îÇ Full stack, great visualizations       ‚îÇ
‚îÇ New Relic      ‚îÇ Deep APM, AI insights                  ‚îÇ
‚îÇ Dynatrace      ‚îÇ Auto-discovery, AI-powered             ‚îÇ
‚îÇ AppDynamics    ‚îÇ Business transaction focus             ‚îÇ
‚îÇ Elastic APM    ‚îÇ Open source, ELK integration          ‚îÇ
‚îÇ Jaeger         ‚îÇ Open source tracing                    ‚îÇ
‚îÇ Prometheus     ‚îÇ Metrics (pair with Grafana)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Performance Optimization Flow

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. MEASURE                                             ‚îÇ
‚îÇ     Set baseline, identify slow endpoints              ‚îÇ
‚îÇ                    ‚îÇ                                    ‚îÇ
‚îÇ                    ‚ñº                                    ‚îÇ
‚îÇ  2. TRACE                                               ‚îÇ
‚îÇ     Find which service/operation is slow               ‚îÇ
‚îÇ                    ‚îÇ                                    ‚îÇ
‚îÇ                    ‚ñº                                    ‚îÇ
‚îÇ  3. PROFILE                                             ‚îÇ
‚îÇ     Find which code/query is slow                      ‚îÇ
‚îÇ                    ‚îÇ                                    ‚îÇ
‚îÇ                    ‚ñº                                    ‚îÇ
‚îÇ  4. OPTIMIZE                                            ‚îÇ
‚îÇ     Fix the bottleneck                                  ‚îÇ
‚îÇ                    ‚îÇ                                    ‚îÇ
‚îÇ                    ‚ñº                                    ‚îÇ
‚îÇ  5. VERIFY                                              ‚îÇ
‚îÇ     Confirm improvement with same metrics              ‚îÇ
‚îÇ                    ‚îÇ                                    ‚îÇ
‚îÇ                    ‚ñº                                    ‚îÇ
‚îÇ  6. REPEAT                                              ‚îÇ
‚îÇ     Performance is a continuous process                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## APM Best Practices

1. **Measure before optimizing** - Data beats intuition
2. **Focus on p99** - Tail latency hurts users
3. **Set performance budgets** - "This page must load in 2s"
4. **Automate performance testing** - Catch regressions early
5. **Monitor real users** - Synthetic tests aren't enough

[WARNING]
## APM Pitfalls

1. **Averages lie** - Use percentiles
2. **Over-instrumentation** - Monitoring overhead
3. **Ignoring cold starts** - First request is different
4. **Not correlating with business metrics** - Is it impacting revenue?
5. **Alert on metrics, not symptoms** - "p99 up" vs "users frustrated"

[REAL-WORLD]
## How Companies Handle APM

**Amazon:** Custom APM, every service tracked
**Google:** Dapper (open-sourced concepts in Jaeger)
**Netflix:** Atlas + Mantis for real-time streaming
**Uber:** Custom APM for ride latency

The pattern: **Measure everything, profile the slow stuff, optimize what matters.**`,

            // ==================== DAY 13: Security ====================
            'hld_36': `## Authentication and Authorization

[STORY]
## The Password Breach That Changed Everything

In 2012, LinkedIn suffered a massive breach: 6.5 million password hashes leaked. The problem? They used SHA-1 without salt. Attackers cracked millions of passwords in hours. This single incident changed how the industry thinks about authentication.

> "Security is not a feature‚Äîit's a foundation. Build it in from day one." - Security engineer

## Authentication vs Authorization

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AUTHENTICATION (AuthN)                                 ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                  ‚îÇ
‚îÇ  "Who are you?"                                         ‚îÇ
‚îÇ  Verify identity                                        ‚îÇ
‚îÇ  Examples: Password, fingerprint, OAuth                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  AUTHORIZATION (AuthZ)                                  ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                  ‚îÇ
‚îÇ  "What can you do?"                                     ‚îÇ
‚îÇ  Verify permissions                                     ‚îÇ
‚îÇ  Examples: Role-based access, ACLs                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Flow:
User ‚îÄ‚îÄ‚ñ∫ Authenticate ‚îÄ‚îÄ‚ñ∫ Authorize ‚îÄ‚îÄ‚ñ∫ Access Resource
         "I'm Alice"     "Alice can    "Here's the
                          read docs"    document"
\`\`\`

## Authentication Methods

### Session-Based Authentication

\`\`\`
Traditional approach:

1. Login
Client ‚îÄ‚îÄ‚ñ∫ POST /login {username, password}
Server ‚îÄ‚îÄ‚ñ∫ Verify credentials
Server ‚îÄ‚îÄ‚ñ∫ Create session, store in database
Server ‚îÄ‚îÄ‚ñ∫ Return session cookie

2. Subsequent requests
Client ‚îÄ‚îÄ‚ñ∫ Request + Cookie (session_id=abc123)
Server ‚îÄ‚îÄ‚ñ∫ Look up session in database
Server ‚îÄ‚îÄ‚ñ∫ Return response

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pros:                                                  ‚îÇ
‚îÇ  ‚úì Server controls sessions (can invalidate)           ‚îÇ
‚îÇ  ‚úì Simple to implement                                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Cons:                                                  ‚îÇ
‚îÇ  ‚úó Server must store all sessions (scaling issue)      ‚îÇ
‚îÇ  ‚úó Sticky sessions or shared storage needed            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Token-Based Authentication (JWT)

\`\`\`
Modern approach:

1. Login
Client ‚îÄ‚îÄ‚ñ∫ POST /login {username, password}
Server ‚îÄ‚îÄ‚ñ∫ Verify credentials
Server ‚îÄ‚îÄ‚ñ∫ Generate JWT, sign with secret
Server ‚îÄ‚îÄ‚ñ∫ Return JWT to client

2. Subsequent requests
Client ‚îÄ‚îÄ‚ñ∫ Request + Header "Authorization: Bearer <jwt>"
Server ‚îÄ‚îÄ‚ñ∫ Verify JWT signature
Server ‚îÄ‚îÄ‚ñ∫ Extract claims from JWT
Server ‚îÄ‚îÄ‚ñ∫ Return response

JWT Structure:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Header.Payload.Signature                               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Header:  {"alg": "HS256", "typ": "JWT"}               ‚îÇ
‚îÇ  Payload: {"sub": "user123", "role": "admin", exp: ...}‚îÇ
‚îÇ  Signature: HMACSHA256(header + payload, secret)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pros:                                                  ‚îÇ
‚îÇ  ‚úì Stateless (server stores nothing)                   ‚îÇ
‚îÇ  ‚úì Scales horizontally                                 ‚îÇ
‚îÇ  ‚úì Works across domains                                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Cons:                                                  ‚îÇ
‚îÇ  ‚úó Can't easily revoke tokens                          ‚îÇ
‚îÇ  ‚úó Token size larger than session ID                   ‚îÇ
‚îÇ  ‚úó Must handle token refresh                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### OAuth 2.0 and OpenID Connect

\`\`\`
Delegated authorization:

"Login with Google" flow:

User ‚îÄ‚îÄ‚ñ∫ App: "Login with Google"
  ‚îÇ
  ‚ñº
App ‚îÄ‚îÄ‚ñ∫ Google: "User wants to login"
  ‚îÇ
  ‚ñº
Google ‚îÄ‚îÄ‚ñ∫ User: "Allow this app?"
  ‚îÇ
  ‚ñº
User ‚îÄ‚îÄ‚ñ∫ Google: "Yes, allow"
  ‚îÇ
  ‚ñº
Google ‚îÄ‚îÄ‚ñ∫ App: Authorization code
  ‚îÇ
  ‚ñº
App ‚îÄ‚îÄ‚ñ∫ Google: Exchange code for tokens
  ‚îÇ
  ‚ñº
Google ‚îÄ‚îÄ‚ñ∫ App: Access token + ID token (OIDC)
  ‚îÇ
  ‚ñº
App ‚îÄ‚îÄ‚ñ∫ User: Logged in!

OAuth = Authorization (what can app access?)
OIDC = Authentication (who is the user?)
\`\`\`

## Authorization Models

### Role-Based Access Control (RBAC)

\`\`\`
Assign permissions to roles, assign roles to users:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Roles:                                                 ‚îÇ
‚îÇ    admin: [create, read, update, delete]               ‚îÇ
‚îÇ    editor: [create, read, update]                      ‚îÇ
‚îÇ    viewer: [read]                                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Users:                                                 ‚îÇ
‚îÇ    alice: admin                                         ‚îÇ
‚îÇ    bob: editor                                          ‚îÇ
‚îÇ    charlie: viewer                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Check: user.role.includes(required_permission)
\`\`\`

### Attribute-Based Access Control (ABAC)

\`\`\`
Fine-grained control based on attributes:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Policy: "Doctors can view patient records in their    ‚îÇ
‚îÇ           department during business hours"            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Attributes:                                            ‚îÇ
‚îÇ    user.role == "doctor" AND                           ‚îÇ
‚îÇ    resource.department == user.department AND          ‚îÇ
‚îÇ    current_time BETWEEN 9am AND 5pm                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

More flexible but more complex than RBAC.
\`\`\`

## Password Security

\`\`\`
NEVER store plain text passwords!

Bad:  password ‚Üí store in database
Bad:  MD5(password) ‚Üí store hash
Bad:  SHA256(password) ‚Üí still too fast to crack!

Good: bcrypt(password, salt, cost_factor) ‚Üí store

Bcrypt features:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚úì Built-in salt (unique per password)                 ‚îÇ
‚îÇ  ‚úì Cost factor (makes it slow, adjustable)             ‚îÇ
‚îÇ  ‚úì Designed to be slow (resists brute force)           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Example:                                               ‚îÇ
‚îÇ  bcrypt.hash("password123", 12)                        ‚îÇ
‚îÇ  = "$2b$12$R9h/cIPz0gi.URNNX3kh2O..."                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Cost 12 = ~300ms per hash (attackers can't brute force)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Auth Best Practices

1. **Use bcrypt/argon2 for passwords** - Never MD5/SHA1
2. **Implement MFA** - Passwords alone aren't enough
3. **Short-lived tokens** - Access tokens expire quickly
4. **Refresh token rotation** - New refresh token each use
5. **Least privilege** - Minimal permissions needed

[WARNING]
## Auth Anti-Patterns

1. **Storing plain passwords** - Instant breach
2. **JWT without expiration** - Token valid forever
3. **Secrets in code** - Use environment variables
4. **No rate limiting on login** - Enables brute force
5. **Rolling your own crypto** - Use proven libraries

[REAL-WORLD]
## How Companies Handle Auth

**Google:** OAuth 2.0 provider, OIDC, MFA everywhere
**Auth0:** Managed authentication service
**Okta:** Enterprise identity management
**AWS Cognito:** Serverless auth with federation

The pattern: **OAuth for delegation, JWT for APIs, bcrypt for passwords.**`,

            'hld_37': `## Encryption and Data Protection

[STORY]
## The Equifax Breach

In 2017, Equifax exposed 147 million people's data. The root cause? An unpatched Apache Struts vulnerability. But the damage was amplified because sensitive data wasn't encrypted at rest. Social Security numbers, birth dates, addresses‚Äîall in plain text.

> "Encryption is the last line of defense. When everything else fails, encrypted data stays protected." - Security researcher

## Encryption Types

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SYMMETRIC ENCRYPTION                                   ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚îÇ
‚îÇ  Same key for encrypt and decrypt                       ‚îÇ
‚îÇ  Fast, used for bulk data                               ‚îÇ
‚îÇ  Examples: AES-256, ChaCha20                           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Encrypt: AES(plaintext, key) ‚Üí ciphertext             ‚îÇ
‚îÇ  Decrypt: AES(ciphertext, key) ‚Üí plaintext             ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Challenge: How to share the key securely?             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ASYMMETRIC ENCRYPTION                                  ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                  ‚îÇ
‚îÇ  Public key encrypts, private key decrypts             ‚îÇ
‚îÇ  Slower, used for key exchange                          ‚îÇ
‚îÇ  Examples: RSA, ECDSA                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Alice has: public_key_alice, private_key_alice        ‚îÇ
‚îÇ  Bob encrypts: RSA(message, public_key_alice)          ‚îÇ
‚îÇ  Alice decrypts: RSA(ciphertext, private_key_alice)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Encryption at Rest

\`\`\`
Protect stored data:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Database Encryption                                    ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Column-level:                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ user_id ‚îÇ ssn (encrypted)            ‚îÇ              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îÇ
‚îÇ  ‚îÇ 1       ‚îÇ a3f4c8d9e2b1...            ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ 2       ‚îÇ 7b2e9f4a8c3d...            ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Full-disk encryption:                                  ‚îÇ
‚îÇ  Entire database files encrypted                        ‚îÇ
‚îÇ  Transparent to application                             ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  AWS RDS: Enable encryption at creation                ‚îÇ
‚îÇ  MongoDB: Enable Encrypted Storage Engine               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Encryption in Transit

\`\`\`
Protect data moving between systems:

TLS (Transport Layer Security):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Client                          Server                 ‚îÇ
‚îÇ    ‚îÇ                                ‚îÇ                   ‚îÇ
‚îÇ    ‚îÇ ‚îÄ‚îÄ ClientHello ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                   ‚îÇ
‚îÇ    ‚îÇ                                ‚îÇ                   ‚îÇ
‚îÇ    ‚îÇ‚óÑ‚îÄ‚îÄ ServerHello, Certificate ‚îÄ‚îÄ‚îÇ                   ‚îÇ
‚îÇ    ‚îÇ                                ‚îÇ                   ‚îÇ
‚îÇ    ‚îÇ ‚îÄ‚îÄ Key Exchange ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                   ‚îÇ
‚îÇ    ‚îÇ                                ‚îÇ                   ‚îÇ
‚îÇ    ‚îÇ‚óÑ‚îÄ‚îÄ Finished ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                   ‚îÇ
‚îÇ    ‚îÇ                                ‚îÇ                   ‚îÇ
‚îÇ    ‚îÇ‚ïê‚ïê Encrypted communication ‚ïê‚ïê‚ïê‚ïê‚îÇ                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Result: All data encrypted with session key           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Always use:
‚Ä¢ HTTPS (TLS 1.3)
‚Ä¢ TLS for database connections
‚Ä¢ mTLS for service-to-service
\`\`\`

## Key Management

\`\`\`
The hardest problem in encryption:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BAD: Keys in code or config files                     ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                       ‚îÇ
‚îÇ  encryption_key = "super_secret_key_123"  // DON'T!    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  GOOD: Key Management Service (KMS)                    ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ
‚îÇ  ‚îÇ     KMS      ‚îÇ ‚Üê Hardware Security Module backed    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îÇ         ‚îÇ                                               ‚îÇ
‚îÇ  Master Key (never leaves KMS)                         ‚îÇ
‚îÇ         ‚îÇ                                               ‚îÇ
‚îÇ         ‚ñº                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ  ‚îÇ Data Key       ‚îÇ ‚Üê Encrypted by Master Key          ‚îÇ
‚îÇ  ‚îÇ (encrypted)    ‚îÇ                                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îÇ         ‚îÇ                                               ‚îÇ
‚îÇ  App requests decryption from KMS                      ‚îÇ
‚îÇ  KMS returns plaintext data key                        ‚îÇ
‚îÇ  App uses data key to decrypt data                     ‚îÇ
‚îÇ  Data key discarded after use                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

AWS KMS, GCP KMS, Azure Key Vault, HashiCorp Vault
\`\`\`

## Hashing vs Encryption

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HASHING (one-way)                                      ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                      ‚îÇ
‚îÇ  Input ‚Üí Hash ‚Üí Fixed-size output                      ‚îÇ
‚îÇ  Cannot reverse to get input                            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Use for: Passwords, integrity checks, deduplication   ‚îÇ
‚îÇ  Algorithms: bcrypt, SHA-256, BLAKE2                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ENCRYPTION (two-way)                                   ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚îÇ
‚îÇ  Input + Key ‚Üí Ciphertext                              ‚îÇ
‚îÇ  Ciphertext + Key ‚Üí Input                              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Use for: Data that must be read later                 ‚îÇ
‚îÇ  Algorithms: AES-256-GCM, ChaCha20-Poly1305           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Never "encrypt" passwords‚ÄîHASH them!
\`\`\`

## Data Classification

\`\`\`
Different data needs different protection:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PUBLIC                                                 ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                 ‚îÇ
‚îÇ  Marketing content, public APIs                         ‚îÇ
‚îÇ  Protection: None required                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  INTERNAL                                               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                               ‚îÇ
‚îÇ  Employee directories, internal docs                    ‚îÇ
‚îÇ  Protection: Access control                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  CONFIDENTIAL                                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                           ‚îÇ
‚îÇ  Customer data, financial records                       ‚îÇ
‚îÇ  Protection: Encryption + access control + audit       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  RESTRICTED                                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ  SSN, payment cards, health records                    ‚îÇ
‚îÇ  Protection: Strong encryption + MFA + strict access   ‚îÇ
‚îÇ              + audit + data masking                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Encryption Best Practices

1. **Encrypt sensitive data at rest** - Database, backups, logs
2. **Always use TLS in transit** - No exceptions
3. **Use KMS for key management** - Don't manage keys yourself
4. **Rotate keys regularly** - Limit breach impact
5. **Encrypt backups** - Often forgotten attack vector

[WARNING]
## Encryption Pitfalls

1. **Rolling your own crypto** - Use established libraries
2. **Weak algorithms** - No MD5, DES, RC4
3. **Keys in source code** - Use secrets management
4. **Encrypting already-hashed data** - Waste of resources
5. **No key rotation** - Compromised key = all data exposed

[REAL-WORLD]
## How Companies Handle Encryption

**Google:** Everything encrypted at rest and in transit
**AWS:** KMS with customer-managed keys option
**Stripe:** PCI DSS compliance, strong encryption
**Apple:** End-to-end encryption, device encryption

The pattern: **Encrypt sensitive data everywhere, use KMS for keys.**`,

            'hld_38': `## DDoS Protection and Security Architecture

[STORY]
## The Attack That Took Down The Internet

In 2016, a massive DDoS attack hit Dyn, a DNS provider. The attack used a botnet of IoT devices (Mirai) to generate 1.2 Tbps of traffic. Twitter, Netflix, Reddit, and hundreds of sites went down. The internet learned that DDoS protection isn't optional.

> "The question isn't IF you'll be attacked, but WHEN. Be prepared." - Security engineer

## Types of DDoS Attacks

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  VOLUMETRIC ATTACKS                                     ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                      ‚îÇ
‚îÇ  Flood bandwidth with massive traffic                   ‚îÇ
‚îÇ  Examples: UDP flood, DNS amplification                 ‚îÇ
‚îÇ  Scale: 100+ Gbps                                       ‚îÇ
‚îÇ  Defense: CDN, scrubbing centers                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  PROTOCOL ATTACKS                                       ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                       ‚îÇ
‚îÇ  Exploit protocol weaknesses                            ‚îÇ
‚îÇ  Examples: SYN flood, Ping of Death                     ‚îÇ
‚îÇ  Scale: Millions of packets/second                      ‚îÇ
‚îÇ  Defense: Firewalls, rate limiting                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  APPLICATION LAYER ATTACKS                              ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                              ‚îÇ
‚îÇ  Target application logic                               ‚îÇ
‚îÇ  Examples: HTTP flood, Slowloris                        ‚îÇ
‚îÇ  Scale: Hundreds of requests/second                     ‚îÇ
‚îÇ  Defense: WAF, rate limiting, CAPTCHA                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## DDoS Defense Architecture

\`\`\`
Defense in depth:

                    ATTACKER
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 1: ISP/Cloud Provider                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                          ‚îÇ
‚îÇ  Blackhole routing (last resort)                       ‚îÇ
‚îÇ  BGP-based scrubbing                                   ‚îÇ
‚îÇ  Anycast distribution                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 2: CDN/DDoS Service                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                              ‚îÇ
‚îÇ  Cloudflare, AWS Shield, Akamai                        ‚îÇ
‚îÇ  Global PoPs absorb traffic                            ‚îÇ
‚îÇ  ML-based anomaly detection                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 3: WAF (Web Application Firewall)               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                   ‚îÇ
‚îÇ  SQL injection protection                              ‚îÇ
‚îÇ  XSS protection                                        ‚îÇ
‚îÇ  Rate limiting                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 4: Application                                  ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚îÇ
‚îÇ  Input validation                                      ‚îÇ
‚îÇ  Authentication                                        ‚îÇ
‚îÇ  Rate limiting per user                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Security Architecture Principles

### Zero Trust

\`\`\`
"Never trust, always verify"

Traditional (Castle & Moat):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Outside = Untrusted                                    ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Firewall ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÇ
‚îÇ  Inside = Trusted  ‚Üê DANGEROUS!                        ‚îÇ
‚îÇ  (Once inside, attacker has access)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Zero Trust:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Every request is verified:                            ‚îÇ
‚îÇ  ‚Ä¢ Who are you? (Identity)                             ‚îÇ
‚îÇ  ‚Ä¢ What device? (Device health)                        ‚îÇ
‚îÇ  ‚Ä¢ What access? (Least privilege)                      ‚îÇ
‚îÇ  ‚Ä¢ Is this normal? (Behavior analysis)                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Even internal traffic is authenticated                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Defense in Depth

\`\`\`
Multiple layers of security:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  If attacker bypasses one layer, others protect       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Network:    Firewall, IDS/IPS                         ‚îÇ
‚îÇ  Transport:  TLS, mTLS                                 ‚îÇ
‚îÇ  Application: WAF, input validation                    ‚îÇ
‚îÇ  Data:       Encryption, access control                ‚îÇ
‚îÇ  Monitoring: Logging, alerting                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Least Privilege

\`\`\`
Minimal permissions needed:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BAD:                                                   ‚îÇ
‚îÇ  Database user: root (full access)                     ‚îÇ
‚îÇ  Service account: admin role                           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  GOOD:                                                  ‚îÇ
‚îÇ  Database user: SELECT on specific tables only         ‚îÇ
‚îÇ  Service account: Only permissions needed              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Principle: "Need to know" basis                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Common Vulnerabilities (OWASP Top 10)

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Injection (SQL, command)                           ‚îÇ
‚îÇ     Fix: Parameterized queries, input validation       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  2. Broken Authentication                              ‚îÇ
‚îÇ     Fix: MFA, secure session management                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  3. Sensitive Data Exposure                            ‚îÇ
‚îÇ     Fix: Encryption, data classification               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  4. XML External Entities (XXE)                        ‚îÇ
‚îÇ     Fix: Disable DTD processing                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  5. Broken Access Control                              ‚îÇ
‚îÇ     Fix: RBAC, deny by default                         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  6. Security Misconfiguration                          ‚îÇ
‚îÇ     Fix: Hardening, security scanning                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  7. Cross-Site Scripting (XSS)                         ‚îÇ
‚îÇ     Fix: Output encoding, CSP headers                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  8. Insecure Deserialization                           ‚îÇ
‚îÇ     Fix: Validate serialized data                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  9. Using Components with Known Vulnerabilities        ‚îÇ
‚îÇ     Fix: Dependency scanning, updates                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  10. Insufficient Logging & Monitoring                 ‚îÇ
‚îÇ      Fix: Comprehensive logging, alerting              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Security Checklist

\`\`\`python
# Input validation
def create_user(data):
    # Validate input
    if not is_valid_email(data['email']):
        raise ValidationError("Invalid email")
    
    # Parameterized query (prevent SQL injection)
    cursor.execute(
        "INSERT INTO users (email, name) VALUES (%s, %s)",
        (data['email'], data['name'])
    )

# Output encoding (prevent XSS)
def render_comment(comment):
    return html.escape(comment)

# HTTPS only
app.config['SESSION_COOKIE_SECURE'] = True

# Security headers
@app.after_request
def add_security_headers(response):
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'DENY'
    response.headers['Content-Security-Policy'] = "default-src 'self'"
    return response
\`\`\`

[TIP]
## Security Best Practices

1. **Defense in depth** - Multiple security layers
2. **Zero trust** - Verify everything
3. **Least privilege** - Minimal permissions
4. **Shift left** - Security in design phase
5. **Automate security** - SAST, DAST, dependency scanning

[WARNING]
## Security Anti-Patterns

1. **Security as afterthought** - Bolt-on security fails
2. **Security through obscurity** - Hidden != Secure
3. **Trusting user input** - Always validate
4. **Homegrown crypto** - Use proven solutions
5. **Ignoring logs** - Can't detect what you don't monitor

[REAL-WORLD]
## How Companies Handle Security

**Cloudflare:** Global DDoS protection, WAF
**Google:** BeyondCorp (zero trust pioneer)
**Netflix:** Security monkey, Chaos engineering
**AWS:** Shared responsibility model, WAF, Shield

The pattern: **Layers of defense, zero trust, automate everything.**`,

            // ==================== DAY 14: API Design ====================
            'hld_39': `## REST API Best Practices

[STORY]
## The API That Scaled to Billions

When Stripe designed their API, they obsessed over developer experience. Clear naming, consistent patterns, excellent documentation. The result? An API so good that developers love it. Stripe processes billions of dollars because their API is a joy to use.

> "The best API is one that developers can use without reading the documentation‚Äîbut the documentation is excellent anyway." - Stripe engineer

## REST Principles

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  REST = Representational State Transfer                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Resources:  Nouns, not verbs                          ‚îÇ
‚îÇ              /users, /orders, /products                ‚îÇ
‚îÇ              NOT: /getUsers, /createOrder              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  HTTP Methods:                                          ‚îÇ
‚îÇ  GET     - Read (idempotent, safe)                     ‚îÇ
‚îÇ  POST    - Create                                       ‚îÇ
‚îÇ  PUT     - Replace (idempotent)                        ‚îÇ
‚îÇ  PATCH   - Partial update                              ‚îÇ
‚îÇ  DELETE  - Remove (idempotent)                         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Stateless: Each request contains all info needed      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## URL Design

\`\`\`
Good URL patterns:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Collection of resources:                              ‚îÇ
‚îÇ  GET    /users                  List all users         ‚îÇ
‚îÇ  POST   /users                  Create user            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Single resource:                                       ‚îÇ
‚îÇ  GET    /users/123              Get user 123           ‚îÇ
‚îÇ  PUT    /users/123              Replace user 123       ‚îÇ
‚îÇ  PATCH  /users/123              Update user 123        ‚îÇ
‚îÇ  DELETE /users/123              Delete user 123        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Nested resources:                                      ‚îÇ
‚îÇ  GET    /users/123/orders       User's orders          ‚îÇ
‚îÇ  POST   /users/123/orders       Create order for user  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Filtering/Pagination:                                  ‚îÇ
‚îÇ  GET    /users?status=active&limit=10&offset=20       ‚îÇ
‚îÇ  GET    /users?sort=created_at&order=desc             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Bad patterns (avoid):
/getUsers, /user/delete/123, /api/v1/getUserById
\`\`\`

## HTTP Status Codes

\`\`\`
Use correct status codes:

2xx Success:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  200 OK           - Success (GET, PUT, PATCH)          ‚îÇ
‚îÇ  201 Created      - Resource created (POST)            ‚îÇ
‚îÇ  204 No Content   - Success, no body (DELETE)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

4xx Client Error:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  400 Bad Request  - Invalid input                      ‚îÇ
‚îÇ  401 Unauthorized - Not authenticated                  ‚îÇ
‚îÇ  403 Forbidden    - Not authorized                     ‚îÇ
‚îÇ  404 Not Found    - Resource doesn't exist             ‚îÇ
‚îÇ  409 Conflict     - Resource state conflict            ‚îÇ
‚îÇ  422 Unprocessable - Validation failed                 ‚îÇ
‚îÇ  429 Too Many Requests - Rate limited                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

5xx Server Error:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  500 Internal Server Error - Server bug               ‚îÇ
‚îÇ  502 Bad Gateway   - Upstream service failed          ‚îÇ
‚îÇ  503 Service Unavailable - Temporarily down           ‚îÇ
‚îÇ  504 Gateway Timeout - Upstream timeout               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Request/Response Design

\`\`\`json
// POST /users
// Request:
{
  "email": "alice@example.com",
  "name": "Alice Smith",
  "role": "admin"
}

// Response (201 Created):
{
  "id": "usr_123abc",
  "email": "alice@example.com",
  "name": "Alice Smith",
  "role": "admin",
  "created_at": "2024-01-15T10:30:00Z",
  "updated_at": "2024-01-15T10:30:00Z"
}

// Error Response (400 Bad Request):
{
  "error": {
    "code": "validation_error",
    "message": "Invalid request body",
    "details": [
      {
        "field": "email",
        "message": "Invalid email format"
      }
    ]
  }
}
\`\`\`

## Pagination

\`\`\`json
// Cursor-based (preferred for large datasets):
GET /users?limit=10&cursor=eyJpZCI6MTIzfQ

{
  "data": [...],
  "pagination": {
    "next_cursor": "eyJpZCI6MTMzfQ",
    "has_more": true
  }
}

// Offset-based (simpler, but problematic at scale):
GET /users?limit=10&offset=20

{
  "data": [...],
  "pagination": {
    "total": 1000,
    "limit": 10,
    "offset": 20
  }
}
\`\`\`

## Versioning

\`\`\`
Options:

1. URL path (most common):
   /v1/users
   /v2/users

2. Header:
   Accept: application/vnd.myapi.v1+json

3. Query parameter:
   /users?version=1

Recommendation: URL path for simplicity
\`\`\`

## API Documentation (OpenAPI)

\`\`\`yaml
openapi: 3.0.0
info:
  title: Users API
  version: 1.0.0

paths:
  /users:
    get:
      summary: List users
      parameters:
        - name: limit
          in: query
          schema:
            type: integer
            default: 10
      responses:
        '200':
          description: List of users
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: '#/components/schemas/User'

components:
  schemas:
    User:
      type: object
      properties:
        id:
          type: string
        email:
          type: string
        name:
          type: string
\`\`\`

[TIP]
## REST API Best Practices

1. **Use nouns, not verbs** - /users, not /getUsers
2. **Consistent naming** - snake_case or camelCase, pick one
3. **Proper status codes** - 201 for create, 404 for not found
4. **Version your API** - /v1/users
5. **Document everything** - OpenAPI/Swagger

[WARNING]
## REST API Pitfalls

1. **Verbs in URLs** - /user/delete/123
2. **Wrong status codes** - 200 for everything
3. **Exposing internal IDs** - Use UUIDs or prefixed IDs
4. **No pagination** - Returns 10,000 records
5. **Breaking changes** - Without versioning

[REAL-WORLD]
## API Design Leaders

**Stripe:** Gold standard for API design
**Twilio:** Developer-first approach
**GitHub:** RESTful with great documentation
**Slack:** Consistent patterns, excellent errors

The pattern: **Design for developers, be consistent, document thoroughly.**`,

            'hld_40': `## GraphQL vs REST

[STORY]
## Facebook's Data Fetching Problem

In 2012, Facebook's mobile app was painfully slow. Each screen required multiple REST API calls, fetching way more data than needed. They invented **GraphQL**‚Äîa query language that lets clients request exactly the data they need in a single request.

> "GraphQL isn't better than REST‚Äîit's different. Choose based on your use case." - Apollo engineer

## REST vs GraphQL Comparison

\`\`\`
REST:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Multiple endpoints for different resources            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  GET /users/123                                         ‚îÇ
‚îÇ  GET /users/123/posts                                   ‚îÇ
‚îÇ  GET /users/123/followers                               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  3 round trips, over-fetching common                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

GraphQL:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Single endpoint, query specifies data needed          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  POST /graphql                                          ‚îÇ
‚îÇ  {                                                       ‚îÇ
‚îÇ    user(id: "123") {                                    ‚îÇ
‚îÇ      name                                               ‚îÇ
‚îÇ      posts { title }                                    ‚îÇ
‚îÇ      followers { name }                                 ‚îÇ
‚îÇ    }                                                     ‚îÇ
‚îÇ  }                                                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  1 round trip, exactly what you need                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## GraphQL Basics

\`\`\`graphql
# Schema definition
type User {
  id: ID!
  name: String!
  email: String!
  posts: [Post!]!
  followers: [User!]!
}

type Post {
  id: ID!
  title: String!
  content: String!
  author: User!
}

type Query {
  user(id: ID!): User
  users(limit: Int): [User!]!
}

type Mutation {
  createUser(input: CreateUserInput!): User!
  updateUser(id: ID!, input: UpdateUserInput!): User!
}

# Query example
query GetUserWithPosts {
  user(id: "123") {
    name
    email
    posts {
      title
      content
    }
  }
}

# Mutation example
mutation CreatePost {
  createPost(input: {
    title: "Hello World"
    content: "My first post"
    authorId: "123"
  }) {
    id
    title
  }
}
\`\`\`

## When to Use What

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  USE REST WHEN:                                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                        ‚îÇ
‚îÇ  ‚úì Simple CRUD operations                              ‚îÇ
‚îÇ  ‚úì Caching is important (HTTP caching)                 ‚îÇ
‚îÇ  ‚úì File uploads/downloads                              ‚îÇ
‚îÇ  ‚úì Team familiar with REST                             ‚îÇ
‚îÇ  ‚úì Public APIs (easier to understand)                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  USE GRAPHQL WHEN:                                      ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                      ‚îÇ
‚îÇ  ‚úì Complex, nested data requirements                   ‚îÇ
‚îÇ  ‚úì Multiple clients with different data needs          ‚îÇ
‚îÇ  ‚úì Mobile apps (minimize data transfer)                ‚îÇ
‚îÇ  ‚úì Rapid frontend iteration                            ‚îÇ
‚îÇ  ‚úì Aggregating multiple data sources                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## GraphQL Challenges

\`\`\`
1. N+1 QUERY PROBLEM:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  query {                                                ‚îÇ
‚îÇ    users {            # 1 query for users              ‚îÇ
‚îÇ      posts { ... }    # N queries (1 per user) for posts‚îÇ
‚îÇ    }                                                    ‚îÇ
‚îÇ  }                                                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Solution: DataLoader (batches queries)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

2. CACHING:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  REST: HTTP caching works great                        ‚îÇ
‚îÇ  GraphQL: POST requests, no HTTP caching               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Solution: Persisted queries, CDN with query hashing   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

3. COMPLEXITY:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Malicious query:                                       ‚îÇ
‚îÇ  {                                                       ‚îÇ
‚îÇ    user {                                               ‚îÇ
‚îÇ      followers {                                        ‚îÇ
‚îÇ        followers {                                      ‚îÇ
‚îÇ          followers {  # Deeply nested = expensive!     ‚îÇ
‚îÇ            ...                                          ‚îÇ
‚îÇ          }                                              ‚îÇ
‚îÇ        }                                                ‚îÇ
‚îÇ      }                                                  ‚îÇ
‚îÇ    }                                                    ‚îÇ
‚îÇ  }                                                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Solution: Query depth limiting, complexity analysis   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## gRPC: The Third Option

\`\`\`
For internal service-to-service:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  gRPC:                                                  ‚îÇ
‚îÇ  ‚Ä¢ Binary protocol (Protocol Buffers)                  ‚îÇ
‚îÇ  ‚Ä¢ Very fast (10x faster than JSON)                    ‚îÇ
‚îÇ  ‚Ä¢ Strongly typed                                       ‚îÇ
‚îÇ  ‚Ä¢ Streaming support                                    ‚îÇ
‚îÇ  ‚Ä¢ Code generation                                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Good for: Microservices, low-latency internal calls   ‚îÇ
‚îÇ  Not for: Browser clients (needs proxy)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

// Proto definition
service UserService {
  rpc GetUser(GetUserRequest) returns (User);
  rpc ListUsers(ListUsersRequest) returns (stream User);
}

message User {
  string id = 1;
  string name = 2;
  string email = 3;
}
\`\`\`

## Comparison Summary

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          ‚îÇ    REST     ‚îÇ  GraphQL    ‚îÇ    gRPC     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Format   ‚îÇ JSON        ‚îÇ JSON        ‚îÇ Protobuf    ‚îÇ
‚îÇ Transport‚îÇ HTTP        ‚îÇ HTTP        ‚îÇ HTTP/2      ‚îÇ
‚îÇ Caching  ‚îÇ Easy        ‚îÇ Hard        ‚îÇ N/A         ‚îÇ
‚îÇ Speed    ‚îÇ Fast        ‚îÇ Fast        ‚îÇ Fastest     ‚îÇ
‚îÇ Learning ‚îÇ Low         ‚îÇ Medium      ‚îÇ Medium      ‚îÇ
‚îÇ Tooling  ‚îÇ Excellent   ‚îÇ Good        ‚îÇ Good        ‚îÇ
‚îÇ Use Case ‚îÇ Public APIs ‚îÇ Complex UIs ‚îÇ Internal    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## API Choice Best Practices

1. **Start with REST** - Simple, well-understood
2. **Add GraphQL** - When frontend needs flexibility
3. **Use gRPC** - For internal microservices
4. **Don't mix unnecessarily** - Adds complexity
5. **Consider BFF pattern** - Backend for Frontend

[WARNING]
## GraphQL Pitfalls

1. **N+1 queries** - Use DataLoader
2. **No depth limiting** - DoS via complex queries
3. **Over-engineering** - Simple CRUD doesn't need GraphQL
4. **Caching challenges** - Plan for this upfront
5. **Security** - Field-level authorization is tricky

[REAL-WORLD]
## Who Uses What

**GraphQL:** Facebook, GitHub, Shopify, Airbnb
**REST:** Most public APIs, AWS, Stripe
**gRPC:** Google, Netflix, Uber (internal)
**Hybrid:** Many companies use REST + GraphQL

The pattern: **REST for public APIs, GraphQL for complex UIs, gRPC for internal services.**`,

            // ==================== DAY 15: Case Studies ====================
            'hld_41': `## Case Study: Design a URL Shortener (like bit.ly)

[STORY]
## The Interview Challenge

"Design a URL shortener like bit.ly" is one of the most common system design interview questions. It seems simple‚Äîbut handling billions of URLs, redirects in milliseconds, and analytics at scale is anything but.

> "A good system design answer shows your thought process, not just the final architecture." - Staff engineer

## Requirements Gathering

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FUNCTIONAL REQUIREMENTS                               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                 ‚îÇ
‚îÇ  1. Given a URL, generate a shorter unique URL         ‚îÇ
‚îÇ  2. Given short URL, redirect to original              ‚îÇ
‚îÇ  3. Optional: Custom short URLs                        ‚îÇ
‚îÇ  4. Optional: Analytics (click count, geography)       ‚îÇ
‚îÇ  5. Optional: URL expiration                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  NON-FUNCTIONAL REQUIREMENTS                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚îÇ
‚îÇ  1. Highly available (redirects must work)             ‚îÇ
‚îÇ  2. Low latency redirects (< 100ms)                    ‚îÇ
‚îÇ  3. Scale: 100M URLs created/month, 10B redirects/month‚îÇ
‚îÇ  4. URLs should not be predictable (security)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Back of Envelope Calculations

\`\`\`
Write volume:
  100M URLs/month = ~40 URLs/second
  
Read volume (10B redirects):
  10B/month = ~4000 redirects/second
  Read:Write ratio = 100:1 (read-heavy!)

Storage:
  100M URLs √ó 12 months √ó 5 years = 6B URLs
  Each URL: ~500 bytes (original URL + metadata)
  Total: 6B √ó 500B = 3TB

Short URL length:
  Using Base62 (a-z, A-Z, 0-9):
  62^6 = 56B combinations (6 chars is enough)
  62^7 = 3.5T combinations (for safety)
\`\`\`

## High-Level Design

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Write Path (Create short URL):                         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Client ‚îÄ‚îÄ‚ñ∫ API Gateway ‚îÄ‚îÄ‚ñ∫ URL Service ‚îÄ‚îÄ‚ñ∫ Database   ‚îÇ
‚îÇ                                  ‚îÇ                       ‚îÇ
‚îÇ                                  ‚îî‚îÄ‚îÄ‚ñ∫ Key Generation    ‚îÇ
‚îÇ                                       Service           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Read Path (Redirect):                                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Client ‚îÄ‚îÄ‚ñ∫ CDN ‚îÄ‚îÄ‚ñ∫ API Gateway ‚îÄ‚îÄ‚ñ∫ URL Service        ‚îÇ
‚îÇ               ‚îÇ           ‚îÇ              ‚îÇ              ‚îÇ
‚îÇ          Cache hit    Cache hit      Database          ‚îÇ
‚îÇ           (fast!)      (Redis)        (fallback)       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Database Design

\`\`\`sql
-- URLs table
CREATE TABLE urls (
    short_code VARCHAR(7) PRIMARY KEY,
    original_url TEXT NOT NULL,
    user_id VARCHAR(36),
    created_at TIMESTAMP DEFAULT NOW(),
    expires_at TIMESTAMP,
    click_count BIGINT DEFAULT 0
);

-- Index for analytics
CREATE INDEX idx_urls_user ON urls(user_id);

-- For custom short codes, check uniqueness
-- Use database constraint or pre-check
\`\`\`

## Key Generation Approaches

\`\`\`
Option 1: Hash the URL
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MD5(original_url) ‚Üí take first 7 chars               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Problem: Collisions possible                          ‚îÇ
‚îÇ  Solution: Check and retry with counter                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  hash("example.com") ‚Üí "a3f4c8d"                       ‚îÇ
‚îÇ  Collision? hash("example.com" + 1) ‚Üí "b7e2f9a"       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Option 2: Counter-based (distributed)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Use auto-increment ID, encode to Base62              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ID: 12345 ‚Üí Base62 ‚Üí "dnh"                            ‚îÇ
‚îÇ  ID: 12346 ‚Üí Base62 ‚Üí "dni"                            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Problem: Predictable URLs                              ‚îÇ
‚îÇ  Solution: Add random bits or shuffle                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Option 3: Pre-generated keys (recommended)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Key Generation Service pre-generates random keys      ‚îÇ
‚îÇ  Store in database, mark as used when assigned         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                  ‚îÇ
‚îÇ  ‚îÇ Key Service      ‚îÇ                                  ‚îÇ
‚îÇ  ‚îÇ Unused: abc123   ‚îÇ‚îÄ‚îÄ‚ñ∫ Give to URL Service          ‚îÇ
‚îÇ  ‚îÇ Unused: xyz789   ‚îÇ                                  ‚îÇ
‚îÇ  ‚îÇ Used: def456     ‚îÇ                                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Pro: No collisions, not predictable                   ‚îÇ
‚îÇ  Con: More complex, need to manage key pool            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Caching Strategy

\`\`\`
Read-heavy = Cache is critical

Cache hierarchy:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. CDN (edge cache)                                    ‚îÇ
‚îÇ     - Cache redirect responses (301/302)               ‚îÇ
‚îÇ     - TTL: 1 hour                                       ‚îÇ
‚îÇ     - Hit rate: 60-70%                                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  2. Application cache (Redis)                          ‚îÇ
‚îÇ     - short_code ‚Üí original_url                        ‚îÇ
‚îÇ     - TTL: 24 hours                                     ‚îÇ
‚îÇ     - Hit rate: 95%+                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  3. Database                                            ‚îÇ
‚îÇ     - Fallback for cache misses                        ‚îÇ
‚îÇ     - Rarely hit for popular URLs                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Complete Architecture

\`\`\`
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇ   CDN       ‚îÇ
                              ‚îÇ (edge cache)‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇ Load Balancer‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                      ‚îÇ                      ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  API Server   ‚îÇ     ‚îÇ  API Server   ‚îÇ     ‚îÇ  API Server   ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                      ‚îÇ                      ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                      ‚îÇ                      ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ    Redis      ‚îÇ     ‚îÇ  Key Service  ‚îÇ     ‚îÇ   Database    ‚îÇ
      ‚îÇ   (Cache)     ‚îÇ     ‚îÇ               ‚îÇ     ‚îÇ  (Sharded)    ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Interview Tips

1. **Clarify requirements first** - Write vs read ratio matters
2. **Do back-of-envelope math** - Shows you can think at scale
3. **Start simple, then optimize** - Don't over-engineer initially
4. **Discuss tradeoffs** - Every decision has pros/cons
5. **Think about failure modes** - What happens when X fails?

[REAL-WORLD]
## How bit.ly Works

**Scale:** 300M+ links/month, billions of redirects
**Stack:** Go, Cassandra, Redis
**Key insight:** Heavy caching (90%+ hit rate)

The pattern: **Pre-generated keys, aggressive caching, simple database.**`,

            'hld_42': `## Case Study: Design a Chat System (like WhatsApp)

[STORY]
## The Real-Time Challenge

WhatsApp handles 100 billion messages per day with just 50 engineers. How? By making smart architectural choices: Erlang for concurrency, minimal metadata, push rather than pull.

> "Real-time systems require different thinking‚Äîyou can't just scale with more servers." - WhatsApp engineer

## Requirements

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FUNCTIONAL                                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ  1. 1-on-1 messaging                                   ‚îÇ
‚îÇ  2. Group messaging (up to 1000 members)               ‚îÇ
‚îÇ  3. Online status (presence)                           ‚îÇ
‚îÇ  4. Read receipts                                       ‚îÇ
‚îÇ  5. Media sharing (images, files)                      ‚îÇ
‚îÇ  6. Offline message delivery                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  NON-FUNCTIONAL                                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                         ‚îÇ
‚îÇ  1. Low latency (< 100ms delivery)                     ‚îÇ
‚îÇ  2. Guaranteed delivery                                ‚îÇ
‚îÇ  3. Ordering within conversation                       ‚îÇ
‚îÇ  4. Support 500M daily active users                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Scale Calculations

\`\`\`
Users: 500M DAU
Messages: 40 messages/user/day average
Total: 20B messages/day = 230K messages/second peak

Message size: ~100 bytes average
Storage: 20B √ó 100B = 2TB/day new messages

Connections: 500M concurrent WebSocket connections
(This is the hard part!)
\`\`\`

## High-Level Architecture

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  User A  ‚îÇ                      ‚îÇ  User B  ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ       ‚îÇ WebSocket                       ‚îÇ WebSocket     ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ           Connection Servers               ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  (Stateful - maintain WebSocket conns)    ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                       ‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ           Message Service                  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  (Route messages to correct server)       ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                       ‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ           Message Queue (Kafka)           ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                       ‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ           Database (Cassandra)            ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Message Flow

\`\`\`
1. User A sends message to User B:

A ‚îÄ‚îÄWebSocket‚îÄ‚îÄ‚ñ∫ Connection Server 1
                        ‚îÇ
                        ‚ñº
                 Message Service
                   ‚îÇ         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚ñº                                        ‚ñº
Store in DB                         Find B's connection
    ‚îÇ                                        ‚îÇ
    ‚îÇ                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                               ‚ñº
    ‚îÇ                      Connection Server 2
    ‚îÇ                               ‚îÇ
    ‚îÇ                               ‚ñº
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ User B

2. If User B is offline:
   - Store in "pending messages" table
   - Notify via push notification
   - Deliver when B reconnects
\`\`\`

## Database Design

\`\`\`
Messages table (Cassandra - optimized for writes):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Partition key: conversation_id                        ‚îÇ
‚îÇ  Clustering key: message_timestamp                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  conversation_id | message_timestamp | sender | content‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  conv_123        | 2024-01-15 10:30  | userA  | "Hi"   ‚îÇ
‚îÇ  conv_123        | 2024-01-15 10:31  | userB  | "Hey!" ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  All messages for a conversation are stored together   ‚îÇ
‚îÇ  Ordered by timestamp automatically                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

User-Connection mapping (Redis):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  user_id ‚Üí {server_id, connection_id}                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  "user_A" ‚Üí {"server": "conn-server-5", "conn": "abc"} ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Used to route messages to correct server              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Handling Scale

\`\`\`
Challenge: 500M WebSocket connections

Solution: Connection server clustering
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                          ‚îÇ
‚îÇ  Each connection server handles ~100K connections      ‚îÇ
‚îÇ  500M / 100K = 5000 connection servers                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇConn Srv ‚îÇ  ‚îÇConn Srv ‚îÇ  ‚îÇConn Srv ‚îÇ  ...5000       ‚îÇ
‚îÇ  ‚îÇ  100K   ‚îÇ  ‚îÇ  100K   ‚îÇ  ‚îÇ  100K   ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ            ‚îÇ                       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                    ‚îÇ                                    ‚îÇ
‚îÇ          Message Routing Layer                         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

User A on Server 5 sends to User B on Server 2000:
1. Server 5 sends message to Message Service
2. Service looks up: User B ‚Üí Server 2000
3. Service sends to Server 2000
4. Server 2000 pushes to User B's WebSocket
\`\`\`

## Group Messaging

\`\`\`
Small groups (< 100 members):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Fan-out on send:                                       ‚îÇ
‚îÇ  Message arrives ‚Üí Send to all members immediately     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Simple, low latency, but doesn't scale for large groups‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Large groups (> 100 members):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Fan-out on read:                                       ‚îÇ
‚îÇ  Message stored once                                    ‚îÇ
‚îÇ  Each member fetches when online                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  More efficient for large groups                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

## Read Receipts

\`\`\`
Message states:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SENT     ‚Üí Message reached server                     ‚îÇ
‚îÇ  DELIVERED ‚Üí Message reached recipient device          ‚îÇ
‚îÇ  READ     ‚Üí Recipient opened conversation              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Track with timestamps:                                ‚îÇ
‚îÇ  {                                                       ‚îÇ
‚îÇ    "message_id": "msg_123",                            ‚îÇ
‚îÇ    "sent_at": "10:30:00",                              ‚îÇ
‚îÇ    "delivered_at": "10:30:01",                         ‚îÇ
‚îÇ    "read_at": "10:30:15"                               ‚îÇ
‚îÇ  }                                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

[TIP]
## Interview Tips

1. **Focus on connection management** - The hardest part
2. **Discuss offline handling** - Messages must persist
3. **Consider push notifications** - For offline users
4. **Think about ordering** - Messages must arrive in order
5. **Media is different** - Upload separately, send link

[REAL-WORLD]
## How WhatsApp Actually Works

**Stack:** Erlang (handles millions of connections)
**Database:** Mnesia (Erlang's built-in DB)
**Key insight:** Minimal metadata, efficient protocol

The pattern: **WebSocket for real-time, message queue for reliability, fan-out based on group size.**`
        };

        // ==================== FALLBACK QUIZZES ====================
        const FALLBACK_QUIZZES = {
            // Day 1: Introduction to System Design
            'hld_0': {
                'easy': [
                    {
                        question: "What is the main purpose of system design?",
                        options: ["Writing clean code", "Defining architecture and components to satisfy requirements", "Choosing programming languages", "Creating user interfaces"],
                        correct: 1,
                        explanation: "System design is about defining the overall architecture, components, and their interactions to meet specific requirements."
                    },
                    {
                        question: "Which of these is a key objective of system design?",
                        options: ["Making code shorter", "Scalability", "Using the newest technology", "Minimizing documentation"],
                        correct: 1,
                        explanation: "Scalability ensures the system can handle increased load as users grow."
                    },
                    {
                        question: "What does 'reliability' mean in system design?",
                        options: ["Fast response times", "The system works correctly even when things fail", "Using reliable programming languages", "Having reliable developers"],
                        correct: 1,
                        explanation: "Reliability means the system continues to function correctly despite hardware or software failures."
                    },
                    {
                        question: "Why did Twitter experience the 'Fail Whale' error?",
                        options: ["Bad programmers", "System couldn't handle the load", "Internet was slow", "Users were too active"],
                        correct: 1,
                        explanation: "The Fail Whale appeared when Twitter's infrastructure couldn't handle traffic spikes."
                    },
                    {
                        question: "System design is similar to which real-world planning?",
                        options: ["Party planning", "City planning", "Menu planning", "Travel planning"],
                        correct: 1,
                        explanation: "City planning involves infrastructure, traffic flow, utilities, and handling peak loads - similar to system design."
                    }
                ],
                'medium': [
                    {
                        question: "Instagram's architecture evolved from simple to complex. What triggered this change?",
                        options: ["New programming languages", "Massive user growth requiring scalability", "Company policy changes", "Developer preferences"],
                        correct: 1,
                        explanation: "Instagram went from 2 engineers and 1 server to 100+ engineers with thousands of servers due to growth from 2 to 25 million users."
                    },
                    {
                        question: "What is the primary trade-off when adding caching to a system?",
                        options: ["Cost vs features", "Speed vs data freshness", "Security vs usability", "Simplicity vs documentation"],
                        correct: 1,
                        explanation: "Caches improve speed but may serve stale data, creating a trade-off between performance and data freshness."
                    },
                    {
                        question: "Which component would you add first when scaling a simple web application?",
                        options: ["Machine learning model", "Load balancer", "Blockchain", "VR interface"],
                        correct: 1,
                        explanation: "A load balancer distributes traffic across multiple servers, enabling horizontal scaling."
                    },
                    {
                        question: "What is 'premature optimization' in system design?",
                        options: ["Optimizing code too early", "Designing for scale before you need it", "Using fast algorithms", "Testing early"],
                        correct: 1,
                        explanation: "Premature optimization means designing for extreme scale (like 1 billion users) when you only have 1000, adding unnecessary complexity."
                    },
                    {
                        question: "Netflix's design philosophy includes which principle?",
                        options: ["Components will never fail", "Design for failure - components will fail", "Use only one database", "Avoid cloud services"],
                        correct: 1,
                        explanation: "Netflix explicitly designs systems expecting failures, which is why their infrastructure is so resilient."
                    }
                ],
                'hard': [
                    {
                        question: "A startup has 10,000 users and expects 100,000 in 6 months. Which approach is most appropriate?",
                        options: ["Design for 10 million users immediately", "Design for current needs with clear scaling path", "Don't plan for scaling at all", "Rewrite everything monthly"],
                        correct: 1,
                        explanation: "Design for 2-3x expected growth with a clear path to scale further. Over-engineering wastes resources."
                    },
                    {
                        question: "When would you choose eventual consistency over strong consistency?",
                        options: ["Banking transactions", "Social media likes", "Medical records", "Inventory for scarce items"],
                        correct: 1,
                        explanation: "Social media likes can be eventually consistent - temporary inconsistency is acceptable. Banking and medical records need strong consistency."
                    },
                    {
                        question: "Your e-commerce site experiences 10x traffic on Black Friday. What architectural pattern helps most?",
                        options: ["Monolithic architecture", "Horizontal scaling with auto-scaling groups", "Single powerful server", "Manual server management"],
                        correct: 1,
                        explanation: "Horizontal scaling with auto-scaling automatically adds/removes servers based on demand."
                    },
                    {
                        question: "What's the main challenge when adding read replicas to a database?",
                        options: ["They're too expensive", "Replication lag causing temporary inconsistency", "They're too fast", "They use too much memory"],
                        correct: 1,
                        explanation: "Replication lag means reads from replicas might return slightly stale data."
                    },
                    {
                        question: "Google's design principle states you should design for how much scale?",
                        options: ["100x current scale", "10x current scale", "1000x current scale", "2x current scale"],
                        correct: 1,
                        explanation: "Google advises designing for 10x current scale - enough headroom without over-engineering."
                    }
                ],
                'expert': [
                    {
                        question: "A global service needs 99.99% availability. Which architecture decision has the MOST impact?",
                        options: ["Programming language choice", "Multi-region deployment with failover", "Database indexing strategy", "Code formatting standards"],
                        correct: 1,
                        explanation: "99.99% uptime (52 minutes/year downtime) requires multi-region deployment to survive regional outages."
                    },
                    {
                        question: "You're designing a system where users must ALWAYS see their own writes immediately, but can tolerate delays seeing others' writes. Which consistency model fits?",
                        options: ["Strong consistency", "Read-your-writes consistency", "Eventual consistency", "No consistency guarantees"],
                        correct: 1,
                        explanation: "Read-your-writes consistency guarantees users see their own changes immediately while allowing eventual consistency for others' changes."
                    },
                    {
                        question: "A message queue between services provides which primary architectural benefit?",
                        options: ["Faster processing", "Decoupling and resilience", "Reduced storage costs", "Simpler code"],
                        correct: 1,
                        explanation: "Message queues decouple services so one service's downtime doesn't cascade to others."
                    },
                    {
                        question: "When designing Netflix's streaming architecture, which factor creates the most complexity?",
                        options: ["User authentication", "CDN edge caching and geographic distribution", "Color scheme selection", "Email notifications"],
                        correct: 1,
                        explanation: "Streaming video globally requires sophisticated CDN edge caching across many geographic locations to minimize latency."
                    },
                    {
                        question: "You're choosing between microservices and monolith for a new project with 3 developers. What's the best approach?",
                        options: ["Always microservices for scalability", "Start monolith, extract services when pain points emerge", "Hybrid from day one", "Neither - use serverless only"],
                        correct: 1,
                        explanation: "Small teams should start with monoliths for simplicity, then extract microservices when specific scaling needs arise."
                    }
                ]
            },

            // Day 1: Requirements Gathering
            'hld_1': {
                'easy': [
                    {
                        question: "What are functional requirements?",
                        options: ["How fast the system should be", "What the system should DO (features)", "How much the system costs", "Who maintains the system"],
                        correct: 1,
                        explanation: "Functional requirements describe what the system does - the features users interact with."
                    },
                    {
                        question: "What are non-functional requirements?",
                        options: ["Features users don't want", "How WELL the system performs (speed, reliability)", "Requirements that don't function", "Optional features"],
                        correct: 1,
                        explanation: "Non-functional requirements define quality attributes like speed, reliability, and scalability."
                    },
                    {
                        question: "'The system should load in under 3 seconds' is what type of requirement?",
                        options: ["Functional", "Non-functional", "Optional", "Technical"],
                        correct: 1,
                        explanation: "This describes HOW WELL the system performs (latency), making it a non-functional requirement."
                    },
                    {
                        question: "Why should you ask clarifying questions before designing a system?",
                        options: ["To waste time", "Requirements are never complete; clarification prevents building wrong thing", "To show off knowledge", "It's not necessary"],
                        correct: 1,
                        explanation: "Initial requirements are almost never complete. Clarifying questions reveal hidden requirements and constraints."
                    },
                    {
                        question: "'Users can upload photos' is what type of requirement?",
                        options: ["Non-functional", "Constraint", "Functional", "Optional"],
                        correct: 2,
                        explanation: "This describes a feature (WHAT the system does), making it a functional requirement."
                    }
                ],
                'medium': [
                    {
                        question: "A URL shortener has a read/write ratio of 100:1. What does this imply for system design?",
                        options: ["Focus on write optimization", "Focus on read optimization with heavy caching", "Equal focus on both", "Ignore the ratio"],
                        correct: 1,
                        explanation: "100:1 read/write means reads dominate. Optimize for reads with caching and read replicas."
                    },
                    {
                        question: "What constraint would GDPR compliance impose on system design?",
                        options: ["Must use European servers only", "Data handling, storage location, and deletion capabilities", "Must use specific programming languages", "No constraints"],
                        correct: 1,
                        explanation: "GDPR affects data storage locations, user data access rights, and deletion requirements."
                    },
                    {
                        question: "The '$440 million button' story teaches us what about requirements?",
                        options: ["Buttons are expensive", "Small UX requirements have massive business impact", "Never use buttons", "Requirements don't matter"],
                        correct: 1,
                        explanation: "A 'Register' button costing $300M in lost sales shows that seemingly minor requirements have huge business impact."
                    },
                    {
                        question: "Which question is MOST important to ask when designing a chat application?",
                        options: ["What color should the UI be?", "Expected number of concurrent users and message volume?", "Which programming language to use?", "Who will write the code?"],
                        correct: 1,
                        explanation: "Scale questions (concurrent users, message volume) fundamentally affect architecture decisions."
                    },
                    {
                        question: "What's the 80/20 rule in requirements gathering?",
                        options: ["80% budget on 20% of features", "Focus on 20% of features that deliver 80% of value", "20% of team does 80% of work", "80% testing, 20% development"],
                        correct: 1,
                        explanation: "Prioritize the critical 20% of features that provide 80% of the value to users."
                    }
                ],
                'hard': [
                    {
                        question: "A streaming service requires '99.9% availability'. How much downtime per year is this?",
                        options: ["0 minutes", "About 8.7 hours", "About 52 minutes", "1 day"],
                        correct: 1,
                        explanation: "99.9% = 0.1% downtime = 0.001 √ó 365 √ó 24 = 8.76 hours per year."
                    },
                    {
                        question: "When requirements conflict (e.g., real-time updates vs. low infrastructure cost), what's the best approach?",
                        options: ["Always choose real-time", "Always minimize cost", "Document trade-offs and get stakeholder decision", "Ignore the conflict"],
                        correct: 2,
                        explanation: "Conflicting requirements need stakeholder input to make informed trade-off decisions."
                    },
                    {
                        question: "For an e-commerce checkout system, which non-functional requirement is MOST critical?",
                        options: ["Page load time under 100ms", "Transaction consistency and reliability", "Support for 10 languages", "Dark mode support"],
                        correct: 1,
                        explanation: "For checkout, consistency (no double charges, no lost orders) is critical for business and user trust."
                    },
                    {
                        question: "Amazon's 'Working Backwards' process starts with which artifact?",
                        options: ["Database schema", "Press release describing the finished product", "Architecture diagram", "Cost estimate"],
                        correct: 1,
                        explanation: "Amazon writes the press release first to clarify what success looks like before designing."
                    },
                    {
                        question: "A requirement states 'the system should be fast'. What's wrong with this?",
                        options: ["Nothing, it's clear", "Too vague - needs specific metrics (e.g., 'p99 latency < 200ms')", "Speed doesn't matter", "Should say 'very fast'"],
                        correct: 1,
                        explanation: "Vague requirements like 'fast' are untestable. Need specific, measurable criteria."
                    }
                ],
                'expert': [
                    {
                        question: "You're designing a system for 100M MAU with 50% DAU, average 10 requests/user/day. What's the average QPS?",
                        options: ["About 500 QPS", "About 5,000 QPS", "About 50,000 QPS", "About 500,000 QPS"],
                        correct: 1,
                        explanation: "50M DAU √ó 10 requests = 500M requests/day √∑ 86,400 seconds ‚âà 5,800 QPS"
                    },
                    {
                        question: "A financial trading system needs 'exactly-once' message delivery. Why is this particularly challenging?",
                        options: ["It's actually easy to implement", "Network failures make it impossible to distinguish 'not sent' from 'sent but no ack'", "Databases don't support it", "It's not a real requirement"],
                        correct: 1,
                        explanation: "Network partitions make exactly-once semantically challenging - was the message lost or just the acknowledgment?"
                    },
                    {
                        question: "When designing for 'five 9s' (99.999%) availability, how much downtime is allowed per year?",
                        options: ["About 5 hours", "About 5 minutes", "About 52 minutes", "0 minutes"],
                        correct: 1,
                        explanation: "99.999% = 0.00001 √ó 365 √ó 24 √ó 60 = 5.26 minutes per year"
                    },
                    {
                        question: "A requirement states 'support 1B daily messages with p99 latency < 100ms'. Which constraint does this most impact?",
                        options: ["UI design", "Database and caching architecture", "Logo design", "Documentation style"],
                        correct: 1,
                        explanation: "This scale + latency requirement demands sophisticated database sharding, caching, and potentially in-memory systems."
                    },
                    {
                        question: "Netflix's systems have explicit 'blast radius' requirements. What does this mean?",
                        options: ["How far explosions travel", "The defined scope of impact when a component fails", "Marketing reach", "User growth rate"],
                        correct: 1,
                        explanation: "Blast radius defines what breaks when a component fails, ensuring failures are contained and predictable."
                    }
                ]
            },

            // Day 1: Back-of-Envelope Calculations
            'hld_2': {
                'easy': [
                    {
                        question: "Approximately how many seconds are in a day?",
                        options: ["10,000", "86,400 (approximately 100,000)", "1 million", "360"],
                        correct: 1,
                        explanation: "24 hours √ó 60 minutes √ó 60 seconds = 86,400 seconds ‚âà 100,000 for estimation."
                    },
                    {
                        question: "What does QPS stand for?",
                        options: ["Quality Per Second", "Queries Per Second", "Queue Processing Speed", "Quick Processing System"],
                        correct: 1,
                        explanation: "QPS = Queries Per Second, a measure of system throughput."
                    },
                    {
                        question: "If a system handles 1 million requests per day, what's the approximate QPS?",
                        options: ["About 1,000", "About 12", "About 100", "About 10,000"],
                        correct: 1,
                        explanation: "1,000,000 √∑ 86,400 ‚âà 11.6, so approximately 12 QPS."
                    },
                    {
                        question: "Why do we multiply storage estimates by 3 (replication factor)?",
                        options: ["Data grows 3x yearly", "Redundancy for durability and availability", "Databases are inefficient", "It's a billing trick"],
                        correct: 1,
                        explanation: "Data is typically stored 3 times across different servers/locations for fault tolerance."
                    },
                    {
                        question: "Which is faster: reading from memory or from SSD?",
                        options: ["SSD is faster", "Memory is about 100x faster", "They're the same speed", "Depends on the file type"],
                        correct: 1,
                        explanation: "Memory access: ~100ns, SSD access: ~100Œºs. Memory is about 1000x faster."
                    }
                ],
                'medium': [
                    {
                        question: "A service has 100M daily active users, each making 5 requests/day. What's the average QPS?",
                        options: ["About 500", "About 5,000", "About 50,000", "About 500,000"],
                        correct: 1,
                        explanation: "100M √ó 5 = 500M requests/day √∑ 86,400 ‚âà 5,800 QPS"
                    },
                    {
                        question: "Peak traffic is typically how much higher than average?",
                        options: ["Same as average", "2-3x average", "100x average", "10,000x average"],
                        correct: 1,
                        explanation: "Peak is typically 2-3x average for most systems, though some events can spike higher."
                    },
                    {
                        question: "An image sharing app has 50M daily photos uploaded, each 500KB. What's daily storage (before replication)?",
                        options: ["500 GB", "25 TB", "250 TB", "2.5 PB"],
                        correct: 1,
                        explanation: "50M √ó 500KB = 25,000,000 MB = 25 TB per day"
                    },
                    {
                        question: "Why did YouTube's initial storage estimates fail?",
                        options: ["They overestimated by 10x", "They underestimated by 10x", "Estimates were perfect", "Storage wasn't considered"],
                        correct: 1,
                        explanation: "YouTube underestimated growth by 10x, showing how estimates can miss explosive growth."
                    },
                    {
                        question: "For a read-heavy application (100:1 read/write), which optimization matters most?",
                        options: ["Write performance", "Read performance and caching", "Neither matters", "Only storage matters"],
                        correct: 1,
                        explanation: "With 100:1 ratio, optimizing the 99% (reads) has far more impact than optimizing writes."
                    }
                ],
                'hard': [
                    {
                        question: "Twitter-like service: 200M DAU, 100 tweets read/user/day. What's read QPS?",
                        options: ["About 23,000", "About 230,000", "About 2.3 million", "About 230"],
                        correct: 1,
                        explanation: "200M √ó 100 = 20B reads/day √∑ 86,400 ‚âà 231,000 QPS"
                    },
                    {
                        question: "A video streaming service has 10M concurrent viewers at 4Mbps each. What's the bandwidth requirement?",
                        options: ["40 Gbps", "400 Gbps", "40 Tbps", "4 Tbps"],
                        correct: 2,
                        explanation: "10M √ó 4Mbps = 40,000,000 Mbps = 40 Tbps"
                    },
                    {
                        question: "Your estimate gives 100 PB storage for a service similar to Instagram's scale. Instagram uses ~50 PB. What should you do?",
                        options: ["Your estimate is fine, they're wrong", "Re-check assumptions - being 2x off might indicate missing optimizations", "Ignore the discrepancy", "Triple your estimate"],
                        correct: 1,
                        explanation: "Sanity check against real systems. Being 2x off warrants reviewing assumptions like compression, deduplication."
                    },
                    {
                        question: "Network latency between US coasts is approximately:",
                        options: ["1 ms", "10 ms", "70-100 ms", "500 ms"],
                        correct: 2,
                        explanation: "Cross-country latency is about 70-100ms due to speed of light limits in fiber."
                    },
                    {
                        question: "You're estimating storage for 1 year of tweets: 500M tweets/day, 500 bytes each, 3x replication. Total?",
                        options: ["About 27 PB", "About 270 TB", "About 27 TB", "About 2.7 PB"],
                        correct: 1,
                        explanation: "500M √ó 500 bytes √ó 365 √ó 3 = 273.75 TB ‚âà 274 TB ‚âà 270 TB"
                    }
                ],
                'expert': [
                    {
                        question: "Design a URL shortener: 10M new URLs/month, 1B redirects/month, 7-char URLs (62^7 space). When will you run out of URL space?",
                        options: ["About 30 years", "About 300 years", "About 3,000 years", "Never practically"],
                        correct: 2,
                        explanation: "62^7 = 3.5 trillion codes. At 10M/month = 120M/year, that's 3.5T √∑ 120M ‚âà 29,000 years"
                    },
                    {
                        question: "A global CDN serves 10TB/second of video. How many 100Gbps links minimum?",
                        options: ["10 links", "100 links", "800+ links", "10,000 links"],
                        correct: 2,
                        explanation: "10TB/s = 80Tbps. At 100Gbps per link: 80,000 Gbps √∑ 100 Gbps = 800 links minimum"
                    },
                    {
                        question: "You need to join two tables: 1B rows √ó 1B rows with proper indexes. Estimated time?",
                        options: ["Milliseconds", "Seconds", "Hours to days", "Impossible"],
                        correct: 2,
                        explanation: "Even with indexes, 1B √ó 1B potential comparisons is massive. Such joins take hours and often require distributed processing."
                    },
                    {
                        question: "A distributed database with 1ms network RTT between nodes needs consensus for each write. What's minimum write latency?",
                        options: ["<1ms", "At least 2-3ms (multiple round trips for consensus)", "Exactly 1ms", "Latency doesn't matter"],
                        correct: 1,
                        explanation: "Consensus (like Raft/Paxos) requires multiple round trips: propose + acknowledge from majority = 2-3+ RTTs minimum."
                    },
                    {
                        question: "Google processes 8.5B searches/day. Assuming 10KB average response, daily egress bandwidth is:",
                        options: ["850 GB", "85 TB", "850 TB", "8.5 PB"],
                        correct: 1,
                        explanation: "8.5B √ó 10KB = 85TB per day of search response data alone"
                    }
                ]
            },

            // Day 2: Vertical vs Horizontal Scaling
            'hld_3': {
                'easy': [
                    {
                        question: "What is vertical scaling?",
                        options: ["Adding more servers", "Adding more power to existing server (CPU, RAM)", "Making servers taller", "Scaling in the cloud"],
                        correct: 1,
                        explanation: "Vertical scaling means upgrading a single server with more CPU, RAM, or storage."
                    },
                    {
                        question: "What is horizontal scaling?",
                        options: ["Making servers wider", "Adding more machines to the pool", "Upgrading existing hardware", "Scaling sideways"],
                        correct: 1,
                        explanation: "Horizontal scaling means adding more servers to distribute the load."
                    },
                    {
                        question: "Which scaling approach has a hardware ceiling?",
                        options: ["Horizontal", "Vertical", "Both", "Neither"],
                        correct: 1,
                        explanation: "Vertical scaling is limited by maximum hardware available - you can't buy a 100,000-core CPU."
                    },
                    {
                        question: "Which scaling approach provides better fault tolerance?",
                        options: ["Vertical (bigger server is more reliable)", "Horizontal (if one server dies, others continue)", "They're equal", "Neither provides fault tolerance"],
                        correct: 1,
                        explanation: "Horizontal scaling provides redundancy - losing one server doesn't take down the system."
                    },
                    {
                        question: "What's the main advantage of vertical scaling?",
                        options: ["Unlimited scale", "Simplicity (no distributed system complexity)", "Always cheaper", "Better fault tolerance"],
                        correct: 1,
                        explanation: "Vertical scaling is simpler - no need for load balancers, distributed databases, or coordination."
                    }
                ],
                'medium': [
                    {
                        question: "Netflix switched from vertical to horizontal scaling because:",
                        options: ["Horizontal was cheaper", "They hit the limits of the biggest available server", "Developers preferred it", "Marketing decision"],
                        correct: 1,
                        explanation: "Netflix's Oracle database hit limits that even the largest server couldn't solve, forcing horizontal scaling."
                    },
                    {
                        question: "Which type of scaling typically requires application code changes?",
                        options: ["Vertical only", "Horizontal only", "Both", "Neither"],
                        correct: 1,
                        explanation: "Horizontal scaling often requires code changes for distributed state, session handling, etc."
                    },
                    {
                        question: "For a startup with 1,000 users, which approach is typically recommended?",
                        options: ["Immediately go horizontal for future growth", "Start vertical, scale horizontally when needed", "Use neither", "Multi-region from day one"],
                        correct: 1,
                        explanation: "Start simple with vertical scaling. Add complexity (horizontal) when you actually need it."
                    },
                    {
                        question: "Why is scaling databases horizontally more challenging than scaling web servers?",
                        options: ["Databases are always smaller", "Data consistency and transactions across servers are complex", "Web servers don't scale", "It's actually easier"],
                        correct: 1,
                        explanation: "Distributed databases face challenges with consistency, distributed transactions, and joins."
                    },
                    {
                        question: "Cost grows how with vertical scaling?",
                        options: ["Linearly (2x power = 2x cost)", "Exponentially (2x power = 4x+ cost)", "Decreases with scale", "Stays constant"],
                        correct: 1,
                        explanation: "High-end server hardware costs grow exponentially - a 2x faster server costs much more than 2x the price."
                    }
                ],
                'hard': [
                    {
                        question: "A stateful application stores user sessions in server memory. What's needed to scale horizontally?",
                        options: ["Just add more servers", "Externalize session storage (e.g., Redis)", "Make sessions smaller", "Delete sessions"],
                        correct: 1,
                        explanation: "Sessions must be externalized so any server can handle any user's request."
                    },
                    {
                        question: "Which companies use purely vertical scaling at scale?",
                        options: ["Google, Amazon, Netflix", "Almost none - all major tech companies use horizontal", "Most Fortune 500 companies", "All banks"],
                        correct: 1,
                        explanation: "At massive scale, horizontal scaling is required. No company at Google/Amazon scale uses purely vertical."
                    },
                    {
                        question: "What scaling strategy would you use for a database with mostly writes?",
                        options: ["Scale reads horizontally, writes vertically until pain", "Scale everything horizontally immediately", "Only use vertical", "Writes don't scale"],
                        correct: 0,
                        explanation: "Read scaling (replicas) is easier. Write scaling requires sharding, which is complex. Vertical scaling for writes buys time."
                    },
                    {
                        question: "How does Instagram handle their photo storage at scale?",
                        options: ["Single large server", "Horizontally sharded across many storage servers", "They don't store photos", "Vertical scaling only"],
                        correct: 1,
                        explanation: "Instagram shards photos across many servers - no single server could hold billions of images."
                    },
                    {
                        question: "What's a 'two-pizza team' in Amazon's scaling philosophy?",
                        options: ["Teams that eat pizza", "Small teams (fed by 2 pizzas) own services end-to-end", "Large teams", "Pizza delivery optimization"],
                        correct: 1,
                        explanation: "Amazon structures teams small enough to be fed by 2 pizzas, each owning scalable services."
                    }
                ],
                'expert': [
                    {
                        question: "You have a monolith on a server at 80% CPU. Traffic is growing 20%/month. Best strategy?",
                        options: ["Immediately rewrite as microservices", "Vertical scale now, plan horizontal migration, extract bottlenecks incrementally", "Do nothing", "Add 10 servers immediately"],
                        correct: 1,
                        explanation: "Vertical scaling buys time. Use that time to identify bottlenecks and extract them as horizontally scalable services."
                    },
                    {
                        question: "Google Spanner provides global consistency with horizontal scaling. What's the key enabling technology?",
                        options: ["Bigger servers", "TrueTime (GPS + atomic clocks for global time sync)", "Magic", "Standard databases"],
                        correct: 1,
                        explanation: "Spanner uses TrueTime (atomic clocks + GPS) to order transactions globally with bounded uncertainty."
                    },
                    {
                        question: "Vertical scaling requires downtime for upgrades. Horizontal scaling handles this how?",
                        options: ["Also requires downtime", "Rolling updates - upgrade servers one at a time", "Never needs updates", "Only update on weekends"],
                        correct: 1,
                        explanation: "With multiple servers, you can update them one at a time while others handle traffic (rolling deployment)."
                    },
                    {
                        question: "A service needs exactly-once processing. How does horizontal scaling complicate this?",
                        options: ["It doesn't", "Multiple servers may process same message without coordination", "Horizontal scaling makes it easier", "Only affects vertical scaling"],
                        correct: 1,
                        explanation: "Without careful coordination (distributed locks, idempotency), multiple servers may process the same item."
                    },
                    {
                        question: "What architecture pattern helps transition from vertical to horizontal database scaling?",
                        options: ["Delete data", "CQRS - separate read and write models/databases", "Use smaller servers", "Avoid databases"],
                        correct: 1,
                        explanation: "CQRS separates reads (easily scaled horizontally) from writes (harder to scale), enabling gradual transition."
                    }
                ]
            },

            // Day 2: Load Balancers (hld_4) - continues same pattern
            'hld_4': {
                'easy': [
                    {
                        question: "What does a load balancer do?",
                        options: ["Stores data", "Distributes incoming traffic across multiple servers", "Balances account loads", "Optimizes database queries"],
                        correct: 1,
                        explanation: "A load balancer distributes network traffic across multiple servers to prevent overload."
                    },
                    {
                        question: "Round-robin load balancing distributes requests how?",
                        options: ["To the fastest server", "Sequentially to each server in turn", "Randomly", "To the newest server"],
                        correct: 1,
                        explanation: "Round-robin cycles through servers sequentially: Server 1, Server 2, Server 3, Server 1..."
                    },
                    {
                        question: "What happens if a load balancer sends traffic to a crashed server?",
                        options: ["Traffic is automatically rerouted", "Requests fail unless health checks detect it", "Nothing bad happens", "The load balancer crashes"],
                        correct: 1,
                        explanation: "Without health checks, the LB will send traffic to dead servers, causing failures."
                    },
                    {
                        question: "Layer 4 load balancers operate at which OSI layer?",
                        options: ["Application layer", "Transport layer (TCP/UDP)", "Physical layer", "Session layer"],
                        correct: 1,
                        explanation: "Layer 4 (Transport) sees IP addresses and ports, but not HTTP content."
                    },
                    {
                        question: "Layer 7 load balancers can route based on:",
                        options: ["Only IP addresses", "URL paths, headers, cookies (HTTP content)", "Physical location only", "Server color"],
                        correct: 1,
                        explanation: "Layer 7 (Application) can inspect HTTP content and route based on URLs, headers, etc."
                    }
                ],
                'medium': [
                    {
                        question: "Least-connections load balancing sends requests to:",
                        options: ["Random server", "Server with fewest active connections", "Fastest server", "Server with most RAM"],
                        correct: 1,
                        explanation: "Least-connections routes to the server handling fewest requests, adapting to actual load."
                    },
                    {
                        question: "IP hash load balancing is useful for:",
                        options: ["Random distribution", "Session affinity (same client ‚Üí same server)", "Faster routing", "Reducing costs"],
                        correct: 1,
                        explanation: "IP hash ensures the same client IP always reaches the same server, useful for session stickiness."
                    },
                    {
                        question: "What's the main risk of having a single load balancer?",
                        options: ["It's too fast", "It becomes a single point of failure", "It's too cheap", "It's too simple"],
                        correct: 1,
                        explanation: "A single load balancer failing takes down the entire system. Always have redundant LBs."
                    },
                    {
                        question: "SSL termination at the load balancer means:",
                        options: ["SSL is disabled", "LB decrypts HTTPS, forwards HTTP to servers internally", "Servers handle all SSL", "SSL certificates expire"],
                        correct: 1,
                        explanation: "SSL termination at LB offloads crypto work from servers and centralizes certificate management."
                    },
                    {
                        question: "During a traffic spike, why might weighted round-robin outperform simple round-robin?",
                        options: ["It's faster", "It can send more traffic to more powerful servers", "It uses less memory", "It doesn't"],
                        correct: 1,
                        explanation: "Weighted round-robin sends more requests to servers with higher capacity (weight)."
                    }
                ],
                'hard': [
                    {
                        question: "Health checks that ping /health every 5 seconds will detect a crashed server in:",
                        options: ["Instantly", "Up to 5 seconds + check timeout", "Never", "Exactly 5 seconds"],
                        correct: 1,
                        explanation: "Detection takes at most the check interval plus timeout - potentially 5-10 seconds."
                    },
                    {
                        question: "Connection draining during server removal helps by:",
                        options: ["Speeding up removal", "Letting existing requests complete before removing server", "Draining server memory", "Reducing costs"],
                        correct: 1,
                        explanation: "Draining lets in-flight requests finish, preventing errors for users mid-request."
                    },
                    {
                        question: "For a WebSocket application, which load balancer configuration is most important?",
                        options: ["Round-robin", "Connection awareness - persistent connection handling", "Fastest response time", "Random distribution"],
                        correct: 1,
                        explanation: "WebSockets are persistent connections requiring connection-aware load balancing."
                    },
                    {
                        question: "AWS ALB vs NLB: Which handles millions of connections with static IPs?",
                        options: ["ALB (Application Load Balancer)", "NLB (Network Load Balancer)", "Both equally", "Neither"],
                        correct: 1,
                        explanation: "NLB (Layer 4) handles extreme throughput with static IPs. ALB is Layer 7 (HTTP-aware)."
                    },
                    {
                        question: "A load balancer has max 65,535 connections per destination port. How to handle more?",
                        options: ["Impossible", "Multiple LBs, multiple source IPs, or use different ports", "Upgrade LB firmware", "Reduce traffic"],
                        correct: 1,
                        explanation: "Port exhaustion is real. Use multiple LBs, source IPs, or ports to multiply available connections."
                    }
                ],
                'expert': [
                    {
                        question: "During a Super Bowl traffic spike (10x normal), which LB algorithm handles best without pre-configuration?",
                        options: ["Round-robin", "Least-connections (adapts to actual load)", "Static weighted", "Random"],
                        correct: 1,
                        explanation: "Least-connections automatically adapts as some servers get overloaded, without needing manual weight adjustment."
                    },
                    {
                        question: "Consistent hashing for load balancing minimizes disruption when:",
                        options: ["Traffic increases", "Servers are added/removed (minimal remapping)", "SSL is enabled", "Costs increase"],
                        correct: 1,
                        explanation: "Adding/removing servers with consistent hashing only remaps ~1/n of requests, not all."
                    },
                    {
                        question: "Global load balancing (GSLB) routes users based on:",
                        options: ["Server CPU only", "Geographic proximity, health, and latency", "Random selection", "Alphabetical order"],
                        correct: 1,
                        explanation: "GSLB routes to nearest/healthiest/fastest region using DNS and anycast."
                    },
                    {
                        question: "An LB detects Server A has 10ms latency, Server B has 100ms. Using least-response-time, what happens?",
                        options: ["Traffic split equally", "Server A gets most traffic until its latency rises", "Server B gets priority", "Both servers are removed"],
                        correct: 1,
                        explanation: "Least-response-time sends traffic to faster server until its increased load raises latency."
                    },
                    {
                        question: "To achieve truly seamless failover, load balancers can use:",
                        options: ["Only round-robin", "Active-active with shared state, or anycast with health checks", "No configuration needed", "Manual intervention only"],
                        correct: 1,
                        explanation: "Seamless failover needs active-active LBs (shared state) or anycast (automatic IP failover)."
                    }
                ]
            },

            // Continue with remaining concepts...
            'hld_5': {
                'easy': [
                    {
                        question: "What is 'state' in the context of servers?",
                        options: ["Geographic location", "Data that must be remembered between requests", "Server version", "Power state"],
                        correct: 1,
                        explanation: "State is data (like sessions, carts) that persists between requests."
                    },
                    {
                        question: "A stateless server:",
                        options: ["Has no data", "Doesn't keep client data in memory - stores it externally", "Is always off", "Can't process requests"],
                        correct: 1,
                        explanation: "Stateless servers don't store client data locally; any server can handle any request."
                    },
                    {
                        question: "Why is stateless architecture easier to scale horizontally?",
                        options: ["Stateless servers are faster", "Any server can handle any request - no session affinity needed", "It's not easier", "Stateless uses less memory"],
                        correct: 1,
                        explanation: "Without local state, requests can go to any server, making it easy to add more."
                    },
                    {
                        question: "JWT (JSON Web Token) enables stateless authentication because:",
                        options: ["It's encrypted", "The token contains user info; servers don't need to store sessions", "It's faster", "It's newer"],
                        correct: 1,
                        explanation: "JWTs contain all needed info (user ID, permissions), so servers don't need session storage."
                    },
                    {
                        question: "Which component is inherently stateful?",
                        options: ["Load balancer", "CDN", "Database", "Stateless API"],
                        correct: 2,
                        explanation: "Databases store data - that's their entire purpose. They're stateful by design."
                    }
                ],
                'medium': [
                    {
                        question: "Sticky sessions solve what problem?",
                        options: ["Performance issues", "Routing user to the same server that has their session", "Security issues", "Network issues"],
                        correct: 1,
                        explanation: "Sticky sessions ensure users hit the same server where their session lives."
                    },
                    {
                        question: "The main drawback of sticky sessions is:",
                        options: ["Too fast", "Defeats load balancing benefits; server loss loses sessions", "Too secure", "Too simple"],
                        correct: 1,
                        explanation: "Sticky sessions prevent even distribution and lose sessions if that server dies."
                    },
                    {
                        question: "Redis is commonly used for session storage because:",
                        options: ["It's a database", "Fast in-memory access with optional persistence", "It's free", "It's required by law"],
                        correct: 1,
                        explanation: "Redis provides fast, distributed session storage accessible by all servers."
                    },
                    {
                        question: "WebSocket connections are inherently:",
                        options: ["Stateless", "Stateful (persistent connection)", "Neither", "Both"],
                        correct: 1,
                        explanation: "WebSockets maintain persistent connections - they're stateful by nature."
                    },
                    {
                        question: "How do you scale WebSocket servers?",
                        options: ["You can't", "Use pub/sub (Redis) for cross-server communication", "Only vertical scaling", "Replace with HTTP"],
                        correct: 1,
                        explanation: "Pub/sub systems let WebSocket servers communicate, enabling horizontal scaling."
                    }
                ],
                'hard': [
                    {
                        question: "A user's shopping cart should persist if their server dies. Best approach?",
                        options: ["Accept cart loss", "Store cart in external storage (Redis/database), not server memory", "Use bigger servers", "Don't allow server failures"],
                        correct: 1,
                        explanation: "Externalizing cart storage means any server can access it, surviving server failures."
                    },
                    {
                        question: "JWT tokens are signed, not encrypted by default. This means:",
                        options: ["They're insecure", "Content is readable but tamper-proof", "They can't be used", "They're encrypted"],
                        correct: 1,
                        explanation: "Signed JWTs can be read by anyone but can't be modified without detection."
                    },
                    {
                        question: "How does Uber handle driver location state at scale?",
                        options: ["Single database", "Specialized real-time systems with in-memory state and geographic sharding", "Stateless everywhere", "No location tracking"],
                        correct: 1,
                        explanation: "Location tracking requires specialized real-time systems optimized for high-frequency updates."
                    },
                    {
                        question: "Token-based auth vs session-based: which is more suitable for microservices?",
                        options: ["Session-based (simpler)", "Token-based (each service validates independently)", "Neither works", "Both equal"],
                        correct: 1,
                        explanation: "Tokens let each microservice validate auth independently without shared session storage."
                    },
                    {
                        question: "When is server-side state unavoidable?",
                        options: ["Never", "Real-time games, video streaming with buffering", "Always", "Web pages only"],
                        correct: 1,
                        explanation: "Some use cases require local state for latency (games) or continuity (video buffers)."
                    }
                ],
                'expert': [
                    {
                        question: "Slack maintains millions of WebSocket connections. How do they handle cross-server messaging?",
                        options: ["Single server", "Redis pub/sub - publish to channel, all servers receive", "They don't", "Only polling"],
                        correct: 1,
                        explanation: "Redis pub/sub enables message delivery across servers to reach connected clients."
                    },
                    {
                        question: "A JWT contains user_id, role, and exp (expiry). If user's role changes mid-session:",
                        options: ["Token auto-updates", "Old token remains valid until expiry (stale permissions)", "Token immediately invalid", "Server crashes"],
                        correct: 1,
                        explanation: "JWTs are self-contained - role changes aren't reflected until new token is issued."
                    },
                    {
                        question: "Netflix handles per-user state (viewing position, preferences) how?",
                        options: ["Stateful servers per user", "Cassandra for state, stateless streaming services", "Local storage only", "No personalization"],
                        correct: 1,
                        explanation: "Netflix uses Cassandra (distributed DB) for user state, keeping streaming services stateless."
                    },
                    {
                        question: "For a collaborative document editor, state management is handled by:",
                        options: ["Stateless only", "CRDTs or Operational Transforms with state synchronization", "No state needed", "Client-side only"],
                        correct: 1,
                        explanation: "Real-time collaboration requires sophisticated state sync like CRDTs or OT."
                    },
                    {
                        question: "What's the 'state spectrum' principle?",
                        options: ["All or nothing", "Minimize state, externalize what remains to appropriate storage", "Maximum state everywhere", "State doesn't exist"],
                        correct: 1,
                        explanation: "Minimize state where possible, put necessary state in appropriate external stores."
                    }
                ]
            },

            // Day 3 concepts (hld_6, hld_7, hld_8) follow same pattern
            'hld_6': {
                'easy': [
                    {
                        question: "What does HTTP stand for?",
                        options: ["High Transfer Text Protocol", "HyperText Transfer Protocol", "Hyper Transfer Text Process", "High Text Transfer Process"],
                        correct: 1,
                        explanation: "HTTP = HyperText Transfer Protocol, the foundation of web communication."
                    },
                    {
                        question: "Which HTTP method is used to retrieve data?",
                        options: ["POST", "PUT", "GET", "DELETE"],
                        correct: 2,
                        explanation: "GET retrieves data without modifying server state."
                    },
                    {
                        question: "REST APIs use resources as:",
                        options: ["Verbs (/createUser)", "Nouns (/users)", "Adjectives", "Random strings"],
                        correct: 1,
                        explanation: "REST uses nouns (resources) like /users, not verbs like /createUser."
                    },
                    {
                        question: "HTTP status code 404 means:",
                        options: ["Success", "Server error", "Resource not found", "Created"],
                        correct: 2,
                        explanation: "404 indicates the requested resource doesn't exist."
                    },
                    {
                        question: "Which status code indicates successful creation?",
                        options: ["200 OK", "201 Created", "404 Not Found", "500 Server Error"],
                        correct: 1,
                        explanation: "201 indicates a new resource was successfully created."
                    }
                ],
                'medium': [
                    {
                        question: "An idempotent HTTP method means:",
                        options: ["It's faster", "Calling it multiple times has the same effect as once", "It never fails", "It's more secure"],
                        correct: 1,
                        explanation: "Idempotent means multiple identical requests produce the same result as one."
                    },
                    {
                        question: "Which methods are idempotent?",
                        options: ["POST only", "GET, PUT, DELETE", "None", "All methods"],
                        correct: 1,
                        explanation: "GET (read), PUT (replace), DELETE (remove) are idempotent. POST (create) is not."
                    },
                    {
                        question: "Cursor-based pagination is preferred over offset because:",
                        options: ["It's simpler", "It handles real-time data changes better", "It's older", "It's faster to implement"],
                        correct: 1,
                        explanation: "Cursor pagination handles inserts/deletes during pagination; offset skips or duplicates items."
                    },
                    {
                        question: "API versioning helps with:",
                        options: ["Performance", "Evolving APIs without breaking existing clients", "Security", "Caching"],
                        correct: 1,
                        explanation: "Versioning (/v1/, /v2/) lets you change APIs while old clients still work."
                    },
                    {
                        question: "HTTP 429 status code indicates:",
                        options: ["Not found", "Server error", "Rate limit exceeded (too many requests)", "Authentication required"],
                        correct: 2,
                        explanation: "429 tells clients they've exceeded their request quota."
                    }
                ],
                'hard': [
                    {
                        question: "PUT vs PATCH: What's the key difference?",
                        options: ["PUT is faster", "PUT replaces entire resource, PATCH modifies partially", "They're identical", "PATCH creates resources"],
                        correct: 1,
                        explanation: "PUT replaces the whole resource; PATCH only updates specified fields."
                    },
                    {
                        question: "Why is 'GET /users/123/delete' bad REST design?",
                        options: ["Too long", "GET with side effects; should be DELETE /users/123", "Users shouldn't be deleted", "123 is invalid"],
                        correct: 1,
                        explanation: "GET should be safe (no side effects). Deletion should use DELETE method."
                    },
                    {
                        question: "Stripe's API is considered exemplary because:",
                        options: ["It's free", "Consistent patterns, excellent errors, clear documentation", "It's the oldest", "It uses GraphQL"],
                        correct: 1,
                        explanation: "Stripe's API is renowned for developer experience: consistency, helpful errors, great docs."
                    },
                    {
                        question: "HATEOAS in REST means:",
                        options: ["A type of error", "Responses include links to related actions/resources", "Hate speech detection", "API versioning"],
                        correct: 1,
                        explanation: "HATEOAS: Hypermedia As The Engine Of Application State - responses guide client to next actions."
                    },
                    {
                        question: "For filtering and sorting in REST, the best practice is:",
                        options: ["Use POST body", "Query parameters (/users?status=active&sort=-created)", "URL path segments", "Headers only"],
                        correct: 1,
                        explanation: "Query params are standard for filtering/sorting: /users?status=active&sort=-created_at"
                    }
                ],
                'expert': [
                    {
                        question: "Your API returns 200 OK with {'error': 'not found'} in the body. Why is this problematic?",
                        options: ["It's correct", "Clients expect status codes to indicate errors; 200 means success", "Error messages aren't allowed", "JSON is wrong format"],
                        correct: 1,
                        explanation: "HTTP status codes should reflect result. Using 200 for errors breaks client expectations."
                    },
                    {
                        question: "GitHub's API supports both REST and GraphQL because:",
                        options: ["They couldn't decide", "REST for simplicity, GraphQL for complex queries reducing round-trips", "Legal requirement", "Random choice"],
                        correct: 1,
                        explanation: "REST works well for simple CRUD; GraphQL lets clients request exactly what they need in one call."
                    },
                    {
                        question: "ETag headers help with:",
                        options: ["Security", "Conditional requests (only fetch if changed), saving bandwidth", "Routing", "Rate limiting"],
                        correct: 1,
                        explanation: "ETags enable conditional GET - only transfer data if it changed since last request."
                    },
                    {
                        question: "For a public API, which versioning strategy scales best?",
                        options: ["No versioning", "URL versioning (/v1/) - explicit and discoverable", "Random versioning", "Date-based only"],
                        correct: 1,
                        explanation: "URL versioning is explicit, cacheable, and easy for clients to understand."
                    },
                    {
                        question: "API rate limiting should return:",
                        options: ["200 OK always", "429 with Retry-After header and limit info", "500 error", "No response"],
                        correct: 1,
                        explanation: "429 with Retry-After tells clients when to retry; include remaining limits in headers."
                    }
                ]
            },

            'hld_7': {
                'easy': [
                    {
                        question: "What problem does WebSocket solve?",
                        options: ["File storage", "Real-time bidirectional communication", "Database queries", "CSS styling"],
                        correct: 1,
                        explanation: "WebSockets enable real-time, two-way communication between client and server."
                    },
                    {
                        question: "HTTP polling means:",
                        options: ["Server pushes data", "Client repeatedly asks server for updates", "Data is streamed", "Connection stays open"],
                        correct: 1,
                        explanation: "Polling: client repeatedly requests data, wasting resources if nothing changed."
                    },
                    {
                        question: "Server-Sent Events (SSE) allow:",
                        options: ["Two-way communication", "Server to push updates to client (one-way)", "File uploads", "Database access"],
                        correct: 1,
                        explanation: "SSE is server‚Üíclient only. For bidirectional, use WebSockets."
                    },
                    {
                        question: "WebSocket connections are:",
                        options: ["Short-lived like HTTP", "Persistent until closed", "Only for mobile apps", "Text-only"],
                        correct: 1,
                        explanation: "WebSocket connections stay open for ongoing bidirectional messaging."
                    },
                    {
                        question: "Which application most needs WebSockets?",
                        options: ["Blog (static content)", "Live chat application", "E-commerce catalog", "Documentation site"],
                        correct: 1,
                        explanation: "Live chat needs instant message delivery - WebSockets are ideal."
                    }
                ],
                'medium': [
                    {
                        question: "Why was Facebook's polling approach problematic at scale?",
                        options: ["Too slow", "Millions of requests returning 'no new messages' wasted server resources", "Users didn't like it", "It was too fast"],
                        correct: 1,
                        explanation: "1M users polling every 3 sec = 333K requests/sec, mostly returning nothing."
                    },
                    {
                        question: "The WebSocket handshake uses which protocol initially?",
                        options: ["WebSocket directly", "HTTP (then upgrades to WebSocket)", "TCP only", "UDP"],
                        correct: 1,
                        explanation: "WebSocket starts with HTTP request including 'Upgrade: websocket' header."
                    },
                    {
                        question: "Long polling differs from short polling how?",
                        options: ["Longer URLs", "Server holds connection open until data is available", "More frequent requests", "Uses more memory"],
                        correct: 1,
                        explanation: "Long polling: server waits to respond until it has data, reducing empty responses."
                    },
                    {
                        question: "WebSocket heartbeats (ping/pong) are used for:",
                        options: ["Speed testing", "Detecting dead connections", "Message encryption", "Load balancing"],
                        correct: 1,
                        explanation: "Heartbeats detect connections that died silently without proper close."
                    },
                    {
                        question: "Redis Pub/Sub helps scale WebSockets by:",
                        options: ["Storing messages", "Enabling cross-server message delivery", "Compressing data", "Encrypting connections"],
                        correct: 1,
                        explanation: "When User A on Server 1 messages User B on Server 2, pub/sub routes the message."
                    }
                ],
                'hard': [
                    {
                        question: "A WebSocket server has 10K connections. Client sends message to offline user. Best handling?",
                        options: ["Drop message", "Queue message for delivery when recipient reconnects", "Keep trying forever", "Send error"],
                        correct: 1,
                        explanation: "Message queuing ensures offline users receive messages when they reconnect."
                    },
                    {
                        question: "Discord handles millions of concurrent connections. The key architectural pattern is:",
                        options: ["Single server", "Sharded gateways with message routing", "Only polling", "Client-side only"],
                        correct: 1,
                        explanation: "Discord shards connections across many gateway servers with intelligent routing."
                    },
                    {
                        question: "WebSocket connection limits per server are constrained by:",
                        options: ["Only RAM", "File descriptors, ports, memory, and CPU", "Just CPU", "Disk space"],
                        correct: 1,
                        explanation: "Each connection uses file descriptors, memory, and CPU for processing."
                    },
                    {
                        question: "For a trading platform, WebSocket message ordering is critical. How to guarantee it?",
                        options: ["Hope for the best", "Sequence numbers in messages, client-side reordering", "TCP guarantees it", "Order doesn't matter"],
                        correct: 1,
                        explanation: "TCP orders per-connection, but with reconnects or multiple servers, sequence numbers are needed."
                    },
                    {
                        question: "Figma's real-time collaboration uses WebSockets plus:",
                        options: ["Just WebSockets", "CRDTs/Operational Transforms for conflict-free editing", "Polling as backup", "Manual refresh"],
                        correct: 1,
                        explanation: "CRDTs enable multiple users to edit simultaneously without conflicts."
                    }
                ],
                'expert': [
                    {
                        question: "WebSocket horizontal scaling challenge: 1M users across 100 servers. User A on Server 1 messages User B on Server 47. Solution?",
                        options: ["Direct connection between servers", "Pub/sub backbone (Redis) - Server 1 publishes, Server 47 subscribes and delivers", "Isn't possible", "Route all through one server"],
                        correct: 1,
                        explanation: "Pub/sub enables any-to-any server communication for message routing."
                    },
                    {
                        question: "Memory leak in WebSocket servers commonly occurs from:",
                        options: ["Too much RAM", "Not cleaning up closed connections properly", "Using too many messages", "Slow network"],
                        correct: 1,
                        explanation: "Failing to remove closed connections from tracking sets/maps causes memory leaks."
                    },
                    {
                        question: "To achieve exactly-once message delivery over WebSockets:",
                        options: ["WebSocket guarantees it", "Application-level deduplication with message IDs", "TCP handles it", "Not possible or necessary"],
                        correct: 1,
                        explanation: "Application must handle deduplication since reconnects may cause re-sends."
                    },
                    {
                        question: "WhatsApp ensures message delivery using:",
                        options: ["Just WebSockets", "Message queuing with acknowledgment and retry", "Polling", "Hope"],
                        correct: 1,
                        explanation: "Messages are queued, delivered, and acknowledged. Unacked messages are retried."
                    },
                    {
                        question: "For browser compatibility, gRPC needs:",
                        options: ["Nothing special", "gRPC-Web proxy (browsers don't support native gRPC)", "WebSocket only", "REST wrapper"],
                        correct: 1,
                        explanation: "Browsers don't support HTTP/2 trailers needed by gRPC; gRPC-Web translates."
                    }
                ]
            },

            'hld_8': {
                'easy': [
                    {
                        question: "What does RPC stand for?",
                        options: ["Remote Processing Call", "Remote Procedure Call", "Rapid Protocol Communication", "Random Process Control"],
                        correct: 1,
                        explanation: "RPC = Remote Procedure Call, making remote calls feel like local function calls."
                    },
                    {
                        question: "gRPC was developed by:",
                        options: ["Facebook", "Google", "Amazon", "Microsoft"],
                        correct: 1,
                        explanation: "gRPC is Google's open-source RPC framework."
                    },
                    {
                        question: "Protocol Buffers (protobuf) are:",
                        options: ["Network protocols", "Binary serialization format used by gRPC", "Database buffers", "Memory protection"],
                        correct: 1,
                        explanation: "Protobuf is a compact binary format for serializing structured data."
                    },
                    {
                        question: "Compared to JSON, Protocol Buffers are:",
                        options: ["Larger and slower", "Smaller and faster (binary vs text)", "The same", "Only for images"],
                        correct: 1,
                        explanation: "Protobuf is binary (smaller) and faster to parse than text-based JSON."
                    },
                    {
                        question: "gRPC uses which HTTP version?",
                        options: ["HTTP/1.0", "HTTP/1.1", "HTTP/2", "HTTP/3"],
                        correct: 2,
                        explanation: "gRPC uses HTTP/2 for multiplexing and bidirectional streaming."
                    }
                ],
                'medium': [
                    {
                        question: ".proto files define:",
                        options: ["Database schemas", "Service interfaces and message formats for gRPC", "HTML templates", "CSS styles"],
                        correct: 1,
                        explanation: ".proto files define services and messages, from which code is generated."
                    },
                    {
                        question: "gRPC supports which communication patterns?",
                        options: ["Only request-response", "Unary, server streaming, client streaming, bidirectional streaming", "Only streaming", "Only bidirectional"],
                        correct: 1,
                        explanation: "gRPC supports all four patterns: unary, and three streaming variations."
                    },
                    {
                        question: "Uber uses gRPC instead of REST internally because:",
                        options: ["REST doesn't work", "Higher performance with less serialization overhead between 2000+ services", "It's required", "Developers prefer it"],
                        correct: 1,
                        explanation: "With thousands of services, gRPC's performance gains (10x over JSON) are significant."
                    },
                    {
                        question: "Type safety in gRPC means:",
                        options: ["Better security", "Compile-time checking of message formats, catching errors early", "Faster runtime", "Smaller messages"],
                        correct: 1,
                        explanation: "Generated code ensures type correctness at compile time, not runtime."
                    },
                    {
                        question: "When should you prefer REST over gRPC?",
                        options: ["Always", "Public APIs, browser clients, simplicity valued", "Never", "Only for streaming"],
                        correct: 1,
                        explanation: "REST is better for public APIs (universal client support) and simpler use cases."
                    }
                ],
                'hard': [
                    {
                        question: "gRPC bidirectional streaming is ideal for:",
                        options: ["Simple requests", "Real-time chat, collaborative editing", "File downloads", "Static websites"],
                        correct: 1,
                        explanation: "Bidirectional streaming enables real-time, two-way communication like chat."
                    },
                    {
                        question: "Debugging gRPC is harder than REST because:",
                        options: ["It's newer", "Binary protocol isn't human-readable like JSON", "It's encrypted", "There are no tools"],
                        correct: 1,
                        explanation: "Binary protobuf requires special tools to inspect, unlike readable JSON."
                    },
                    {
                        question: "HTTP/2 multiplexing in gRPC means:",
                        options: ["Multiple copies", "Multiple requests/responses over single TCP connection", "Faster encryption", "Better compression"],
                        correct: 1,
                        explanation: "Multiplexing avoids connection overhead by reusing one connection for many calls."
                    },
                    {
                        question: "Proto field numbers (= 1, = 2) are important because:",
                        options: ["Just for ordering", "They're used in binary encoding, changing breaks compatibility", "Documentation only", "Optional"],
                        correct: 1,
                        explanation: "Field numbers are encoded in binary. Changing them breaks existing data/clients."
                    },
                    {
                        question: "Load balancing gRPC differs from HTTP/1.1 because:",
                        options: ["It's the same", "HTTP/2 connections are long-lived, need connection-aware LBs", "gRPC doesn't use load balancers", "Only client-side balancing works"],
                        correct: 1,
                        explanation: "HTTP/2 multiplexing means one connection handles many calls, requiring smarter LBs."
                    }
                ],
                'expert': [
                    {
                        question: "Netflix uses gRPC internally but REST externally because:",
                        options: ["They couldn't decide", "Internal: performance matters. External: universal client support needed", "Random choice", "Legal reasons"],
                        correct: 1,
                        explanation: "Internal services benefit from gRPC performance; public APIs need broad compatibility."
                    },
                    {
                        question: "Server reflection in gRPC allows:",
                        options: ["Mirroring servers", "Runtime service discovery without .proto files", "Reflecting errors", "Better performance"],
                        correct: 1,
                        explanation: "Reflection lets clients discover services dynamically, useful for debugging tools."
                    },
                    {
                        question: "Adding a new required field to a proto message:",
                        options: ["Is safe", "Breaks existing clients (they can't provide the field)", "Improves performance", "Is recommended"],
                        correct: 1,
                        explanation: "Required fields break backward compatibility; prefer optional with defaults."
                    },
                    {
                        question: "gRPC interceptors are used for:",
                        options: ["Blocking traffic", "Cross-cutting concerns: logging, auth, metrics", "Faster serialization", "Message compression"],
                        correct: 1,
                        explanation: "Interceptors add behavior (logging, auth) to all calls without changing each service."
                    },
                    {
                        question: "For a service mesh (Istio), gRPC integration provides:",
                        options: ["No benefit", "Automatic load balancing, retries, observability", "Only naming", "Security only"],
                        correct: 1,
                        explanation: "Service meshes enhance gRPC with traffic management, resilience, and observability."
                    }
                ]
            },

            // Day 4: Databases - SQL vs NoSQL
            'hld_9': {
                'easy': [
                    {
                        question: "SQL databases use which type of schema?",
                        options: ["Flexible schema", "Fixed schema with tables and relationships", "No schema", "Binary schema"],
                        correct: 1,
                        explanation: "SQL databases have predefined schemas with tables, columns, and relationships."
                    },
                    {
                        question: "NoSQL stands for:",
                        options: ["No SQL at all", "Not Only SQL", "New SQL", "Next SQL"],
                        correct: 1,
                        explanation: "NoSQL means 'Not Only SQL' - it includes SQL-like features but also other data models."
                    },
                    {
                        question: "ACID in databases stands for:",
                        options: ["A database vendor", "Atomicity, Consistency, Isolation, Durability", "A programming language", "Advanced Computing Interface Design"],
                        correct: 1,
                        explanation: "ACID properties ensure reliable database transactions."
                    },
                    {
                        question: "Which is an example of a SQL database?",
                        options: ["MongoDB", "Redis", "PostgreSQL", "Cassandra"],
                        correct: 2,
                        explanation: "PostgreSQL is a relational SQL database. MongoDB, Redis, and Cassandra are NoSQL."
                    },
                    {
                        question: "Document databases store data as:",
                        options: ["Tables with rows", "JSON-like documents", "Key-value pairs only", "Graphs"],
                        correct: 1,
                        explanation: "Document databases like MongoDB store data as flexible JSON-like documents."
                    }
                ],
                'medium': [
                    {
                        question: "When should you choose SQL over NoSQL?",
                        options: ["Always", "When you need complex transactions and data integrity", "Never", "Only for small data"],
                        correct: 1,
                        explanation: "SQL databases excel at complex transactions, joins, and maintaining data integrity."
                    },
                    {
                        question: "BASE properties in NoSQL mean:",
                        options: ["Basic SQL", "Basically Available, Soft state, Eventually consistent", "Better And Simpler Experience", "A database type"],
                        correct: 1,
                        explanation: "BASE is the NoSQL alternative to ACID, prioritizing availability over immediate consistency."
                    },
                    {
                        question: "MongoDB is best suited for:",
                        options: ["Banking transactions", "Flexible, evolving data models and rapid development", "Graph relationships", "Time-series data"],
                        correct: 1,
                        explanation: "MongoDB's flexible schema is ideal for evolving data models and rapid iteration."
                    },
                    {
                        question: "A wide-column store like Cassandra is optimized for:",
                        options: ["Complex joins", "High write throughput and time-series data", "Graph traversals", "Small datasets"],
                        correct: 1,
                        explanation: "Cassandra excels at high write volumes and time-series data across clusters."
                    },
                    {
                        question: "Graph databases like Neo4j are best for:",
                        options: ["Simple key lookups", "Highly connected data with complex relationships", "Document storage", "Time-series"],
                        correct: 1,
                        explanation: "Graph databases excel at traversing relationships like social networks or recommendations."
                    }
                ],
                'hard': [
                    {
                        question: "The CAP theorem states that a distributed database can have at most:",
                        options: ["All three: Consistency, Availability, Partition tolerance", "Two of three: Consistency, Availability, Partition tolerance", "One property only", "None of these"],
                        correct: 1,
                        explanation: "CAP theorem: during a partition, you must choose between consistency and availability."
                    },
                    {
                        question: "Facebook uses MySQL for social graphs because:",
                        options: ["It's the only option", "They built custom sharding and caching to overcome limitations", "NoSQL didn't exist", "It's the cheapest"],
                        correct: 1,
                        explanation: "Facebook heavily customized MySQL with sharding and TAO cache layer for their scale."
                    },
                    {
                        question: "Eventual consistency means:",
                        options: ["Never consistent", "Given enough time without updates, all replicas converge to same value", "Always consistent", "Random consistency"],
                        correct: 1,
                        explanation: "Eventually consistent systems will become consistent if no new updates are made."
                    },
                    {
                        question: "Redis is commonly used as:",
                        options: ["Primary database", "Cache layer in front of other databases", "File storage", "Backup solution"],
                        correct: 1,
                        explanation: "Redis's in-memory speed makes it ideal as a caching layer before slower databases."
                    },
                    {
                        question: "Polyglot persistence means:",
                        options: ["Multiple languages", "Using different databases for different use cases in one system", "One database for all", "Backup databases"],
                        correct: 1,
                        explanation: "Polyglot persistence: choose the right database for each specific use case."
                    }
                ],
                'expert': [
                    {
                        question: "DynamoDB's partition key design is critical because:",
                        options: ["Aesthetic reasons", "Poor key choice creates hot partitions and throttling", "It's optional", "Only affects naming"],
                        correct: 1,
                        explanation: "Uneven partition key distribution leads to hot spots and performance issues."
                    },
                    {
                        question: "Twitter moved from MySQL to Manhattan (internal NoSQL) because:",
                        options: ["Cost savings", "Timeline queries needed better write performance and horizontal scaling", "MySQL is bad", "Random decision"],
                        correct: 1,
                        explanation: "Twitter's timeline feature required high write throughput that MySQL couldn't handle."
                    },
                    {
                        question: "In a multi-region setup, Google Spanner achieves external consistency using:",
                        options: ["Magic", "TrueTime API with GPS and atomic clocks", "Regular timestamps", "No consistency"],
                        correct: 1,
                        explanation: "Spanner uses TrueTime (GPS + atomic clocks) for globally consistent timestamps."
                    },
                    {
                        question: "The best approach for a system needing both transactions and scale is:",
                        options: ["Avoid the problem", "Use distributed SQL like CockroachDB or Spanner, or carefully separate concerns", "Use only NoSQL", "Use only SQL"],
                        correct: 1,
                        explanation: "NewSQL databases or carefully designed hybrid architectures can provide both."
                    },
                    {
                        question: "Schema evolution in production databases is challenging because:",
                        options: ["It's not", "Migrations can lock tables, require downtime, and break applications", "Schemas never change", "Only affects NoSQL"],
                        correct: 1,
                        explanation: "Schema changes on large tables can cause outages without careful planning."
                    }
                ]
            },

            // Day 4: Database Sharding
            'hld_10': {
                'easy': [
                    {
                        question: "Database sharding is:",
                        options: ["Deleting data", "Splitting data across multiple database servers", "Compressing data", "Encrypting data"],
                        correct: 1,
                        explanation: "Sharding distributes data horizontally across multiple database instances."
                    },
                    {
                        question: "A shard key determines:",
                        options: ["Database password", "Which shard stores each piece of data", "Table names", "Column types"],
                        correct: 1,
                        explanation: "The shard key is used to calculate which shard holds each record."
                    },
                    {
                        question: "Hash-based sharding distributes data using:",
                        options: ["Alphabetical order", "A hash function on the shard key", "Random selection", "Date ranges"],
                        correct: 1,
                        explanation: "Hash sharding applies a hash function to the key to determine the shard."
                    },
                    {
                        question: "Range-based sharding groups data by:",
                        options: ["Random values", "Ranges of the shard key (e.g., A-M, N-Z)", "Hash values", "File size"],
                        correct: 1,
                        explanation: "Range sharding assigns continuous ranges of keys to each shard."
                    },
                    {
                        question: "The main benefit of sharding is:",
                        options: ["Simpler queries", "Horizontal scalability beyond single server limits", "Lower cost always", "Better security"],
                        correct: 1,
                        explanation: "Sharding allows scaling beyond what a single database server can handle."
                    }
                ],
                'medium': [
                    {
                        question: "A hot shard occurs when:",
                        options: ["Server overheats", "One shard receives disproportionate traffic", "Shards are balanced", "Data is deleted"],
                        correct: 1,
                        explanation: "Hot shards happen when the shard key doesn't distribute load evenly."
                    },
                    {
                        question: "Cross-shard queries are problematic because:",
                        options: ["They're faster", "They require aggregating data from multiple servers, increasing latency", "They don't work", "They're simpler"],
                        correct: 1,
                        explanation: "Cross-shard queries must contact multiple shards and combine results."
                    },
                    {
                        question: "Instagram shards by user_id because:",
                        options: ["Random choice", "Most queries are user-centric, keeping related data together", "It's required", "Cost savings"],
                        correct: 1,
                        explanation: "User-centric queries (posts, followers) stay within a single shard."
                    },
                    {
                        question: "Resharding (adding shards) is complex because:",
                        options: ["It's simple", "Data must be redistributed while maintaining availability", "It's automatic", "No complexity"],
                        correct: 1,
                        explanation: "Resharding requires careful data migration without downtime."
                    },
                    {
                        question: "A shard map or routing table stores:",
                        options: ["User passwords", "The mapping of shard keys to shard locations", "Query results", "Backup data"],
                        correct: 1,
                        explanation: "The routing table tells the system which shard contains which data."
                    }
                ],
                'hard': [
                    {
                        question: "Consistent hashing improves sharding by:",
                        options: ["Making it inconsistent", "Minimizing data movement when adding/removing shards", "Removing all shards", "Slowing queries"],
                        correct: 1,
                        explanation: "Consistent hashing only remaps a fraction of keys when shards change."
                    },
                    {
                        question: "Discord shards messages by channel_id rather than user_id because:",
                        options: ["Random", "Messages are read by channel, keeping channel history together", "Users are unimportant", "Cheaper"],
                        correct: 1,
                        explanation: "Channel-based queries are most common, so keeping channel data together is optimal."
                    },
                    {
                        question: "Vitess (YouTube's sharding layer) provides:",
                        options: ["Video storage", "MySQL sharding with application-transparent query routing", "NoSQL features", "CDN services"],
                        correct: 1,
                        explanation: "Vitess handles sharding complexity, making MySQL scale horizontally."
                    },
                    {
                        question: "When choosing a shard key, you should prioritize:",
                        options: ["Shortest key", "Even distribution and query locality", "Longest key", "Alphabetical order"],
                        correct: 1,
                        explanation: "Good shard keys distribute data evenly and keep related data together."
                    },
                    {
                        question: "Secondary indexes in sharded databases are challenging because:",
                        options: ["They're easy", "The index may span multiple shards, requiring scatter-gather queries", "Indexes don't work", "They're automatic"],
                        correct: 1,
                        explanation: "Non-shard-key indexes must query all shards to find matching records."
                    }
                ],
                'expert': [
                    {
                        question: "Facebook's approach to shard rebalancing involves:",
                        options: ["Downtime", "Live migration with double-writes during transition", "Deleting data", "Manual copying"],
                        correct: 1,
                        explanation: "Double-writes to old and new shards ensure consistency during migration."
                    },
                    {
                        question: "The 'shard exhaustion' problem occurs when:",
                        options: ["Too few shards", "Growth exceeds initial shard count, requiring expensive resharding", "Too many shards", "No sharding"],
                        correct: 1,
                        explanation: "Under-provisioning shards initially leads to painful resharding later."
                    },
                    {
                        question: "For global distribution, sharding by geography helps because:",
                        options: ["Looks nice", "Users are served by nearby shards, reducing latency", "Cheaper", "Required by law always"],
                        correct: 1,
                        explanation: "Geo-sharding reduces latency by keeping user data close to them."
                    },
                    {
                        question: "Sharding affects ACID transactions by:",
                        options: ["Improving them", "Making cross-shard transactions complex, often requiring 2PC or sagas", "No effect", "Removing them"],
                        correct: 1,
                        explanation: "Distributed transactions across shards need coordination protocols."
                    },
                    {
                        question: "The best strategy for a rapidly growing startup's database is:",
                        options: ["Shard immediately", "Start simple, plan for sharding, implement when metrics demand", "Never shard", "Use only NoSQL"],
                        correct: 1,
                        explanation: "Premature sharding adds complexity; plan ahead but implement when needed."
                    }
                ]
            },

            // Day 4: Database Replication
            'hld_11': {
                'easy': [
                    {
                        question: "Database replication creates:",
                        options: ["Backups only", "Copies of data on multiple servers", "Compressed data", "Encrypted data"],
                        correct: 1,
                        explanation: "Replication maintains copies of data across multiple database servers."
                    },
                    {
                        question: "The primary/leader in replication handles:",
                        options: ["Only reads", "All writes and optionally reads", "Backups", "Nothing"],
                        correct: 1,
                        explanation: "The primary accepts writes and propagates changes to replicas."
                    },
                    {
                        question: "Read replicas are used to:",
                        options: ["Handle writes", "Scale read operations by distributing queries", "Store backups", "Compress data"],
                        correct: 1,
                        explanation: "Read replicas handle read queries, reducing load on the primary."
                    },
                    {
                        question: "Synchronous replication means:",
                        options: ["Eventually consistent", "Write confirmed only after replica acknowledges", "No consistency", "Random timing"],
                        correct: 1,
                        explanation: "Synchronous replication waits for replica confirmation before acknowledging writes."
                    },
                    {
                        question: "Asynchronous replication has:",
                        options: ["No lag", "Potential replication lag between primary and replicas", "Instant sync", "No replicas"],
                        correct: 1,
                        explanation: "Async replication can have lag where replicas are behind the primary."
                    }
                ],
                'medium': [
                    {
                        question: "Replication lag can cause:",
                        options: ["Faster reads", "Stale reads where user sees outdated data", "Better consistency", "No issues"],
                        correct: 1,
                        explanation: "Reading from a lagged replica may return outdated information."
                    },
                    {
                        question: "Multi-master replication allows:",
                        options: ["Only one writer", "Multiple nodes to accept writes", "No writes", "Read-only mode"],
                        correct: 1,
                        explanation: "Multi-master allows writes to any node, increasing write availability."
                    },
                    {
                        question: "Conflict resolution in multi-master is needed because:",
                        options: ["No conflicts occur", "Same data might be modified on different masters simultaneously", "Masters don't communicate", "It's optional"],
                        correct: 1,
                        explanation: "Concurrent writes to different masters can conflict and need resolution."
                    },
                    {
                        question: "Netflix uses replication across regions for:",
                        options: ["Cost savings", "Disaster recovery and serving users from nearby regions", "Legal requirements", "Testing only"],
                        correct: 1,
                        explanation: "Cross-region replication provides DR and reduces latency for global users."
                    },
                    {
                        question: "Write-ahead logging (WAL) in replication:",
                        options: ["Slows replication", "Records changes before applying, enabling replay to replicas", "Is optional", "Only for reads"],
                        correct: 1,
                        explanation: "WAL ensures durability and provides a stream of changes for replicas."
                    }
                ],
                'hard': [
                    {
                        question: "Read-your-writes consistency ensures:",
                        options: ["Others see your writes", "You always see your own recent writes", "No consistency", "Writes are delayed"],
                        correct: 1,
                        explanation: "Read-your-writes routes reads to ensure users see their own updates."
                    },
                    {
                        question: "Semi-synchronous replication balances:",
                        options: ["Nothing", "Durability (one replica confirms) with performance (others async)", "Cost", "Security"],
                        correct: 1,
                        explanation: "Semi-sync waits for one replica, balancing durability and latency."
                    },
                    {
                        question: "Chain replication improves:",
                        options: ["Nothing", "Read consistency and write throughput by ordering replicas", "Only latency", "Only durability"],
                        correct: 1,
                        explanation: "Chain replication provides strong consistency with good throughput."
                    },
                    {
                        question: "Promoting a replica to primary during failover requires:",
                        options: ["Nothing special", "Ensuring no data loss and redirecting writes to new primary", "User intervention always", "Deleting data"],
                        correct: 1,
                        explanation: "Failover must handle in-flight transactions and update routing."
                    },
                    {
                        question: "Logical vs physical replication differs in:",
                        options: ["Nothing", "Logical replicates SQL operations, physical replicates disk blocks", "Speed only", "Cost only"],
                        correct: 1,
                        explanation: "Logical is more flexible; physical is faster but requires identical setups."
                    }
                ],
                'expert': [
                    {
                        question: "GitHub's MySQL replication uses Orchestrator because:",
                        options: ["It's free", "Automated failover with topology management for high availability", "Required by MySQL", "Random choice"],
                        correct: 1,
                        explanation: "Orchestrator automates complex MySQL replication management and failover."
                    },
                    {
                        question: "Quorum-based replication (W+R>N) ensures:",
                        options: ["Eventual consistency", "Reads always see latest write when read and write quorums overlap", "No consistency", "Fastest reads"],
                        correct: 1,
                        explanation: "Overlapping quorums guarantee reading the latest written value."
                    },
                    {
                        question: "Streaming replication vs statement-based replication:",
                        options: ["Same thing", "Streaming sends WAL records; statement-based replays SQL commands", "No difference", "Only speed differs"],
                        correct: 1,
                        explanation: "Streaming is more reliable; statement-based can have non-deterministic issues."
                    },
                    {
                        question: "For a banking system requiring zero data loss:",
                        options: ["Async replication", "Synchronous replication with automatic failover", "No replication", "Manual backups"],
                        correct: 1,
                        explanation: "Synchronous replication ensures no committed transaction is lost."
                    },
                    {
                        question: "Global transaction identifiers (GTIDs) solve:",
                        options: ["Nothing", "Tracking replication position across failovers without binary log positions", "Security issues", "Cost problems"],
                        correct: 1,
                        explanation: "GTIDs uniquely identify transactions, simplifying failover and recovery."
                    }
                ]
            },

            // Day 5: Indexing
            'hld_12': {
                'easy': [
                    {
                        question: "A database index is:",
                        options: ["The main data storage", "A data structure that speeds up queries", "A backup system", "A security feature"],
                        correct: 1,
                        explanation: "Indexes are data structures that allow faster data retrieval."
                    },
                    {
                        question: "B-tree indexes are optimized for:",
                        options: ["Full table scans", "Range queries and equality lookups", "Text search only", "Graph traversals"],
                        correct: 1,
                        explanation: "B-trees excel at range queries (BETWEEN, <, >) and exact matches."
                    },
                    {
                        question: "Creating too many indexes can:",
                        options: ["Always improve performance", "Slow down write operations", "Use no storage", "Have no effect"],
                        correct: 1,
                        explanation: "Each index must be updated on writes, adding overhead."
                    },
                    {
                        question: "A primary key index is:",
                        options: ["Optional", "Automatically created on the primary key column", "For secondary columns", "For text only"],
                        correct: 1,
                        explanation: "Primary keys automatically get an index for fast lookups."
                    },
                    {
                        question: "EXPLAIN in SQL shows:",
                        options: ["Data content", "How the database plans to execute a query", "Table structure", "User permissions"],
                        correct: 1,
                        explanation: "EXPLAIN reveals the query execution plan, including index usage."
                    }
                ],
                'medium': [
                    {
                        question: "A composite index on (a, b) can be used for:",
                        options: ["Only queries on b", "Queries on a, queries on a and b, not just b alone", "Any column combination", "Only queries on both"],
                        correct: 1,
                        explanation: "Composite indexes follow leftmost prefix rule: (a), (a,b), not just (b)."
                    },
                    {
                        question: "Covering indexes improve performance by:",
                        options: ["Using more disk", "Including all needed columns, avoiding table lookups", "Slowing reads", "Only for writes"],
                        correct: 1,
                        explanation: "Covering indexes contain all columns needed, eliminating table access."
                    },
                    {
                        question: "Hash indexes are best for:",
                        options: ["Range queries", "Exact equality lookups only", "Sorting", "Text search"],
                        correct: 1,
                        explanation: "Hash indexes excel at exact matches but can't do ranges or sorting."
                    },
                    {
                        question: "Index selectivity refers to:",
                        options: ["Index size", "The ratio of unique values to total rows", "Query speed", "Storage type"],
                        correct: 1,
                        explanation: "High selectivity (many unique values) makes indexes more effective."
                    },
                    {
                        question: "A full-text index is used for:",
                        options: ["Numeric ranges", "Searching text content with relevance ranking", "Primary keys", "Foreign keys"],
                        correct: 1,
                        explanation: "Full-text indexes enable efficient text search with relevance scoring."
                    }
                ],
                'hard': [
                    {
                        question: "Index-only scans avoid:",
                        options: ["Using indexes", "Reading the actual table data (heap)", "Query planning", "Sorting"],
                        correct: 1,
                        explanation: "Index-only scans read all data from the index, skipping table access."
                    },
                    {
                        question: "Partial indexes index only rows that:",
                        options: ["Are primary keys", "Match a specified condition (WHERE clause)", "Are null", "Are numeric"],
                        correct: 1,
                        explanation: "Partial indexes only include rows matching a predicate, saving space."
                    },
                    {
                        question: "The problem with indexing low-cardinality columns is:",
                        options: ["Too unique", "Poor selectivity means index scan isn't much better than table scan", "Too fast", "No storage"],
                        correct: 1,
                        explanation: "Columns like 'gender' have few unique values, making indexes less useful."
                    },
                    {
                        question: "Bitmap indexes are suited for:",
                        options: ["High-cardinality OLTP", "Low-cardinality columns in data warehouses", "Text search", "Primary keys"],
                        correct: 1,
                        explanation: "Bitmap indexes work well for few distinct values in analytical queries."
                    },
                    {
                        question: "Index bloat occurs when:",
                        options: ["Indexes are too small", "Deleted rows leave empty space in indexes", "Too few indexes", "Indexes are unused"],
                        correct: 1,
                        explanation: "Deleted rows leave gaps; REINDEX or maintenance is needed to reclaim space."
                    }
                ],
                'expert': [
                    {
                        question: "Google's search uses inverted indexes because:",
                        options: ["It's simple", "They map words to documents, enabling fast term lookups", "Required by law", "Cheaper"],
                        correct: 1,
                        explanation: "Inverted indexes map terms to document lists, essential for search engines."
                    },
                    {
                        question: "LSM trees (used in Cassandra) vs B-trees:",
                        options: ["Same thing", "LSM optimizes writes (append-only); B-trees optimize reads", "No difference", "LSM is slower"],
                        correct: 1,
                        explanation: "LSM trees batch writes for speed; B-trees are optimized for reads."
                    },
                    {
                        question: "Query hints to force index usage should be:",
                        options: ["Always used", "Used sparingly as optimizer usually knows best", "Required", "Ignored"],
                        correct: 1,
                        explanation: "The optimizer usually chooses correctly; hints can become outdated."
                    },
                    {
                        question: "Clustered vs non-clustered indexes differ in:",
                        options: ["Nothing", "Clustered stores data in index order; non-clustered stores pointers", "Speed only", "Size only"],
                        correct: 1,
                        explanation: "Clustered indexes determine physical data order; non-clustered point to rows."
                    },
                    {
                        question: "Index maintenance strategies for high-write systems include:",
                        options: ["Never maintain", "Online rebuilds, scheduled maintenance windows, monitoring bloat", "Drop all indexes", "Manual updates"],
                        correct: 1,
                        explanation: "High-write systems need regular maintenance to prevent index degradation."
                    }
                ]
            },

            // Day 5: Object Storage
            'hld_13': {
                'easy': [
                    {
                        question: "Object storage stores data as:",
                        options: ["Tables with rows", "Files in folders", "Objects with metadata and unique IDs", "Key-value pairs only"],
                        correct: 2,
                        explanation: "Object storage treats data as objects with metadata and unique identifiers."
                    },
                    {
                        question: "Amazon S3 is an example of:",
                        options: ["Block storage", "File storage", "Object storage", "Database storage"],
                        correct: 2,
                        explanation: "S3 (Simple Storage Service) is Amazon's object storage service."
                    },
                    {
                        question: "Object storage is ideal for:",
                        options: ["Frequently changing data", "Unstructured data like images, videos, backups", "Transactional data", "Small files only"],
                        correct: 1,
                        explanation: "Object storage excels at storing large amounts of unstructured data."
                    },
                    {
                        question: "In object storage, objects are accessed by:",
                        options: ["File path only", "Unique keys/IDs via HTTP APIs", "SQL queries", "Block addresses"],
                        correct: 1,
                        explanation: "Objects are retrieved using unique keys through REST APIs."
                    },
                    {
                        question: "Object storage typically provides:",
                        options: ["ACID transactions", "Eventual consistency for high availability", "Strong consistency always", "No consistency"],
                        correct: 1,
                        explanation: "Most object storage systems prioritize availability with eventual consistency."
                    }
                ],
                'medium': [
                    {
                        question: "CDNs cache objects from origin storage to:",
                        options: ["Save origin costs", "Reduce latency by serving from edge locations", "Increase latency", "Delete objects"],
                        correct: 1,
                        explanation: "CDNs cache content at edge locations near users for faster delivery."
                    },
                    {
                        question: "Object storage lifecycle policies can:",
                        options: ["Only delete objects", "Automatically transition objects between storage tiers", "Only create objects", "Change object content"],
                        correct: 1,
                        explanation: "Lifecycle policies can move objects to cheaper tiers or delete old data."
                    },
                    {
                        question: "Presigned URLs allow:",
                        options: ["Admin access", "Temporary access to private objects without credentials", "Permanent access", "Deleting buckets"],
                        correct: 1,
                        explanation: "Presigned URLs grant time-limited access to private objects."
                    },
                    {
                        question: "Netflix stores video content in object storage because:",
                        options: ["It's the only option", "Cost-effective, durable, and scalable for massive media files", "Fastest option", "Required by law"],
                        correct: 1,
                        explanation: "Object storage's durability and cost make it ideal for media libraries."
                    },
                    {
                        question: "Multipart upload is used for:",
                        options: ["Small files", "Uploading large files in parallel chunks", "Downloading only", "Deleting objects"],
                        correct: 1,
                        explanation: "Multipart upload splits large files into parts for parallel, resumable uploads."
                    }
                ],
                'hard': [
                    {
                        question: "Object storage durability of 99.999999999% (11 9s) means:",
                        options: ["Perfect", "Expected to lose 1 object per 10 billion over a year", "No redundancy", "Backup required"],
                        correct: 1,
                        explanation: "11 9s durability means extremely low probability of data loss."
                    },
                    {
                        question: "S3 versioning enables:",
                        options: ["Faster access", "Keeping multiple versions of objects for recovery", "Compression", "Encryption only"],
                        correct: 1,
                        explanation: "Versioning preserves all versions, protecting against accidental deletes."
                    },
                    {
                        question: "Cross-region replication in object storage provides:",
                        options: ["Cost savings", "Disaster recovery and compliance with data residency", "Faster writes", "Smaller files"],
                        correct: 1,
                        explanation: "CRR copies objects to another region for DR and compliance."
                    },
                    {
                        question: "Object storage vs block storage: applications should use object storage when:",
                        options: ["Need file system semantics", "Storing static content accessed via API, not as mounted volume", "Need database storage", "Running operating systems"],
                        correct: 1,
                        explanation: "Object storage suits static content; block storage for databases and VMs."
                    },
                    {
                        question: "Content-addressable storage uses:",
                        options: ["Random IDs", "Hash of content as the object key for deduplication", "Timestamps", "User-defined names only"],
                        correct: 1,
                        explanation: "CAS derives keys from content hash, automatically deduplicating identical data."
                    }
                ],
                'expert': [
                    {
                        question: "Dropbox's migration to their own object storage (Magic Pocket) saved:",
                        options: ["Nothing", "Hundreds of millions in cloud costs", "Time only", "Bandwidth only"],
                        correct: 1,
                        explanation: "Building custom infrastructure at Dropbox's scale justified the investment."
                    },
                    {
                        question: "Object storage metadata limitations affect design by:",
                        options: ["No effect", "Requiring external databases for complex queries on metadata", "Improving queries", "Reducing storage"],
                        correct: 1,
                        explanation: "Object storage has limited metadata query capabilities; external DBs help."
                    },
                    {
                        question: "Storage tiering (hot/warm/cold/archive) optimizes:",
                        options: ["Only speed", "Cost by matching storage class to access patterns", "Only security", "Only durability"],
                        correct: 1,
                        explanation: "Tiering balances cost and performance based on access frequency."
                    },
                    {
                        question: "S3 Select allows:",
                        options: ["Selecting buckets", "Querying object contents server-side to reduce data transfer", "Moving objects", "Deleting objects"],
                        correct: 1,
                        explanation: "S3 Select runs queries on the server, returning only needed data."
                    },
                    {
                        question: "For globally distributed media, combining CDN with object storage requires:",
                        options: ["Nothing special", "Cache invalidation strategy and origin shield configuration", "Manual distribution", "Separate systems"],
                        correct: 1,
                        explanation: "Effective CDN integration needs proper cache management and origin protection."
                    }
                ]
            },

            // Day 5: Time-Series Databases
            'hld_14': {
                'easy': [
                    {
                        question: "Time-series data is characterized by:",
                        options: ["Random timestamps", "Data points indexed by time, usually append-only", "No timestamps", "Only text data"],
                        correct: 1,
                        explanation: "Time-series data consists of timestamped observations, typically only appended."
                    },
                    {
                        question: "Common time-series use cases include:",
                        options: ["User profiles", "Metrics, IoT sensors, stock prices, logs", "Document storage", "Graph relationships"],
                        correct: 1,
                        explanation: "Time-series DBs excel at metrics, monitoring, IoT, and financial data."
                    },
                    {
                        question: "InfluxDB is a:",
                        options: ["Relational database", "Time-series database", "Graph database", "Document database"],
                        correct: 1,
                        explanation: "InfluxDB is a popular purpose-built time-series database."
                    },
                    {
                        question: "Time-series databases optimize for:",
                        options: ["Complex joins", "High write throughput and time-based queries", "Random updates", "Small datasets"],
                        correct: 1,
                        explanation: "TSDBs are optimized for continuous writes and time-range queries."
                    },
                    {
                        question: "Downsampling in time-series means:",
                        options: ["Increasing resolution", "Reducing data resolution by aggregating over time intervals", "Deleting all data", "Duplicating data"],
                        correct: 1,
                        explanation: "Downsampling aggregates fine-grained data into coarser time buckets."
                    }
                ],
                'medium': [
                    {
                        question: "Retention policies in TSDBs:",
                        options: ["Keep data forever", "Automatically delete or downsample old data", "Only affect new data", "Are optional"],
                        correct: 1,
                        explanation: "Retention policies manage data lifecycle, deleting or aggregating old data."
                    },
                    {
                        question: "Prometheus stores metrics using:",
                        options: ["SQL tables", "A local time-series database optimized for monitoring", "Key-value pairs", "Document format"],
                        correct: 1,
                        explanation: "Prometheus has a built-in TSDB designed for monitoring use cases."
                    },
                    {
                        question: "Tags/labels in time-series databases enable:",
                        options: ["Only naming", "Filtering and grouping metrics by dimensions", "Compression", "Encryption"],
                        correct: 1,
                        explanation: "Tags allow filtering (region=us-east) and grouping in queries."
                    },
                    {
                        question: "Uber uses time-series databases for:",
                        options: ["User profiles", "Real-time monitoring of millions of trips and services", "Payment storage", "Map rendering"],
                        correct: 1,
                        explanation: "TSDBs power Uber's monitoring of rides, services, and infrastructure."
                    },
                    {
                        question: "Compression in TSDBs achieves high ratios because:",
                        options: ["Data is random", "Sequential timestamps and similar values compress well", "No compression used", "Data is already small"],
                        correct: 1,
                        explanation: "Time-series data's sequential nature enables excellent compression."
                    }
                ],
                'hard': [
                    {
                        question: "TimescaleDB extends PostgreSQL by:",
                        options: ["Removing features", "Adding automatic partitioning by time (hypertables)", "Changing SQL syntax", "Removing SQL support"],
                        correct: 1,
                        explanation: "TimescaleDB adds time-partitioned hypertables while keeping full SQL."
                    },
                    {
                        question: "Cardinality explosion in time-series occurs when:",
                        options: ["Few metrics", "Too many unique tag combinations create performance issues", "Data is compressed", "Timestamps are wrong"],
                        correct: 1,
                        explanation: "High cardinality (many unique series) degrades TSDB performance."
                    },
                    {
                        question: "Write-ahead logging vs append-only storage in TSDBs:",
                        options: ["Same thing", "WAL ensures durability; append-only optimizes write speed", "No difference", "Append is slower"],
                        correct: 1,
                        explanation: "TSDBs often use append-only structures with WAL for durability."
                    },
                    {
                        question: "Gorilla compression (Facebook) achieves:",
                        options: ["No compression", "10x+ compression for timestamps and values", "2x compression", "Lossy compression"],
                        correct: 1,
                        explanation: "Gorilla uses XOR-based compression achieving ~10x ratio for metrics."
                    },
                    {
                        question: "Out-of-order writes in TSDBs are problematic because:",
                        options: ["They're easy to handle", "They disrupt time-partitioned storage assumptions", "They're faster", "They compress better"],
                        correct: 1,
                        explanation: "Most TSDBs assume ordered writes; out-of-order requires special handling."
                    }
                ],
                'expert': [
                    {
                        question: "Netflix's Atlas time-series system is designed for:",
                        options: ["Storage only", "Real-time streaming metrics with dimensional data model", "Batch processing", "User data"],
                        correct: 1,
                        explanation: "Atlas handles Netflix's real-time monitoring with millions of metrics."
                    },
                    {
                        question: "Long-term storage for Prometheus typically uses:",
                        options: ["Prometheus itself", "External systems like Thanos or Cortex for scalable retention", "File storage", "No long-term storage"],
                        correct: 1,
                        explanation: "Thanos/Cortex extend Prometheus for long-term, scalable storage."
                    },
                    {
                        question: "The tradeoff in TSDB resolution is:",
                        options: ["None", "Higher resolution = more storage; lower = less detail for debugging", "Always use highest", "Always use lowest"],
                        correct: 1,
                        explanation: "Resolution choice balances storage costs against query granularity needs."
                    },
                    {
                        question: "For IoT with millions of devices, sharding should be by:",
                        options: ["Random", "Device ID to keep each device's data together", "Time only", "No sharding needed"],
                        correct: 1,
                        explanation: "Device-based sharding keeps each device's series on one shard."
                    },
                    {
                        question: "Continuous queries/materialized views in TSDBs:",
                        options: ["Are unnecessary", "Pre-compute aggregates for faster dashboard queries", "Slow down reads", "Use more storage only"],
                        correct: 1,
                        explanation: "Pre-aggregation speeds up common dashboard queries significantly."
                    }
                ]
            },

            // Day 6-15 quizzes continue with similar structure...
            // Adding placeholder structure for remaining days

            // Day 6: Caching Fundamentals
            'hld_15': {
                'easy': [
                    {
                        question: "A cache stores:",
                        options: ["Original data only", "Frequently accessed data for faster retrieval", "Backup data", "Encrypted data"],
                        correct: 1,
                        explanation: "Caches store copies of data that's expensive to compute or fetch."
                    },
                    {
                        question: "Cache hit means:",
                        options: ["Cache is full", "Requested data was found in cache", "Cache failed", "Data was deleted"],
                        correct: 1,
                        explanation: "A cache hit occurs when the requested data is found in the cache."
                    },
                    {
                        question: "Cache miss means:",
                        options: ["Cache is empty", "Requested data was not in cache, must fetch from source", "Cache succeeded", "Data is corrupted"],
                        correct: 1,
                        explanation: "A cache miss requires fetching data from the slower original source."
                    },
                    {
                        question: "TTL (Time To Live) in caching:",
                        options: ["Cache size", "How long data stays in cache before expiring", "Network latency", "Data format"],
                        correct: 1,
                        explanation: "TTL defines when cached data should be considered stale and refreshed."
                    },
                    {
                        question: "The main benefit of caching is:",
                        options: ["More storage", "Reduced latency and load on backend systems", "Better security", "Data encryption"],
                        correct: 1,
                        explanation: "Caching speeds up responses and reduces load on databases/services."
                    }
                ],
                'medium': [
                    {
                        question: "Write-through cache:",
                        options: ["Writes to cache only", "Writes to both cache and database synchronously", "Reads only", "Never writes"],
                        correct: 1,
                        explanation: "Write-through updates both cache and storage together for consistency."
                    },
                    {
                        question: "Write-behind (write-back) cache:",
                        options: ["Never writes", "Writes to cache first, database later asynchronously", "Writes to database only", "Same as write-through"],
                        correct: 1,
                        explanation: "Write-behind batches writes, improving performance but risking data loss."
                    },
                    {
                        question: "Cache invalidation is hard because:",
                        options: ["It's simple", "Knowing when data changes across distributed systems is complex", "Caches don't support it", "It's automatic"],
                        correct: 1,
                        explanation: "Coordinating cache updates when source data changes is a classic hard problem."
                    },
                    {
                        question: "A cache-aside pattern means:",
                        options: ["Cache manages everything", "Application manages cache: check cache, if miss fetch and populate", "No application involvement", "Automatic caching"],
                        correct: 1,
                        explanation: "In cache-aside, the application explicitly manages cache reads and writes."
                    },
                    {
                        question: "Cache stampede occurs when:",
                        options: ["Cache is empty", "Many requests hit expired cache simultaneously, overwhelming backend", "Cache is too fast", "Data is corrupted"],
                        correct: 1,
                        explanation: "Cache stampede happens when mass expiration causes sudden backend load."
                    }
                ],
                'hard': [
                    {
                        question: "Facebook's Memcache uses lease tokens to prevent:",
                        options: ["Reading", "Thundering herd and stale data from racing updates", "Writing", "Cache hits"],
                        correct: 1,
                        explanation: "Lease tokens prevent race conditions when multiple clients update cache."
                    },
                    {
                        question: "Cache warming means:",
                        options: ["Heating servers", "Pre-populating cache before traffic hits", "Clearing cache", "Slowing cache"],
                        correct: 1,
                        explanation: "Cache warming fills the cache proactively to avoid cold-start misses."
                    },
                    {
                        question: "Multi-tier caching (L1/L2) helps by:",
                        options: ["Using more memory", "Local cache (L1) for fastest hits, shared cache (L2) for distributed", "Slowing down", "Reducing capacity"],
                        correct: 1,
                        explanation: "Multiple tiers balance speed (local) with efficiency (shared)."
                    },
                    {
                        question: "Cache coherence in distributed systems ensures:",
                        options: ["Caches are slow", "All caches have consistent view of data", "Caches disagree", "Data is deleted"],
                        correct: 1,
                        explanation: "Coherence means updates propagate to all cache copies correctly."
                    },
                    {
                        question: "Negative caching stores:",
                        options: ["Only positive results", "Absence of data (null results) to prevent repeated misses", "Corrupted data", "Old data"],
                        correct: 1,
                        explanation: "Caching 'not found' prevents repeated expensive lookups for missing data."
                    }
                ],
                'expert': [
                    {
                        question: "Twitter's cache architecture handles:",
                        options: ["Small scale", "Billions of timeline requests with multi-layer caching", "No caching", "Manual caching"],
                        correct: 1,
                        explanation: "Twitter's timeline cache is a complex multi-tier system for massive scale."
                    },
                    {
                        question: "Cache-aside with eventual consistency can cause:",
                        options: ["No issues", "Stale reads when database updates before cache invalidation", "Faster writes", "Better consistency"],
                        correct: 1,
                        explanation: "Race conditions between DB writes and cache invalidation can serve stale data."
                    },
                    {
                        question: "Read-through vs cache-aside differs in:",
                        options: ["Nothing", "Read-through has cache fetch on miss; cache-aside requires app to fetch", "Speed only", "Cost only"],
                        correct: 1,
                        explanation: "Read-through abstracts fetching; cache-aside gives app full control."
                    },
                    {
                        question: "For personalized content, caching is challenging because:",
                        options: ["It's easy", "Each user has different data, reducing cache hit rates", "Caches don't work", "It's automatic"],
                        correct: 1,
                        explanation: "Personalization creates many unique variants, fragmenting the cache."
                    },
                    {
                        question: "Cache versioning helps with:",
                        options: ["Nothing", "Rolling out schema changes without invalidating all cached data", "Faster reads", "Smaller caches"],
                        correct: 1,
                        explanation: "Versioned cache keys allow graceful migration during data model changes."
                    }
                ]
            },

            // Day 6: Redis/Memcached
            'hld_16': {
                'easy': [
                    {
                        question: "Redis is:",
                        options: ["A relational database", "An in-memory data structure store", "A file system", "An operating system"],
                        correct: 1,
                        explanation: "Redis is an in-memory store supporting various data structures."
                    },
                    {
                        question: "Memcached is optimized for:",
                        options: ["Complex data", "Simple key-value caching with high performance", "Disk storage", "Graph data"],
                        correct: 1,
                        explanation: "Memcached is a simple, fast distributed memory caching system."
                    },
                    {
                        question: "Redis supports data structures like:",
                        options: ["Only strings", "Strings, lists, sets, hashes, sorted sets", "Only numbers", "Only JSON"],
                        correct: 1,
                        explanation: "Redis offers rich data structures beyond simple key-value."
                    },
                    {
                        question: "Redis persistence options include:",
                        options: ["No persistence", "RDB snapshots and AOF append-only file", "Only disk", "Only memory"],
                        correct: 1,
                        explanation: "Redis can persist data using snapshots (RDB) or logging (AOF)."
                    },
                    {
                        question: "Redis pub/sub allows:",
                        options: ["File storage", "Publishing messages to channels for real-time messaging", "Database queries", "Compression"],
                        correct: 1,
                        explanation: "Redis pub/sub enables real-time messaging between publishers and subscribers."
                    }
                ],
                'medium': [
                    {
                        question: "Redis vs Memcached: Redis adds:",
                        options: ["Nothing", "Data structures, persistence, pub/sub, clustering", "Only speed", "Only security"],
                        correct: 1,
                        explanation: "Redis offers more features while Memcached focuses on simplicity."
                    },
                    {
                        question: "Redis sorted sets are useful for:",
                        options: ["Random data", "Leaderboards, ranking, time-based data", "Encryption", "Compression"],
                        correct: 1,
                        explanation: "Sorted sets maintain order by score, perfect for rankings."
                    },
                    {
                        question: "Redis cluster provides:",
                        options: ["Single node", "Automatic sharding across multiple nodes", "No scaling", "Only replication"],
                        correct: 1,
                        explanation: "Redis Cluster automatically partitions data across nodes."
                    },
                    {
                        question: "Instagram uses Redis for:",
                        options: ["Image storage", "Storing followers, following lists, and activity feeds", "Video encoding", "Payment processing"],
                        correct: 1,
                        explanation: "Redis's data structures are ideal for social graph and feed data."
                    },
                    {
                        question: "Redis transactions (MULTI/EXEC) provide:",
                        options: ["Rollback like SQL", "Atomic execution of multiple commands", "No atomicity", "Distributed transactions"],
                        correct: 1,
                        explanation: "MULTI/EXEC ensures commands execute atomically, though without rollback."
                    }
                ],
                'hard': [
                    {
                        question: "Redis Lua scripting enables:",
                        options: ["Only logging", "Atomic operations on multiple keys with server-side logic", "Client-side processing", "Network optimization"],
                        correct: 1,
                        explanation: "Lua scripts run atomically on the server for complex operations."
                    },
                    {
                        question: "Redis memory eviction policies like LRU:",
                        options: ["Increase memory", "Remove least recently used keys when memory is full", "Add more data", "Compress data"],
                        correct: 1,
                        explanation: "Eviction policies determine which keys to remove under memory pressure."
                    },
                    {
                        question: "Redis Sentinel provides:",
                        options: ["Data storage", "High availability through automatic failover", "Compression", "Encryption"],
                        correct: 1,
                        explanation: "Sentinel monitors Redis and handles automatic master failover."
                    },
                    {
                        question: "Memcached's consistent hashing helps with:",
                        options: ["Security", "Minimizing key redistribution when nodes are added/removed", "Compression", "Encryption"],
                        correct: 1,
                        explanation: "Consistent hashing reduces cache invalidation during cluster changes."
                    },
                    {
                        question: "Redis streams (added in 5.0) are for:",
                        options: ["File streaming", "Log-like data structures for event streaming", "Video streaming", "Network streaming"],
                        correct: 1,
                        explanation: "Redis streams provide Kafka-like log data structure capabilities."
                    }
                ],
                'expert': [
                    {
                        question: "Twitter's Redis usage includes:",
                        options: ["All data", "Timeline caching, rate limiting, session storage at massive scale", "No Redis", "Only testing"],
                        correct: 1,
                        explanation: "Twitter uses Redis extensively for real-time features and caching."
                    },
                    {
                        question: "Redis pipeline improves performance by:",
                        options: ["Slowing down", "Batching multiple commands to reduce network round trips", "Using more memory", "Compressing data"],
                        correct: 1,
                        explanation: "Pipelining sends multiple commands at once, reducing latency."
                    },
                    {
                        question: "For distributed locking, Redis Redlock algorithm:",
                        options: ["Uses single node", "Acquires locks across multiple independent Redis instances", "Doesn't work", "Is deprecated"],
                        correct: 1,
                        explanation: "Redlock uses majority of nodes for reliable distributed locking."
                    },
                    {
                        question: "Redis memory optimization techniques include:",
                        options: ["Using more RAM", "Smaller keys, compression, appropriate data structures", "Larger values", "More nodes only"],
                        correct: 1,
                        explanation: "Memory efficiency requires careful key design and data structure choice."
                    },
                    {
                        question: "When Redis cluster loses master without replica:",
                        options: ["Nothing happens", "That hash slot becomes unavailable until recovery", "Data is safe", "Other nodes take over"],
                        correct: 1,
                        explanation: "Lost masters without replicas leave slots unavailable, causing partial outage."
                    }
                ]
            },

            // Day 6: Cache Eviction
            'hld_17': {
                'easy': [
                    {
                        question: "Cache eviction is needed when:",
                        options: ["Cache is empty", "Cache is full and new data needs space", "Cache is fast", "Data is old"],
                        correct: 1,
                        explanation: "Eviction removes entries when cache capacity is reached."
                    },
                    {
                        question: "LRU stands for:",
                        options: ["Last Recently Updated", "Least Recently Used", "Least Required Update", "Last Required Use"],
                        correct: 1,
                        explanation: "LRU evicts the entry that hasn't been accessed for the longest time."
                    },
                    {
                        question: "FIFO eviction removes:",
                        options: ["Most recent", "The oldest entry regardless of access pattern", "Random entries", "Smallest entries"],
                        correct: 1,
                        explanation: "FIFO (First In, First Out) removes entries in insertion order."
                    },
                    {
                        question: "LFU stands for:",
                        options: ["Last Frequently Updated", "Least Frequently Used", "Least First Update", "Last First Use"],
                        correct: 1,
                        explanation: "LFU evicts entries that are accessed least often."
                    },
                    {
                        question: "TTL-based eviction removes entries:",
                        options: ["Randomly", "After a specified time period expires", "By size", "By frequency"],
                        correct: 1,
                        explanation: "TTL eviction removes entries when their time-to-live expires."
                    }
                ],
                'medium': [
                    {
                        question: "LRU is preferred over FIFO because:",
                        options: ["It's simpler", "It considers access patterns, not just insertion order", "It's faster", "It uses less memory"],
                        correct: 1,
                        explanation: "LRU keeps frequently accessed items, improving hit rates."
                    },
                    {
                        question: "LFU can suffer from:",
                        options: ["Nothing", "Keeping old popular items even when access patterns change", "Too few evictions", "Too many hits"],
                        correct: 1,
                        explanation: "LFU may keep historically popular items that are no longer relevant."
                    },
                    {
                        question: "Redis's LRU implementation is:",
                        options: ["Exact LRU", "Approximate LRU sampling for efficiency", "Random", "FIFO"],
                        correct: 1,
                        explanation: "Redis samples keys rather than tracking all accesses for efficiency."
                    },
                    {
                        question: "Combining LRU with TTL provides:",
                        options: ["Nothing extra", "Both access-based and time-based eviction", "Slower performance", "Less accuracy"],
                        correct: 1,
                        explanation: "Combined policies handle both staleness and capacity management."
                    },
                    {
                        question: "Random eviction is sometimes used because:",
                        options: ["It's best", "It's simple and performs surprisingly well in some cases", "It's most accurate", "It's required"],
                        correct: 1,
                        explanation: "Random eviction has low overhead and can work well for uniform access."
                    }
                ],
                'hard': [
                    {
                        question: "ARC (Adaptive Replacement Cache) improves on LRU by:",
                        options: ["Being simpler", "Adapting between recency and frequency based on workload", "Using more memory", "Being slower"],
                        correct: 1,
                        explanation: "ARC balances LRU and LFU behaviors dynamically."
                    },
                    {
                        question: "2Q algorithm uses two queues to:",
                        options: ["Double capacity", "Prevent scan pollution from one-time accesses", "Slow eviction", "Simplify LRU"],
                        correct: 1,
                        explanation: "2Q filters out items accessed only once before promoting to main cache."
                    },
                    {
                        question: "Cache eviction in distributed systems must consider:",
                        options: ["Only local state", "Consistency across nodes when evicting shared data", "Nothing extra", "Only performance"],
                        correct: 1,
                        explanation: "Distributed eviction needs coordination to maintain consistency."
                    },
                    {
                        question: "Memory-efficient LRU implementations use:",
                        options: ["More memory", "Hash map + doubly linked list for O(1) operations", "Arrays only", "Trees only"],
                        correct: 1,
                        explanation: "HashMap provides O(1) lookup; linked list provides O(1) reordering."
                    },
                    {
                        question: "W-TinyLFU (used in Caffeine) combines:",
                        options: ["Random + FIFO", "Window LRU for recency + TinyLFU for frequency", "Only LRU", "Only LFU"],
                        correct: 1,
                        explanation: "W-TinyLFU balances recent and frequent access patterns efficiently."
                    }
                ],
                'expert': [
                    {
                        question: "Frequency-based eviction's count-min sketch provides:",
                        options: ["Exact counts", "Space-efficient approximate frequency counts", "No benefit", "Slower counting"],
                        correct: 1,
                        explanation: "Count-min sketch estimates frequency with bounded error using little memory."
                    },
                    {
                        question: "For CDN edge caches, eviction should prioritize:",
                        options: ["Random content", "Content popularity at that edge location", "Oldest content", "Smallest content"],
                        correct: 1,
                        explanation: "Edge popularity varies by location; eviction should be location-aware."
                    },
                    {
                        question: "Ghost entries in ARC and 2Q track:",
                        options: ["Deleted data", "Recently evicted keys to inform future admission decisions", "Corrupted data", "Backup data"],
                        correct: 1,
                        explanation: "Ghost lists remember evicted keys to improve future decisions."
                    },
                    {
                        question: "Size-aware eviction considers:",
                        options: ["Only count", "Entry sizes to optimize total value retained in cache", "Only time", "Only frequency"],
                        correct: 1,
                        explanation: "Large entries cost more capacity; size-aware eviction balances this."
                    },
                    {
                        question: "Admission policies in caching determine:",
                        options: ["Eviction order", "Whether new items should enter cache at all", "Cache size", "TTL values"],
                        correct: 1,
                        explanation: "Admission filters prevent low-value items from displacing better ones."
                    }
                ]
            },

            // Day 7: Consistent Hashing
            'hld_18': {
                'easy': [
                    {
                        question: "Consistent hashing is used to:",
                        options: ["Encrypt data", "Distribute data across nodes with minimal redistribution on changes", "Compress data", "Sort data"],
                        correct: 1,
                        explanation: "Consistent hashing minimizes data movement when nodes are added or removed."
                    },
                    {
                        question: "In consistent hashing, nodes are placed on:",
                        options: ["A linked list", "A hash ring", "A binary tree", "A stack"],
                        correct: 1,
                        explanation: "Nodes and keys are mapped to positions on a circular hash ring."
                    },
                    {
                        question: "When a node fails in consistent hashing:",
                        options: ["All data redistributes", "Only that node's data moves to the next node", "System stops", "Data is lost"],
                        correct: 1,
                        explanation: "Only keys from the failed node move to its successor on the ring."
                    },
                    {
                        question: "Virtual nodes in consistent hashing:",
                        options: ["Are physical servers", "Multiple ring positions per physical node for better distribution", "Store backups", "Handle encryption"],
                        correct: 1,
                        explanation: "Virtual nodes give each physical node multiple positions for even load."
                    },
                    {
                        question: "Consistent hashing is used by:",
                        options: ["Only databases", "Distributed caches, databases, and load balancers", "Only web servers", "Only storage"],
                        correct: 1,
                        explanation: "Consistent hashing is fundamental to many distributed systems."
                    }
                ],
                'medium': [
                    {
                        question: "Without consistent hashing, adding a node requires:",
                        options: ["No changes", "Redistributing most/all keys (modulo N changes)", "Only local changes", "Deleting data"],
                        correct: 1,
                        explanation: "Traditional hash(key) % N redistributes almost everything when N changes."
                    },
                    {
                        question: "DynamoDB uses consistent hashing for:",
                        options: ["Encryption", "Partitioning data across storage nodes", "Compression", "Caching only"],
                        correct: 1,
                        explanation: "DynamoDB partitions data using consistent hashing for scalability."
                    },
                    {
                        question: "The 'hot spot' problem in consistent hashing occurs when:",
                        options: ["Servers overheat", "Some nodes receive disproportionate traffic", "Ring is too small", "Too many nodes"],
                        correct: 1,
                        explanation: "Uneven key distribution can overload some nodes."
                    },
                    {
                        question: "Virtual nodes solve hot spots by:",
                        options: ["Adding servers", "Spreading each node's responsibility across the ring", "Removing nodes", "Caching more"],
                        correct: 1,
                        explanation: "Multiple positions per node averages out load distribution."
                    },
                    {
                        question: "Cassandra uses consistent hashing to:",
                        options: ["Encrypt data", "Determine which nodes store each partition", "Compress data", "Sort queries"],
                        correct: 1,
                        explanation: "Cassandra maps partition keys to nodes using consistent hashing."
                    }
                ],
                'hard': [
                    {
                        question: "Jump consistent hash improves on ring-based by:",
                        options: ["Using more memory", "O(1) memory and faster computation", "Being slower", "Requiring more nodes"],
                        correct: 1,
                        explanation: "Jump hash uses a formula instead of storing ring positions."
                    },
                    {
                        question: "Replication in consistent hashing typically copies data to:",
                        options: ["Random nodes", "The next N nodes clockwise on the ring", "All nodes", "Only the primary"],
                        correct: 1,
                        explanation: "Data replicates to successive nodes on the ring for fault tolerance."
                    },
                    {
                        question: "Weighted consistent hashing assigns:",
                        options: ["Equal positions", "More virtual nodes to higher-capacity servers", "Fewer nodes overall", "Random positions"],
                        correct: 1,
                        explanation: "Weights let powerful nodes handle proportionally more data."
                    },
                    {
                        question: "When using consistent hashing for caching, cache misses after node addition:",
                        options: ["Affect all keys", "Only affect keys that moved to the new node", "Don't happen", "Crash the system"],
                        correct: 1,
                        explanation: "Only ~1/N keys move, minimizing cache miss impact."
                    },
                    {
                        question: "Rendezvous hashing (HRW) differs from consistent hashing by:",
                        options: ["Being identical", "Computing scores for all nodes per key instead of ring position", "Using no hash", "Being slower always"],
                        correct: 1,
                        explanation: "HRW picks the node with highest hash(key, node) score."
                    }
                ],
                'expert': [
                    {
                        question: "Discord uses consistent hashing for:",
                        options: ["User authentication", "Routing messages to correct guild servers", "Payment processing", "Email delivery"],
                        correct: 1,
                        explanation: "Discord routes guild traffic using consistent hashing for scalability."
                    },
                    {
                        question: "Maglev hashing (Google) provides:",
                        options: ["Slower lookups", "O(1) lookup with minimal disruption and good distribution", "More memory usage", "Simpler implementation"],
                        correct: 1,
                        explanation: "Maglev offers fast lookups with excellent load balancing properties."
                    },
                    {
                        question: "Bounded-load consistent hashing ensures:",
                        options: ["Unlimited load", "No node exceeds a capacity threshold", "Equal load always", "Random distribution"],
                        correct: 1,
                        explanation: "Bounded-load variants cap individual node load for fairness."
                    },
                    {
                        question: "For stateful services, consistent hashing enables:",
                        options: ["Stateless operation", "Session affinity - same user routes to same server", "Random routing", "No benefits"],
                        correct: 1,
                        explanation: "Consistent hashing provides sticky sessions for stateful workloads."
                    },
                    {
                        question: "Multi-probe consistent hashing improves distribution by:",
                        options: ["Single lookup", "Checking multiple positions and picking least loaded", "Random selection", "Using more rings"],
                        correct: 1,
                        explanation: "Checking multiple candidates helps avoid overloaded nodes."
                    }
                ]
            },

            // Day 7: Service Discovery
            'hld_19': {
                'easy': [
                    {
                        question: "Service discovery helps services:",
                        options: ["Find configuration", "Find and communicate with other services dynamically", "Store data", "Handle authentication"],
                        correct: 1,
                        explanation: "Service discovery enables services to locate each other without hardcoded addresses."
                    },
                    {
                        question: "A service registry stores:",
                        options: ["User data", "Locations and health status of service instances", "Application code", "Database schemas"],
                        correct: 1,
                        explanation: "The registry maintains a directory of available service instances."
                    },
                    {
                        question: "Client-side discovery means:",
                        options: ["Server finds clients", "Client queries registry and chooses an instance", "No discovery needed", "Automatic routing"],
                        correct: 1,
                        explanation: "In client-side discovery, clients are responsible for finding services."
                    },
                    {
                        question: "Server-side discovery uses:",
                        options: ["No intermediary", "A load balancer that queries the registry", "Direct connections", "Manual configuration"],
                        correct: 1,
                        explanation: "A load balancer handles discovery, simplifying clients."
                    },
                    {
                        question: "Consul and etcd are examples of:",
                        options: ["Databases", "Service discovery tools", "Web servers", "Caching systems"],
                        correct: 1,
                        explanation: "Consul and etcd are popular service discovery and configuration tools."
                    }
                ],
                'medium': [
                    {
                        question: "Health checks in service discovery:",
                        options: ["Check user health", "Verify service instances are running and healthy", "Monitor CPU only", "Are optional"],
                        correct: 1,
                        explanation: "Health checks ensure only healthy instances receive traffic."
                    },
                    {
                        question: "DNS-based service discovery limitations include:",
                        options: ["No limitations", "TTL caching can serve stale records", "Too fast updates", "Perfect consistency"],
                        correct: 1,
                        explanation: "DNS caching means changes don't propagate instantly."
                    },
                    {
                        question: "Kubernetes service discovery uses:",
                        options: ["External tools only", "Built-in DNS and environment variables", "Manual configuration", "No discovery"],
                        correct: 1,
                        explanation: "Kubernetes provides automatic DNS-based service discovery."
                    },
                    {
                        question: "Netflix Eureka is designed for:",
                        options: ["Database queries", "Service registration and discovery in AWS", "File storage", "Message queuing"],
                        correct: 1,
                        explanation: "Eureka is Netflix's service discovery server for microservices."
                    },
                    {
                        question: "Service mesh (like Istio) handles discovery via:",
                        options: ["Manual config", "Sidecar proxies that intercept traffic", "Client libraries", "No discovery"],
                        correct: 1,
                        explanation: "Service meshes use sidecars to handle discovery transparently."
                    }
                ],
                'hard': [
                    {
                        question: "Self-registration pattern means:",
                        options: ["Admin registers services", "Services register themselves with the registry", "Automatic detection", "No registration"],
                        correct: 1,
                        explanation: "Services are responsible for registering and deregistering themselves."
                    },
                    {
                        question: "Heartbeat-based health checking:",
                        options: ["Checks once", "Services periodically signal they're alive", "Is passive", "Uses no network"],
                        correct: 1,
                        explanation: "Heartbeats are periodic signals indicating a service is healthy."
                    },
                    {
                        question: "Stale registry entries can cause:",
                        options: ["Faster routing", "Requests to dead instances, causing errors", "Better performance", "No issues"],
                        correct: 1,
                        explanation: "Stale entries lead to failed connections to unavailable services."
                    },
                    {
                        question: "Consul's gossip protocol provides:",
                        options: ["Slow updates", "Eventually consistent membership across nodes", "Strong consistency", "No communication"],
                        correct: 1,
                        explanation: "Gossip spreads membership information efficiently across the cluster."
                    },
                    {
                        question: "Service discovery in multi-datacenter setups requires:",
                        options: ["Single registry", "Federation or replication across datacenters", "No special handling", "Manual sync"],
                        correct: 1,
                        explanation: "Multi-DC discovery needs registry coordination across regions."
                    }
                ],
                'expert': [
                    {
                        question: "Airbnb's SmartStack uses:",
                        options: ["Centralized discovery", "Local HAProxy synced by Synapse from Zookeeper", "No discovery", "Manual configuration"],
                        correct: 1,
                        explanation: "SmartStack combines Nerve (registration) and Synapse (discovery) with HAProxy."
                    },
                    {
                        question: "Zero-downtime deployments require discovery that:",
                        options: ["Ignores health", "Drains connections before deregistration", "Immediately removes instances", "Has no requirements"],
                        correct: 1,
                        explanation: "Graceful deregistration lets in-flight requests complete."
                    },
                    {
                        question: "Service discovery anti-patterns include:",
                        options: ["Using health checks", "Hardcoding IPs, not handling failures, no health checks", "Using DNS", "Using registries"],
                        correct: 1,
                        explanation: "Common mistakes defeat the purpose of dynamic discovery."
                    },
                    {
                        question: "For serverless functions, discovery often uses:",
                        options: ["Traditional registries", "API Gateway routing based on function name", "IP addresses", "Manual configuration"],
                        correct: 1,
                        explanation: "API Gateways route to functions without traditional service registries."
                    },
                    {
                        question: "Locality-aware discovery prioritizes:",
                        options: ["Random instances", "Instances in the same zone/region for lower latency", "Farthest instances", "Oldest instances"],
                        correct: 1,
                        explanation: "Locality awareness reduces network latency and costs."
                    }
                ]
            },

            // Day 7: API Gateway
            'hld_20': {
                'easy': [
                    {
                        question: "An API Gateway is:",
                        options: ["A database", "A single entry point that routes requests to backend services", "A cache", "A message queue"],
                        correct: 1,
                        explanation: "API Gateways provide a unified entry point for all client requests."
                    },
                    {
                        question: "API Gateways typically handle:",
                        options: ["Only routing", "Authentication, rate limiting, routing, and more", "Only security", "Only logging"],
                        correct: 1,
                        explanation: "Gateways centralize cross-cutting concerns for all APIs."
                    },
                    {
                        question: "Kong and AWS API Gateway are examples of:",
                        options: ["Databases", "API Gateway products", "Cache systems", "Message brokers"],
                        correct: 1,
                        explanation: "Kong and AWS API Gateway are popular gateway solutions."
                    },
                    {
                        question: "Without an API Gateway, clients must:",
                        options: ["Use one endpoint", "Know and call each microservice directly", "Use no authentication", "Cache everything"],
                        correct: 1,
                        explanation: "Without a gateway, clients need to manage multiple service endpoints."
                    },
                    {
                        question: "API Gateway rate limiting protects:",
                        options: ["Nothing", "Backend services from being overwhelmed", "Only the gateway", "Only clients"],
                        correct: 1,
                        explanation: "Rate limiting prevents abuse and protects backend resources."
                    }
                ],
                'medium': [
                    {
                        question: "Request aggregation in API Gateway:",
                        options: ["Slows requests", "Combines multiple backend calls into one client response", "Splits requests", "Caches nothing"],
                        correct: 1,
                        explanation: "Aggregation reduces client round trips by combining responses."
                    },
                    {
                        question: "Protocol translation at the gateway:",
                        options: ["Is impossible", "Converts between protocols (REST to gRPC, etc.)", "Only works one way", "Requires client changes"],
                        correct: 1,
                        explanation: "Gateways can translate between different API protocols."
                    },
                    {
                        question: "API versioning through gateway uses:",
                        options: ["No versioning", "URL paths, headers, or query params to route to versions", "Random selection", "Only deprecation"],
                        correct: 1,
                        explanation: "Gateways can route to different service versions based on request info."
                    },
                    {
                        question: "Netflix Zuul gateway provides:",
                        options: ["Database access", "Dynamic routing, monitoring, and security", "File storage", "Message queuing"],
                        correct: 1,
                        explanation: "Zuul is Netflix's edge service for routing and filtering."
                    },
                    {
                        question: "Circuit breaker in API Gateway:",
                        options: ["Increases load", "Prevents cascading failures by failing fast", "Slows responses", "Has no effect"],
                        correct: 1,
                        explanation: "Circuit breakers stop sending requests to failing services."
                    }
                ],
                'hard': [
                    {
                        question: "BFF (Backend for Frontend) pattern uses gateways to:",
                        options: ["One gateway for all", "Provide specialized APIs for different client types", "Remove all gateways", "Share everything"],
                        correct: 1,
                        explanation: "BFF creates tailored APIs for mobile, web, etc."
                    },
                    {
                        question: "API Gateway as single point of failure is mitigated by:",
                        options: ["Accepting failure", "Multiple gateway instances with load balancing", "Removing the gateway", "Manual failover"],
                        correct: 1,
                        explanation: "Running multiple gateway instances provides high availability."
                    },
                    {
                        question: "Request/response transformation at gateway:",
                        options: ["Is not possible", "Modifies payloads without changing backend services", "Requires backend changes", "Only works for JSON"],
                        correct: 1,
                        explanation: "Gateways can transform requests/responses on the fly."
                    },
                    {
                        question: "API Gateway caching improves performance by:",
                        options: ["Slowing responses", "Serving cached responses without hitting backends", "Increasing backend load", "Removing data"],
                        correct: 1,
                        explanation: "Caching at the edge reduces backend calls for repeated requests."
                    },
                    {
                        question: "GraphQL gateway can:",
                        options: ["Only pass through", "Federate multiple GraphQL schemas into one", "Only use REST", "Not aggregate"],
                        correct: 1,
                        explanation: "GraphQL federation combines multiple schemas at the gateway."
                    }
                ],
                'expert': [
                    {
                        question: "Uber's API Gateway handles:",
                        options: ["Simple routing", "Millions of requests/sec with complex routing rules", "No traffic", "Only internal APIs"],
                        correct: 1,
                        explanation: "Uber's gateway manages massive scale with sophisticated routing."
                    },
                    {
                        question: "Gateway latency concerns are addressed by:",
                        options: ["Adding more features", "Keeping processing minimal, using efficient proxies", "Removing the gateway", "Ignoring latency"],
                        correct: 1,
                        explanation: "Gateway overhead must be minimized to avoid adding latency."
                    },
                    {
                        question: "Service mesh vs API Gateway:",
                        options: ["Same thing", "Mesh handles service-to-service; gateway handles external traffic", "Gateway replaces mesh", "Mesh replaces gateway"],
                        correct: 1,
                        explanation: "They serve different purposes and often coexist."
                    },
                    {
                        question: "API Gateway observability includes:",
                        options: ["No monitoring", "Logging, metrics, and distributed tracing for all requests", "Only errors", "Only successes"],
                        correct: 1,
                        explanation: "Comprehensive observability at the gateway is crucial for debugging."
                    },
                    {
                        question: "Canary deployments via gateway route:",
                        options: ["All traffic to new version", "Small percentage of traffic to new version for testing", "No traffic changes", "Random routing"],
                        correct: 1,
                        explanation: "Canary releases gradually shift traffic to test new versions safely."
                    }
                ]
            },

            // Day 8: CAP Theorem
            'hld_21': {
                'easy': [
                    {
                        question: "CAP theorem states distributed systems can guarantee:",
                        options: ["All three always", "At most two of: Consistency, Availability, Partition tolerance", "Only one", "None"],
                        correct: 1,
                        explanation: "CAP theorem: during a network partition, choose consistency or availability."
                    },
                    {
                        question: "Consistency in CAP means:",
                        options: ["Data is encrypted", "All nodes see the same data at the same time", "Data is compressed", "Data is replicated"],
                        correct: 1,
                        explanation: "Consistency ensures all reads return the most recent write."
                    },
                    {
                        question: "Availability in CAP means:",
                        options: ["Sometimes responds", "Every request receives a response (success or failure)", "Only reads work", "Only writes work"],
                        correct: 1,
                        explanation: "Availability guarantees the system responds to every request."
                    },
                    {
                        question: "Partition tolerance means:",
                        options: ["No network issues", "System works despite network partitions between nodes", "Partitioning data", "Removing nodes"],
                        correct: 1,
                        explanation: "Partition tolerance handles network failures between nodes."
                    },
                    {
                        question: "In practice, partition tolerance is:",
                        options: ["Optional", "Required because network failures happen", "Never needed", "Only for large systems"],
                        correct: 1,
                        explanation: "Networks fail; systems must handle partitions, choosing C or A."
                    }
                ],
                'medium': [
                    {
                        question: "A CP system during partition:",
                        options: ["Returns stale data", "Refuses requests to maintain consistency", "Always available", "Ignores partitions"],
                        correct: 1,
                        explanation: "CP systems sacrifice availability to ensure consistent data."
                    },
                    {
                        question: "An AP system during partition:",
                        options: ["Stops working", "Continues serving possibly stale data", "Guarantees consistency", "Loses all data"],
                        correct: 1,
                        explanation: "AP systems stay available but may return inconsistent data."
                    },
                    {
                        question: "MongoDB is typically classified as:",
                        options: ["AP system", "CP system (with replica set)", "CA system", "None of these"],
                        correct: 1,
                        explanation: "MongoDB prioritizes consistency over availability during partitions."
                    },
                    {
                        question: "Cassandra is typically classified as:",
                        options: ["CP system", "AP system (tunable consistency)", "CA system", "None of these"],
                        correct: 1,
                        explanation: "Cassandra defaults to availability but offers tunable consistency."
                    },
                    {
                        question: "Why CA systems don't exist in distributed systems:",
                        options: ["They're too expensive", "Network partitions are inevitable", "They're too slow", "They're not useful"],
                        correct: 1,
                        explanation: "Real networks partition; you must choose C or A during partitions."
                    }
                ],
                'hard': [
                    {
                        question: "PACELC extends CAP by considering:",
                        options: ["Nothing new", "Latency vs consistency tradeoffs when no partition", "Only partitions", "Only availability"],
                        correct: 1,
                        explanation: "PACELC: during Partition choose A or C; Else choose Latency or Consistency."
                    },
                    {
                        question: "Tunable consistency in databases allows:",
                        options: ["No tuning", "Choosing consistency level per operation", "Only global settings", "Automatic optimization"],
                        correct: 1,
                        explanation: "Tunable consistency lets you choose CP or AP behavior per query."
                    },
                    {
                        question: "DynamoDB's consistency options include:",
                        options: ["Only strong", "Eventually consistent (default) and strongly consistent reads", "Only eventual", "No options"],
                        correct: 1,
                        explanation: "DynamoDB offers both consistency levels with different costs."
                    },
                    {
                        question: "Linearizability is stronger than CAP consistency because:",
                        options: ["It's weaker", "It requires real-time ordering of operations", "They're the same", "It's unrelated"],
                        correct: 1,
                        explanation: "Linearizability adds real-time constraints to consistency."
                    },
                    {
                        question: "Split-brain in distributed systems occurs when:",
                        options: ["Nodes agree", "Partitioned nodes both think they're the leader", "Network is healthy", "Only one leader exists"],
                        correct: 1,
                        explanation: "Split-brain happens when partition creates multiple leaders."
                    }
                ],
                'expert': [
                    {
                        question: "Google Spanner achieves external consistency by:",
                        options: ["Ignoring CAP", "Using TrueTime for globally synchronized timestamps", "Being eventually consistent", "Sacrificing availability completely"],
                        correct: 1,
                        explanation: "TrueTime's bounded uncertainty enables strong consistency globally."
                    },
                    {
                        question: "Banking systems often choose CP because:",
                        options: ["It's faster", "Incorrect balances are worse than temporary unavailability", "It's cheaper", "Regulations require AP"],
                        correct: 1,
                        explanation: "Financial data must be consistent; brief unavailability is acceptable."
                    },
                    {
                        question: "Social media feeds often choose AP because:",
                        options: ["Consistency doesn't matter", "Showing slightly stale content is better than no content", "They don't use databases", "CAP doesn't apply"],
                        correct: 1,
                        explanation: "User experience favors availability; eventual consistency is acceptable."
                    },
                    {
                        question: "CRDTs help with CAP by:",
                        options: ["Eliminating tradeoffs", "Enabling automatic conflict resolution for eventual consistency", "Guaranteeing strong consistency", "Removing partitions"],
                        correct: 1,
                        explanation: "CRDTs merge concurrent updates automatically, making AP practical."
                    },
                    {
                        question: "The harvest/yield framework measures:",
                        options: ["Crop production", "Completeness of data (harvest) vs availability (yield)", "Network speed", "Storage capacity"],
                        correct: 1,
                        explanation: "Harvest/yield provides nuanced view of availability and consistency."
                    }
                ]
            },

            // Day 8: Consistency Models
            'hld_22': {
                'easy': [
                    {
                        question: "Strong consistency means:",
                        options: ["Eventually consistent", "Reads always return the most recent write", "Sometimes consistent", "Never consistent"],
                        correct: 1,
                        explanation: "Strong consistency guarantees reads see the latest committed write."
                    },
                    {
                        question: "Eventual consistency means:",
                        options: ["Never consistent", "Given time, all replicas converge to the same value", "Immediately consistent", "Random consistency"],
                        correct: 1,
                        explanation: "Eventual consistency guarantees convergence if no new updates occur."
                    },
                    {
                        question: "Read-your-writes consistency ensures:",
                        options: ["Others see your writes", "You always see your own recent writes", "Global consistency", "No reads allowed"],
                        correct: 1,
                        explanation: "Read-your-writes means a user sees their own updates immediately."
                    },
                    {
                        question: "Monotonic reads guarantee:",
                        options: ["Faster reads", "Once you read a value, you never see older values", "Older values only", "Random values"],
                        correct: 1,
                        explanation: "Monotonic reads prevent seeing data go backward in time."
                    },
                    {
                        question: "Causal consistency ensures:",
                        options: ["No ordering", "Causally related operations appear in order", "Random ordering", "Alphabetical ordering"],
                        correct: 1,
                        explanation: "Causal consistency preserves the order of related operations."
                    }
                ],
                'medium': [
                    {
                        question: "Linearizability requires:",
                        options: ["No ordering", "Operations appear to occur instantaneously at some point", "Eventual consistency", "Weak consistency"],
                        correct: 1,
                        explanation: "Linearizability makes the system behave as if single-threaded."
                    },
                    {
                        question: "Sequential consistency differs from linearizability in:",
                        options: ["No difference", "Doesn't require real-time ordering, just consistent order", "Being weaker overall", "Being stronger overall"],
                        correct: 1,
                        explanation: "Sequential consistency requires consistent order but not real-time."
                    },
                    {
                        question: "Session consistency provides:",
                        options: ["Global guarantees", "Consistency within a single client session", "No guarantees", "Random behavior"],
                        correct: 1,
                        explanation: "Session consistency ensures a client sees consistent data in their session."
                    },
                    {
                        question: "Quorum reads/writes help achieve:",
                        options: ["Weaker consistency", "Configurable consistency levels", "No consistency", "Only eventual"],
                        correct: 1,
                        explanation: "Quorums let you trade off consistency and availability."
                    },
                    {
                        question: "Vector clocks track:",
                        options: ["Wall clock time", "Causal relationships between events", "Server load", "Network latency"],
                        correct: 1,
                        explanation: "Vector clocks capture happens-before relationships."
                    }
                ],
                'hard': [
                    {
                        question: "Last-write-wins (LWW) can lose data when:",
                        options: ["Never", "Concurrent writes happen with clock skew", "Writes are sequential", "Clocks are synchronized"],
                        correct: 1,
                        explanation: "LWW silently discards concurrent updates based on timestamps."
                    },
                    {
                        question: "Read repair in Cassandra:",
                        options: ["Repairs clients", "Fixes inconsistencies detected during reads", "Only writes", "Never repairs"],
                        correct: 1,
                        explanation: "Read repair updates stale replicas when inconsistencies are found."
                    },
                    {
                        question: "Anti-entropy protocols like Merkle trees:",
                        options: ["Add entropy", "Efficiently detect and repair replica inconsistencies", "Slow down sync", "Increase inconsistency"],
                        correct: 1,
                        explanation: "Merkle trees enable efficient comparison and sync of replica state."
                    },
                    {
                        question: "Bounded staleness consistency guarantees:",
                        options: ["No bounds", "Reads are within a time or version window of latest", "Immediate consistency", "No staleness"],
                        correct: 1,
                        explanation: "Bounded staleness limits how out-of-date reads can be."
                    },
                    {
                        question: "Consistent prefix reads ensure:",
                        options: ["Random order", "Reads see writes in the order they were made", "No ordering", "Reverse order"],
                        correct: 1,
                        explanation: "Consistent prefix prevents seeing effects without their causes."
                    }
                ],
                'expert': [
                    {
                        question: "Jepsen testing framework verifies:",
                        options: ["Performance only", "Consistency guarantees under network failures", "Code style", "Security only"],
                        correct: 1,
                        explanation: "Jepsen tests if databases maintain consistency claims under faults."
                    },
                    {
                        question: "Serializability means transactions appear:",
                        options: ["In parallel", "As if executed one at a time in some order", "In random order", "With conflicts"],
                        correct: 1,
                        explanation: "Serializable execution is equivalent to some serial order."
                    },
                    {
                        question: "Snapshot isolation prevents:",
                        options: ["All anomalies", "Dirty reads and non-repeatable reads, but not write skew", "No anomalies", "Only dirty reads"],
                        correct: 1,
                        explanation: "Snapshot isolation has weaker guarantees than serializability."
                    },
                    {
                        question: "Multi-version concurrency control (MVCC) enables:",
                        options: ["Single version", "Readers and writers to not block each other", "Blocking reads", "No concurrency"],
                        correct: 1,
                        explanation: "MVCC keeps multiple versions so reads don't block writes."
                    },
                    {
                        question: "Choosing consistency level should consider:",
                        options: ["Only performance", "Business requirements, latency needs, and failure scenarios", "Only cost", "Random selection"],
                        correct: 1,
                        explanation: "Consistency choice depends on business needs and acceptable tradeoffs."
                    }
                ]
            },

            // Day 8: Consensus Algorithms
            'hld_23': {
                'easy': [
                    {
                        question: "Consensus algorithms help distributed nodes:",
                        options: ["Disagree", "Agree on a single value or state", "Work independently", "Ignore each other"],
                        correct: 1,
                        explanation: "Consensus ensures all nodes agree on shared decisions."
                    },
                    {
                        question: "Raft is designed to be:",
                        options: ["Complex", "Understandable while providing strong consistency", "Eventually consistent", "Fastest possible"],
                        correct: 1,
                        explanation: "Raft prioritizes understandability over optimization."
                    },
                    {
                        question: "In Raft, the leader:",
                        options: ["Is optional", "Handles all client writes and replicates to followers", "Only reads", "Changes every request"],
                        correct: 1,
                        explanation: "Raft uses a single leader for writes to simplify consensus."
                    },
                    {
                        question: "Paxos is known for being:",
                        options: ["Simple", "Correct but complex and hard to implement", "Fast", "Eventually consistent"],
                        correct: 1,
                        explanation: "Paxos is provably correct but notoriously difficult to understand."
                    },
                    {
                        question: "A quorum in consensus is:",
                        options: ["All nodes", "A majority of nodes (N/2 + 1)", "Any single node", "No nodes"],
                        correct: 1,
                        explanation: "Quorum ensures overlapping majorities for consistency."
                    }
                ],
                'medium': [
                    {
                        question: "Raft leader election uses:",
                        options: ["Random selection", "Randomized timeouts to trigger elections", "Manual selection", "Round-robin"],
                        correct: 1,
                        explanation: "Randomized timeouts prevent simultaneous elections."
                    },
                    {
                        question: "etcd uses Raft for:",
                        options: ["Data compression", "Reliable key-value storage with strong consistency", "Caching", "Message queuing"],
                        correct: 1,
                        explanation: "etcd provides consistent configuration storage using Raft."
                    },
                    {
                        question: "Log replication in Raft ensures:",
                        options: ["No logs", "All followers have the same log as the leader", "Different logs", "Partial logs"],
                        correct: 1,
                        explanation: "Raft replicates the leader's log to all followers for consistency."
                    },
                    {
                        question: "Split-brain prevention in consensus requires:",
                        options: ["No prevention", "Only one leader can have a quorum at a time", "Multiple leaders", "No communication"],
                        correct: 1,
                        explanation: "Quorum requirements prevent multiple leaders from accepting writes."
                    },
                    {
                        question: "ZooKeeper uses ZAB which is:",
                        options: ["Not consensus", "Similar to Paxos, optimized for primary-backup", "Identical to Raft", "Eventually consistent"],
                        correct: 1,
                        explanation: "ZAB (Zookeeper Atomic Broadcast) is a Paxos variant."
                    }
                ],
                'hard': [
                    {
                        question: "FLP impossibility theorem states:",
                        options: ["Consensus is easy", "Deterministic consensus is impossible with even one faulty node in async systems", "Consensus always works", "Only affects Paxos"],
                        correct: 1,
                        explanation: "FLP proves consensus limitations in asynchronous systems."
                    },
                    {
                        question: "Multi-Paxos optimizes Paxos by:",
                        options: ["Running more rounds", "Skipping prepare phase for subsequent values with stable leader", "Removing the leader", "Using more nodes"],
                        correct: 1,
                        explanation: "Multi-Paxos amortizes leader election over many decisions."
                    },
                    {
                        question: "Byzantine fault tolerance handles:",
                        options: ["Only crashes", "Malicious or arbitrary node behavior", "No faults", "Network partitions only"],
                        correct: 1,
                        explanation: "BFT handles nodes that lie or behave arbitrarily."
                    },
                    {
                        question: "PBFT requires at least this many nodes to tolerate f Byzantine faults:",
                        options: ["f nodes", "3f + 1 nodes", "2f nodes", "f + 1 nodes"],
                        correct: 1,
                        explanation: "PBFT needs 3f + 1 nodes to tolerate f Byzantine failures."
                    },
                    {
                        question: "Raft's term number prevents:",
                        options: ["Nothing", "Stale leaders from causing inconsistency", "New elections", "All failures"],
                        correct: 1,
                        explanation: "Higher terms invalidate commands from old leaders."
                    }
                ],
                'expert': [
                    {
                        question: "Google's Chubby lock service uses Paxos to provide:",
                        options: ["Caching", "Highly available distributed locking", "Message queuing", "Data storage only"],
                        correct: 1,
                        explanation: "Chubby provides locks and small file storage using Paxos."
                    },
                    {
                        question: "Viewstamped Replication influenced:",
                        options: ["Nothing", "Raft's design with similar concepts", "Only Paxos", "No modern systems"],
                        correct: 1,
                        explanation: "VR predates Raft and shares many similar ideas."
                    },
                    {
                        question: "EPaxos improves on Multi-Paxos by:",
                        options: ["Adding more phases", "Allowing any node to lead with no ordering conflicts", "Requiring more nodes", "Removing consensus"],
                        correct: 1,
                        explanation: "EPaxos enables leaderless operation with fast commit paths."
                    },
                    {
                        question: "Consensus for configuration changes (like adding nodes) requires:",
                        options: ["No special handling", "Joint consensus or similar safe transition protocol", "System restart", "Manual intervention always"],
                        correct: 1,
                        explanation: "Membership changes need careful handling to avoid safety violations."
                    },
                    {
                        question: "CockroachDB uses Raft because:",
                        options: ["It's the only option", "Understandability aids debugging in distributed SQL", "It's fastest", "Paxos doesn't work"],
                        correct: 1,
                        explanation: "Raft's clarity helps maintain correctness in complex systems."
                    }
                ]
            },

            // Day 9: Distributed Transactions
            'hld_24': {
                'easy': [
                    {
                        question: "A distributed transaction spans:",
                        options: ["One database", "Multiple databases or services", "No databases", "Only caches"],
                        correct: 1,
                        explanation: "Distributed transactions coordinate operations across multiple systems."
                    },
                    {
                        question: "2PC stands for:",
                        options: ["Two Primary Clusters", "Two-Phase Commit", "Two Process Communication", "Two Partition Control"],
                        correct: 1,
                        explanation: "Two-Phase Commit coordinates distributed transaction commits."
                    },
                    {
                        question: "The coordinator in 2PC:",
                        options: ["Does nothing", "Orchestrates the prepare and commit phases", "Only stores data", "Is optional"],
                        correct: 1,
                        explanation: "The coordinator manages the voting and commit/abort decision."
                    },
                    {
                        question: "Saga pattern is an alternative to:",
                        options: ["Caching", "Distributed transactions using compensating actions", "Single transactions", "No transactions"],
                        correct: 1,
                        explanation: "Sagas use compensating transactions instead of distributed locks."
                    },
                    {
                        question: "Atomicity in transactions means:",
                        options: ["Partially complete", "All operations succeed or all fail", "Random success", "No guarantees"],
                        correct: 1,
                        explanation: "Atomicity ensures transactions are all-or-nothing."
                    }
                ],
                'medium': [
                    {
                        question: "2PC's main weakness is:",
                        options: ["Too fast", "Blocking if coordinator fails after prepare", "Too simple", "No consensus needed"],
                        correct: 1,
                        explanation: "Participants holding locks can block indefinitely if coordinator dies."
                    },
                    {
                        question: "Compensating transactions in Saga:",
                        options: ["Are automatic", "Undo the effects of previous steps when failure occurs", "Never needed", "Only for success"],
                        correct: 1,
                        explanation: "Compensation logic must be explicitly designed for each step."
                    },
                    {
                        question: "Choreography-based Saga uses:",
                        options: ["Central coordinator", "Events for services to react to each other", "No communication", "Only synchronous calls"],
                        correct: 1,
                        explanation: "Services communicate via events without central coordination."
                    },
                    {
                        question: "Orchestration-based Saga uses:",
                        options: ["No coordinator", "A central orchestrator to direct the workflow", "Only events", "Random execution"],
                        correct: 1,
                        explanation: "An orchestrator explicitly controls the saga flow."
                    },
                    {
                        question: "Idempotency in distributed transactions ensures:",
                        options: ["Different results", "Repeated operations have the same effect", "More operations", "Fewer operations"],
                        correct: 1,
                        explanation: "Idempotency makes retries safe in unreliable networks."
                    }
                ],
                'hard': [
                    {
                        question: "3PC (Three-Phase Commit) adds:",
                        options: ["More blocking", "A pre-commit phase to reduce blocking window", "Less safety", "No improvement"],
                        correct: 1,
                        explanation: "3PC adds a phase to prevent indefinite blocking."
                    },
                    {
                        question: "TCC (Try-Confirm-Cancel) pattern:",
                        options: ["Same as 2PC", "Reserves resources, then confirms or cancels", "No reservation", "Only cancels"],
                        correct: 1,
                        explanation: "TCC reserves resources first, allowing safer cancellation."
                    },
                    {
                        question: "Outbox pattern helps with:",
                        options: ["Nothing", "Reliable event publishing with database transactions", "Faster transactions", "Less events"],
                        correct: 1,
                        explanation: "Outbox ensures events are published only when DB commits."
                    },
                    {
                        question: "Distributed deadlocks occur when:",
                        options: ["Never happen", "Transactions across services wait for each other's locks", "Only in single DB", "Locks don't exist"],
                        correct: 1,
                        explanation: "Distributed deadlock detection is much harder than single-node."
                    },
                    {
                        question: "XA protocol standardizes:",
                        options: ["Nothing", "Two-phase commit interface for databases", "REST APIs", "Event formats"],
                        correct: 1,
                        explanation: "XA defines how transaction managers coordinate with databases."
                    }
                ],
                'expert': [
                    {
                        question: "Uber's Cadence/Temporal provides:",
                        options: ["Simple caching", "Durable execution for long-running workflows", "Database storage", "Message queuing only"],
                        correct: 1,
                        explanation: "Temporal orchestrates complex workflows with reliability guarantees."
                    },
                    {
                        question: "Exactly-once semantics in distributed transactions requires:",
                        options: ["Nothing special", "Idempotency plus deduplication", "Only retries", "Only timeouts"],
                        correct: 1,
                        explanation: "Exactly-once is typically achieved through idempotent operations."
                    },
                    {
                        question: "Seata provides:",
                        options: ["Caching", "Distributed transaction management for microservices", "Message queuing", "Service discovery"],
                        correct: 1,
                        explanation: "Seata offers AT, TCC, and Saga transaction modes."
                    },
                    {
                        question: "Local-first then coordinate pattern:",
                        options: ["Coordinates first", "Commits locally first, then handles cross-service coordination", "No local commits", "Only coordination"],
                        correct: 1,
                        explanation: "This pattern reduces distributed transaction scope."
                    },
                    {
                        question: "Choosing between 2PC and Saga depends on:",
                        options: ["Random choice", "Consistency requirements vs availability needs", "Only performance", "Only cost"],
                        correct: 1,
                        explanation: "2PC for strong consistency; Saga for availability and resilience."
                    }
                ]
            },

            // Day 9: Eventual Consistency Patterns
            'hld_25': {
                'easy': [
                    {
                        question: "Eventual consistency guarantees:",
                        options: ["Immediate consistency", "All replicas converge given enough time", "Never consistent", "Random consistency"],
                        correct: 1,
                        explanation: "Eventually consistent systems converge when updates stop."
                    },
                    {
                        question: "Last-write-wins (LWW) resolves conflicts by:",
                        options: ["First write", "Using the write with latest timestamp", "Merging all writes", "Rejecting all writes"],
                        correct: 1,
                        explanation: "LWW keeps the value with the most recent timestamp."
                    },
                    {
                        question: "CRDTs are:",
                        options: ["Databases", "Data structures that automatically merge without conflicts", "Message queues", "Caching systems"],
                        correct: 1,
                        explanation: "Conflict-free Replicated Data Types merge deterministically."
                    },
                    {
                        question: "Read repair fixes:",
                        options: ["Client bugs", "Stale replicas detected during reads", "Write errors", "Network issues"],
                        correct: 1,
                        explanation: "Read repair updates outdated replicas when inconsistency is detected."
                    },
                    {
                        question: "Anti-entropy is a process that:",
                        options: ["Increases disorder", "Compares and synchronizes replica data", "Deletes data", "Adds randomness"],
                        correct: 1,
                        explanation: "Anti-entropy periodically reconciles differences between replicas."
                    }
                ],
                'medium': [
                    {
                        question: "Vector clocks help detect:",
                        options: ["Time zones", "Concurrent updates that need conflict resolution", "Server locations", "Network speed"],
                        correct: 1,
                        explanation: "Vector clocks identify causally unrelated (concurrent) operations."
                    },
                    {
                        question: "G-Counter CRDT allows:",
                        options: ["Decrements only", "Only increments, mergeable by taking max per node", "Any operations", "No operations"],
                        correct: 1,
                        explanation: "G-Counter counts by node, merging via max for each."
                    },
                    {
                        question: "PN-Counter CRDT enables:",
                        options: ["Only positives", "Both increments and decrements using two G-Counters", "Only decrements", "No counting"],
                        correct: 1,
                        explanation: "PN-Counter uses positive and negative G-Counters."
                    },
                    {
                        question: "Amazon's shopping cart uses eventual consistency because:",
                        options: ["It's cheaper", "Availability is more important than perfect consistency", "It's required", "No other option"],
                        correct: 1,
                        explanation: "Users prefer seeing a cart (possibly stale) over errors."
                    },
                    {
                        question: "Merge functions in CRDTs must be:",
                        options: ["Complex", "Commutative, associative, and idempotent", "Random", "Manual"],
                        correct: 1,
                        explanation: "These properties ensure merges work regardless of order."
                    }
                ],
                'hard': [
                    {
                        question: "LWW-Element-Set CRDT handles:",
                        options: ["Only additions", "Adds and removes with timestamps for conflict resolution", "Only removes", "No changes"],
                        correct: 1,
                        explanation: "LWW-Element-Set uses timestamps for add/remove conflicts."
                    },
                    {
                        question: "Observed-Remove Set (OR-Set) improves on LWW-Set by:",
                        options: ["Being simpler", "Using unique tags to track adds for proper removes", "Ignoring removes", "Rejecting adds"],
                        correct: 1,
                        explanation: "OR-Set tracks add operations uniquely for precise removal."
                    },
                    {
                        question: "Convergence time in eventual consistency depends on:",
                        options: ["Nothing", "Network latency, replication lag, and sync frequency", "Only data size", "Only node count"],
                        correct: 1,
                        explanation: "Multiple factors affect how quickly replicas converge."
                    },
                    {
                        question: "Riak uses CRDTs to:",
                        options: ["Store files", "Automatically resolve conflicts in distributed storage", "Manage users", "Handle authentication"],
                        correct: 1,
                        explanation: "Riak provides CRDT data types for conflict-free operations."
                    },
                    {
                        question: "Conflict-free doesn't mean:",
                        options: ["No conflicts occur", "Conflicts are resolved automatically without user intervention", "Always consistent", "Perfectly ordered"],
                        correct: 1,
                        explanation: "Conflicts still occur but are resolved deterministically."
                    }
                ],
                'expert': [
                    {
                        question: "Automerge uses operational transformation because:",
                        options: ["It's simpler", "Collaborative editing needs to merge concurrent text changes", "It's required", "No other option"],
                        correct: 1,
                        explanation: "OT handles concurrent text edits in collaborative applications."
                    },
                    {
                        question: "Œ¥-CRDTs (delta-state) improve efficiency by:",
                        options: ["Using more bandwidth", "Sending only state changes instead of full state", "Being slower", "Requiring more storage"],
                        correct: 1,
                        explanation: "Delta-state CRDTs reduce bandwidth by transmitting differences."
                    },
                    {
                        question: "Facebook's Apollo uses CRDTs for:",
                        options: ["User profiles", "Counting aggregations in distributed cache", "File storage", "Authentication"],
                        correct: 1,
                        explanation: "Apollo uses CRDTs for aggregated metrics across cache nodes."
                    },
                    {
                        question: "When designing for eventual consistency, always consider:",
                        options: ["Only happy path", "What happens when readers see stale data", "Only performance", "Only cost"],
                        correct: 1,
                        explanation: "Application logic must handle temporarily inconsistent views."
                    },
                    {
                        question: "Hybrid logical clocks combine:",
                        options: ["Nothing", "Physical time with logical counters for causality", "Only physical time", "Only logical time"],
                        correct: 1,
                        explanation: "HLCs provide causality tracking with real-time correlation."
                    }
                ]
            },

            // Day 9: Distributed Locking
            'hld_26': {
                'easy': [
                    {
                        question: "Distributed locks ensure:",
                        options: ["Data encryption", "Only one process accesses a resource across multiple nodes", "Data compression", "Data replication"],
                        correct: 1,
                        explanation: "Distributed locks provide mutual exclusion across systems."
                    },
                    {
                        question: "Redis SETNX is used for locks because:",
                        options: ["It's slow", "It's atomic set-if-not-exists", "It's synchronous", "It's persistent"],
                        correct: 1,
                        explanation: "SETNX atomically sets a key only if it doesn't exist."
                    },
                    {
                        question: "Lock TTL (expiration) prevents:",
                        options: ["Fast execution", "Locks being held forever if holder crashes", "Lock acquisition", "Normal operation"],
                        correct: 1,
                        explanation: "TTL ensures locks eventually release even if holder dies."
                    },
                    {
                        question: "ZooKeeper locks use:",
                        options: ["Files", "Ephemeral sequential nodes", "Regular nodes", "No nodes"],
                        correct: 1,
                        explanation: "Ephemeral nodes auto-delete when session ends; sequential for ordering."
                    },
                    {
                        question: "Fencing tokens prevent:",
                        options: ["Lock acquisition", "Delayed lock holders from causing issues after lock expires", "All writes", "All reads"],
                        correct: 1,
                        explanation: "Fencing tokens identify which lock holder's operations are valid."
                    }
                ],
                'medium': [
                    {
                        question: "Redlock algorithm uses:",
                        options: ["Single Redis", "Multiple independent Redis instances for reliability", "No Redis", "Only ZooKeeper"],
                        correct: 1,
                        explanation: "Redlock acquires locks from majority of Redis instances."
                    },
                    {
                        question: "Lock holder identification prevents:",
                        options: ["Lock acquisition", "Other processes accidentally releasing your lock", "Fast release", "TTL expiration"],
                        correct: 1,
                        explanation: "Unique IDs ensure only the lock owner can release it."
                    },
                    {
                        question: "Distributed locks are harder than local locks because:",
                        options: ["Simpler", "Network delays and failures can cause inconsistencies", "No differences", "Faster"],
                        correct: 1,
                        explanation: "Network partitions and delays complicate distributed locking."
                    },
                    {
                        question: "Lock renewal (heartbeat) is needed when:",
                        options: ["Never", "Operation may take longer than TTL", "Operations are fast", "Locks have no TTL"],
                        correct: 1,
                        explanation: "Long operations need lock extension before TTL expires."
                    },
                    {
                        question: "Google's Chubby provides:",
                        options: ["Caching", "Distributed lock service for infrastructure coordination", "Database storage", "Message queuing"],
                        correct: 1,
                        explanation: "Chubby is Google's distributed lock and file service."
                    }
                ],
                'hard': [
                    {
                        question: "The Redlock controversy centers on:",
                        options: ["Speed", "Whether it's safe during clock drift and network issues", "Cost", "Complexity only"],
                        correct: 1,
                        explanation: "Martin Kleppmann argued Redlock can fail under certain conditions."
                    },
                    {
                        question: "Lock ordering prevents:",
                        options: ["Lock acquisition", "Deadlocks by always acquiring locks in consistent order", "Fast execution", "Normal locking"],
                        correct: 1,
                        explanation: "Consistent ordering eliminates circular wait conditions."
                    },
                    {
                        question: "Try-lock with timeout helps prevent:",
                        options: ["Successful locks", "Indefinite blocking waiting for locks", "TTL expiration", "Lock release"],
                        correct: 1,
                        explanation: "Timeouts prevent processes from blocking forever."
                    },
                    {
                        question: "Lease-based locks in etcd provide:",
                        options: ["Permanent locks", "Automatic expiration with session heartbeat", "No expiration", "Manual release only"],
                        correct: 1,
                        explanation: "etcd leases auto-expire unless renewed, preventing orphan locks."
                    },
                    {
                        question: "For correctness, fencing tokens must be:",
                        options: ["Random", "Monotonically increasing", "Decreasing", "Constant"],
                        correct: 1,
                        explanation: "Increasing tokens ensure older holders can't override newer ones."
                    }
                ],
                'expert': [
                    {
                        question: "DynamoDB conditional writes can act as locks because:",
                        options: ["They're slow", "They atomically check conditions before writing", "They're simple", "No reason"],
                        correct: 1,
                        explanation: "Conditional writes provide atomic check-and-set operations."
                    },
                    {
                        question: "Lock-free algorithms avoid distributed locks by:",
                        options: ["Using more locks", "Using atomic operations like CAS instead", "Ignoring concurrency", "Using global locks"],
                        correct: 1,
                        explanation: "Lock-free uses compare-and-swap for coordination without locks."
                    },
                    {
                        question: "Advisory locks differ from enforced locks in:",
                        options: ["Nothing", "Advisory relies on cooperation; enforced prevents access", "Speed only", "Being weaker"],
                        correct: 1,
                        explanation: "Advisory locks require all participants to check; enforced blocks."
                    },
                    {
                        question: "Preferring idempotent operations over locks improves:",
                        options: ["Nothing", "Availability and reduces coordination overhead", "Only speed", "Only consistency"],
                        correct: 1,
                        explanation: "Idempotent operations can retry safely without lock complexity."
                    },
                    {
                        question: "Read-write locks in distributed systems need:",
                        options: ["No special handling", "Careful coordination to prevent writer starvation", "Only readers", "Only writers"],
                        correct: 1,
                        explanation: "Balancing read/write access requires fair scheduling."
                    }
                ]
            },

            // Day 10-15 quizzes (abbreviated for space, similar pattern)
            'hld_27': {
                'easy': [
                    { question: "Message queues decouple systems by:", options: ["Direct calls", "Storing messages between producer and consumer", "Removing communication", "Synchronous calls"], correct: 1, explanation: "Queues buffer messages, allowing independent operation." },
                    { question: "Point-to-point queue delivers each message to:", options: ["All consumers", "One consumer only", "No consumers", "Random consumers"], correct: 1, explanation: "Each message goes to exactly one consumer." },
                    { question: "Pub/sub delivers messages to:", options: ["One subscriber", "All subscribers", "No subscribers", "Random subscriber"], correct: 1, explanation: "All subscribers receive each published message." },
                    { question: "At-least-once delivery means:", options: ["Messages may be lost", "Messages may be delivered multiple times", "Exactly once", "Never delivered"], correct: 1, explanation: "At-least-once may duplicate but won't lose messages." },
                    { question: "Dead letter queue stores:", options: ["Successful messages", "Messages that failed processing", "All messages", "No messages"], correct: 1, explanation: "DLQ holds messages that couldn't be processed." }
                ],
                'medium': [
                    { question: "Idempotent consumers handle duplicates by:", options: ["Crashing", "Producing same result regardless of repetition", "Ignoring all messages", "Deleting messages"], correct: 1, explanation: "Idempotency makes repeated processing safe." },
                    { question: "Message ordering in queues:", options: ["Always guaranteed", "Often guaranteed only within partitions", "Never guaranteed", "Not important"], correct: 1, explanation: "Many queues order within partitions, not globally." },
                    { question: "RabbitMQ exchanges route messages using:", options: ["Random selection", "Binding keys and patterns", "Direct connection", "No routing"], correct: 1, explanation: "Exchanges route based on binding and routing keys." },
                    { question: "SQS visibility timeout prevents:", options: ["All processing", "Other consumers seeing in-process messages", "Message delivery", "Queue access"], correct: 1, explanation: "Visibility timeout hides messages being processed." },
                    { question: "Backpressure in message systems:", options: ["Speeds up producers", "Slows producers when consumers can't keep up", "Removes messages", "Adds consumers"], correct: 1, explanation: "Backpressure prevents overwhelming slow consumers." }
                ],
                'hard': [
                    { question: "Exactly-once semantics requires:", options: ["Simple retry", "Idempotent processing or transactional consumers", "No retries", "Ignoring failures"], correct: 1, explanation: "True exactly-once needs careful design or transactions." },
                    { question: "Message deduplication at broker level uses:", options: ["Content inspection", "Message IDs within a time window", "Random selection", "No deduplication"], correct: 1, explanation: "Brokers track message IDs to filter duplicates." },
                    { question: "Poison messages are:", options: ["Normal messages", "Messages that repeatedly fail processing", "Encrypted messages", "Compressed messages"], correct: 1, explanation: "Poison messages cause consumer failures repeatedly." },
                    { question: "FIFO queues in SQS guarantee:", options: ["No ordering", "Strict ordering and exactly-once delivery", "Random order", "Best-effort order"], correct: 1, explanation: "FIFO SQS provides ordering and deduplication." },
                    { question: "Message batching improves:", options: ["Latency always", "Throughput at cost of latency", "Nothing", "Only reliability"], correct: 1, explanation: "Batching trades latency for better throughput." }
                ],
                'expert': [
                    { question: "Transactional outbox pattern ensures:", options: ["Faster messages", "Database changes and messages are atomic", "No transactions", "Simpler code"], correct: 1, explanation: "Outbox commits messages with DB transaction." },
                    { question: "Consumer groups in message systems:", options: ["Slow processing", "Allow parallel consumption of partitions", "Serialize everything", "Remove partitions"], correct: 1, explanation: "Groups distribute partitions among consumers." },
                    { question: "Message priority queues risk:", options: ["Nothing", "Lower priority messages never being processed", "Faster processing", "Better ordering"], correct: 1, explanation: "Low priority can starve without fairness controls." },
                    { question: "Replay capability requires:", options: ["No retention", "Message retention and offset tracking", "Deleting old messages", "No tracking"], correct: 1, explanation: "Replay needs messages stored and position trackable." },
                    { question: "Choosing queue vs stream depends on:", options: ["Random choice", "Whether you need replay and retention vs simple task dispatch", "Only cost", "Only speed"], correct: 1, explanation: "Streams retain; queues delete after consumption." }
                ]
            },

            'hld_28': {
                'easy': [
                    { question: "Kafka stores messages in:", options: ["Memory only", "Distributed commit log", "Relational tables", "File system only"], correct: 1, explanation: "Kafka is a distributed commit log system." },
                    { question: "Kafka topics are divided into:", options: ["Tables", "Partitions", "Databases", "Files"], correct: 1, explanation: "Topics contain partitions for parallelism." },
                    { question: "Kafka consumers track position using:", options: ["Timestamps only", "Offsets", "Random position", "No tracking"], correct: 1, explanation: "Offsets mark consumer position in partitions." },
                    { question: "Kafka retains messages:", options: ["Until consumed", "Based on retention policy (time or size)", "Forever always", "Never"], correct: 1, explanation: "Retention is configurable by time or size." },
                    { question: "Kafka producers send messages to:", options: ["Random partitions", "Specific partitions based on key or round-robin", "All partitions", "Only one partition"], correct: 1, explanation: "Partitioning is by key hash or round-robin." }
                ],
                'medium': [
                    { question: "Consumer groups in Kafka:", options: ["Duplicate messages", "Share partitions for parallel consumption", "Ignore partitions", "Consume all partitions each"], correct: 1, explanation: "Groups divide partitions among members." },
                    { question: "Kafka replication provides:", options: ["Compression", "Fault tolerance through partition replicas", "Encryption", "Faster writes"], correct: 1, explanation: "Replicas protect against broker failures." },
                    { question: "ISR (In-Sync Replicas) are:", options: ["All replicas", "Replicas caught up with leader", "Failed replicas", "New replicas"], correct: 1, explanation: "ISR contains replicas fully synchronized with leader." },
                    { question: "Kafka Connect is for:", options: ["Client connections", "Streaming data between Kafka and external systems", "Network config", "Security"], correct: 1, explanation: "Connect imports/exports data to/from Kafka." },
                    { question: "LinkedIn created Kafka to handle:", options: ["File storage", "Activity stream data at massive scale", "User authentication", "Database queries"], correct: 1, explanation: "Kafka was built for LinkedIn's activity data needs." }
                ],
                'hard': [
                    { question: "Kafka Streams provides:", options: ["Batch processing only", "Stateful stream processing as a library", "Only stateless processing", "External clusters"], correct: 1, explanation: "Kafka Streams enables stateful processing without clusters." },
                    { question: "Compacted topics keep:", options: ["All messages", "Only latest value per key", "No messages", "Random messages"], correct: 1, explanation: "Compaction retains the latest value for each key." },
                    { question: "Exactly-once in Kafka requires:", options: ["Nothing special", "Idempotent producers and transactional consumers", "Only retries", "Only acks"], correct: 1, explanation: "EOS needs producer idempotence and transactions." },
                    { question: "Kafka's controller:", options: ["Handles all writes", "Manages partition leadership and cluster metadata", "Only caches", "Is optional"], correct: 1, explanation: "Controller handles leader election and metadata." },
                    { question: "Schema Registry with Kafka ensures:", options: ["No schemas", "Compatible message formats across versions", "Random formats", "Only JSON"], correct: 1, explanation: "Schema Registry enforces schema compatibility." }
                ],
                'expert': [
                    { question: "Uber processes Kafka messages at:", options: ["Thousands/day", "Trillions per day", "Hundreds/day", "None"], correct: 1, explanation: "Uber's scale requires massive Kafka throughput." },
                    { question: "Kafka's zero-copy optimization:", options: ["Copies more", "Sends data directly from disk to network", "Uses more CPU", "Slows transfers"], correct: 1, explanation: "Zero-copy avoids CPU overhead for data transfer." },
                    { question: "KRaft mode replaces:", options: ["Producers", "ZooKeeper with Raft-based metadata management", "Consumers", "Topics"], correct: 1, explanation: "KRaft removes ZooKeeper dependency." },
                    { question: "Partition reassignment affects:", options: ["Nothing", "Data movement and consumer rebalancing", "Only consumers", "Only producers"], correct: 1, explanation: "Reassignment moves data and updates consumers." },
                    { question: "Mirror Maker 2 enables:", options: ["Local replication", "Cross-datacenter Kafka replication", "Compression", "Encryption"], correct: 1, explanation: "MM2 replicates between Kafka clusters." }
                ]
            },

            'hld_29': {
                'easy': [
                    { question: "Event-driven architecture communicates via:", options: ["Direct calls only", "Events representing state changes", "Shared database", "File transfers"], correct: 1, explanation: "Services emit and react to events." },
                    { question: "Events represent:", options: ["Commands", "Facts that happened in the past", "Future actions", "Nothing"], correct: 1, explanation: "Events are immutable records of what occurred." },
                    { question: "Event producers:", options: ["Consume events", "Emit events when state changes", "Delete events", "Ignore events"], correct: 1, explanation: "Producers publish events when something happens." },
                    { question: "Event consumers:", options: ["Create events", "React to events from producers", "Modify events", "Delete producers"], correct: 1, explanation: "Consumers subscribe and respond to events." },
                    { question: "Loose coupling in event-driven means:", options: ["Tight dependencies", "Services don't know about each other directly", "No communication", "Direct calls"], correct: 1, explanation: "Services interact through events, not direct references." }
                ],
                'medium': [
                    { question: "Event sourcing stores:", options: ["Current state only", "Sequence of all events that led to current state", "No history", "Random events"], correct: 1, explanation: "Event sourcing preserves complete history as events." },
                    { question: "CQRS separates:", options: ["Nothing", "Read and write models for different optimization", "Only databases", "Only APIs"], correct: 1, explanation: "CQRS uses different models for commands and queries." },
                    { question: "Event choreography means:", options: ["Central control", "Services react independently to events", "No events", "Synchronous calls"], correct: 1, explanation: "Choreography has no central coordinator." },
                    { question: "Event orchestration uses:", options: ["No coordinator", "Central orchestrator directing flow", "Random routing", "Direct calls only"], correct: 1, explanation: "Orchestration has a controller managing the flow." },
                    { question: "Netflix uses event-driven for:", options: ["File storage", "Decoupling microservices at scale", "User login only", "Nothing"], correct: 1, explanation: "Netflix's microservices communicate via events." }
                ],
                'hard': [
                    { question: "Event versioning handles:", options: ["No changes", "Schema evolution over time", "Deleting events", "Faster events"], correct: 1, explanation: "Versioning manages backward compatibility." },
                    { question: "Projections in event sourcing:", options: ["Delete events", "Build read models from event streams", "Create events", "Modify history"], correct: 1, explanation: "Projections create queryable views from events." },
                    { question: "Temporal coupling in event systems:", options: ["Is desired", "Should be avoided for resilience", "Improves performance", "Has no effect"], correct: 1, explanation: "Temporal coupling reduces system resilience." },
                    { question: "Event replay enables:", options: ["Nothing", "Rebuilding state or creating new projections", "Deleting history", "Faster writes"], correct: 1, explanation: "Replay reconstructs state from event history." },
                    { question: "Saga pattern in event-driven handles:", options: ["Single transactions", "Distributed transactions via compensating events", "No transactions", "Only local transactions"], correct: 1, explanation: "Sagas coordinate cross-service transactions." }
                ],
                'expert': [
                    { question: "Event storming is a technique for:", options: ["Storing events", "Discovering domain events through collaboration", "Deleting events", "Compressing events"], correct: 1, explanation: "Event storming explores domain events collaboratively." },
                    { question: "Outbox pattern ensures:", options: ["Faster events", "Events are published when database commits", "No events", "Random publishing"], correct: 1, explanation: "Outbox guarantees event publication with DB atomicity." },
                    { question: "Change data capture (CDC) generates events from:", options: ["Manual triggers", "Database transaction logs", "User input only", "Random sources"], correct: 1, explanation: "CDC captures database changes as events." },
                    { question: "Event-carried state transfer includes:", options: ["Only event type", "Full data needed to process without callback", "No data", "Partial data only"], correct: 1, explanation: "Including all data eliminates back-queries." },
                    { question: "Eventual consistency in event-driven requires:", options: ["Ignoring consistency", "Designing for and handling temporary inconsistency", "Strong consistency", "No consideration"], correct: 1, explanation: "Applications must handle eventual consistency." }
                ]
            },

            // Day 11: Designing for Failure
            'hld_30': {
                'easy': [
                    { question: "Designing for failure assumes:", options: ["Nothing fails", "Components will fail and system must handle it", "Perfect infrastructure", "No planning needed"], correct: 1, explanation: "Assume failure; design systems to be resilient." },
                    { question: "Redundancy means:", options: ["Removing components", "Having backup components ready to take over", "Single point only", "No backups"], correct: 1, explanation: "Redundancy provides alternatives when components fail." },
                    { question: "Graceful degradation means:", options: ["Complete failure", "Reduced functionality instead of total outage", "No degradation", "Full performance always"], correct: 1, explanation: "Systems should partially work when components fail." },
                    { question: "Health checks determine:", options: ["User health", "Whether services are running and responsive", "Database size", "Network speed"], correct: 1, explanation: "Health checks detect service availability." },
                    { question: "Failover switches to:", options: ["Same failed component", "A backup when primary fails", "No component", "Random component"], correct: 1, explanation: "Failover activates standby resources." }
                ],
                'medium': [
                    { question: "Netflix's Chaos Monkey:", options: ["Organizes chaos", "Randomly terminates instances to test resilience", "Monitors chaos", "Creates backups"], correct: 1, explanation: "Chaos Monkey verifies systems survive random failures." },
                    { question: "Bulkhead pattern isolates:", options: ["Nothing", "Failures to prevent cascade across system", "All traffic", "Only successful requests"], correct: 1, explanation: "Bulkheads contain failures like ship compartments." },
                    { question: "Retry with exponential backoff:", options: ["Retries immediately", "Increases wait time between retries", "Never retries", "Decreases wait time"], correct: 1, explanation: "Exponential backoff prevents overwhelming services." },
                    { question: "Idempotent operations enable safe:", options: ["No retries", "Retries without side effects", "Failures", "Crashes"], correct: 1, explanation: "Idempotent operations can be safely repeated." },
                    { question: "Timeout settings prevent:", options: ["Fast responses", "Indefinite waiting for failed services", "All communication", "Success responses"], correct: 1, explanation: "Timeouts bound how long to wait for responses." }
                ],
                'hard': [
                    { question: "Cascading failures happen when:", options: ["One failure is contained", "One failure triggers others across the system", "Nothing fails", "Failures are isolated"], correct: 1, explanation: "Cascading failures spread through dependencies." },
                    { question: "Game days at companies involve:", options: ["Playing games", "Simulating failures to practice incident response", "Day off", "No testing"], correct: 1, explanation: "Game days test incident response procedures." },
                    { question: "Blue-green deployment enables:", options: ["No rollback", "Quick rollback by switching traffic to old version", "Only blue systems", "Only green systems"], correct: 1, explanation: "Blue-green keeps old version ready for instant rollback." },
                    { question: "Canary deployments detect failures by:", options: ["Full rollout", "Testing with small percentage of traffic first", "No testing", "Random deployment"], correct: 1, explanation: "Canary catches issues before full rollout." },
                    { question: "Failure injection testing:", options: ["Prevents testing", "Deliberately causes failures to verify resilience", "Is dangerous always", "Never used"], correct: 1, explanation: "Controlled failures verify system handles them." }
                ],
                'expert': [
                    { question: "Netflix's Simian Army includes:", options: ["Only Chaos Monkey", "Multiple tools testing different failure modes", "No tools", "One tool"], correct: 1, explanation: "Simian Army has various chaos engineering tools." },
                    { question: "Blast radius of a failure is:", options: ["Explosion size", "Scope of impact when something fails", "Always total", "Always minimal"], correct: 1, explanation: "Limiting blast radius contains failure impact." },
                    { question: "Cell-based architecture limits failures by:", options: ["No isolation", "Isolating customer groups into independent cells", "Sharing everything", "Global deployment"], correct: 1, explanation: "Cells isolate failures to subset of users." },
                    { question: "Shuffle sharding reduces correlated failures by:", options: ["More sharing", "Randomizing which resources serve which customers", "Fixed assignment", "No sharding"], correct: 1, explanation: "Shuffle sharding spreads blast radius randomly." },
                    { question: "Chaos engineering principles include:", options: ["Break production randomly", "Hypothesis-driven experiments with controlled impact", "No hypotheses", "Unlimited blast radius"], correct: 1, explanation: "Chaos engineering is scientific, not random destruction." }
                ]
            },

            // Day 11: Circuit Breakers
            'hld_31': {
                'easy': [
                    { question: "Circuit breaker pattern prevents:", options: ["All calls", "Calling failing services repeatedly", "Successful calls", "No prevention"], correct: 1, explanation: "Circuit breakers stop wasting resources on failing services." },
                    { question: "Closed circuit breaker state means:", options: ["Blocked", "Requests pass through normally", "Always failing", "Half working"], correct: 1, explanation: "Closed state allows normal operation." },
                    { question: "Open circuit breaker state means:", options: ["Working normally", "Requests fail fast without calling service", "Half working", "No state"], correct: 1, explanation: "Open state returns failures immediately." },
                    { question: "Half-open circuit breaker:", options: ["Never opens", "Allows test requests to check recovery", "Always blocks", "Has no purpose"], correct: 1, explanation: "Half-open tests if service has recovered." },
                    { question: "Netflix Hystrix provided:", options: ["Video streaming", "Circuit breaker and isolation patterns", "Database access", "Load balancing"], correct: 1, explanation: "Hystrix was Netflix's latency and fault tolerance library." }
                ],
                'medium': [
                    { question: "Circuit breaker trips after:", options: ["One failure", "Configurable number/rate of failures", "Never", "Random failures"], correct: 1, explanation: "Thresholds determine when to open the circuit." },
                    { question: "Fallback in circuit breaker:", options: ["Has no alternative", "Provides default response when circuit is open", "Only errors", "Ignores failures"], correct: 1, explanation: "Fallbacks give users something instead of errors." },
                    { question: "Circuit breaker timeout settings:", options: ["Don't matter", "Determine how long in open state before testing", "Are fixed", "Can't be configured"], correct: 1, explanation: "Timeout controls recovery attempt frequency." },
                    { question: "Request volume threshold:", options: ["Ignores traffic", "Requires minimum requests before tripping", "Trips on first request", "Has no effect"], correct: 1, explanation: "Volume threshold prevents tripping on low traffic." },
                    { question: "Error percentage threshold:", options: ["Measures speed", "Trips when failure rate exceeds limit", "Ignores errors", "Only counts successes"], correct: 1, explanation: "Error percentage determines circuit health." }
                ],
                'hard': [
                    { question: "Resilience4j is:", options: ["Database library", "Modern circuit breaker library for Java", "Message queue", "Web framework"], correct: 1, explanation: "Resilience4j replaced Hystrix with lightweight design." },
                    { question: "Circuit breaker per-service isolation:", options: ["Shares state", "Separate circuits prevent one failure affecting others", "Global state", "No isolation"], correct: 1, explanation: "Each dependency should have its own circuit breaker." },
                    { question: "Thread pool isolation with circuit breakers:", options: ["Shares threads", "Prevents slow services blocking all threads", "Uses one thread", "No threads"], correct: 1, explanation: "Thread pools contain impact of slow dependencies." },
                    { question: "Metrics from circuit breakers help:", options: ["Nothing", "Identify problematic dependencies and tune thresholds", "Only debugging", "Only logging"], correct: 1, explanation: "Metrics reveal service health and failure patterns." },
                    { question: "Circuit breaker vs retry:", options: ["Same thing", "Retry attempts again; breaker stops attempting", "Retry never retries", "Breaker retries more"], correct: 1, explanation: "They're complementary: retry for transient, breaker for sustained failures." }
                ],
                'expert': [
                    { question: "Adaptive circuit breakers:", options: ["Are fixed", "Automatically tune thresholds based on patterns", "Never adapt", "Only manual tuning"], correct: 1, explanation: "Adaptive breakers learn optimal thresholds." },
                    { question: "Semaphore vs thread pool isolation:", options: ["Same performance", "Semaphore lighter but less isolation than threads", "Thread pool is lighter", "No difference"], correct: 1, explanation: "Semaphores limit concurrency without thread overhead." },
                    { question: "Circuit breaker dashboards display:", options: ["Nothing useful", "Real-time circuit states and metrics across services", "Only errors", "Only successes"], correct: 1, explanation: "Dashboards provide visibility into system health." },
                    { question: "Combining circuit breaker with rate limiting:", options: ["Is redundant", "Provides defense in depth for resilience", "Breaks both patterns", "Is not possible"], correct: 1, explanation: "Multiple patterns complement each other." },
                    { question: "Testing circuit breakers requires:", options: ["Production only", "Simulating failures to verify behavior", "No testing", "Only unit tests"], correct: 1, explanation: "Circuit breaker logic needs failure simulation testing." }
                ]
            },

            // Day 11: Rate Limiting
            'hld_32': {
                'easy': [
                    { question: "Rate limiting controls:", options: ["Data size", "How many requests a client can make", "Response content", "Database queries only"], correct: 1, explanation: "Rate limiting prevents request abuse." },
                    { question: "Rate limits protect against:", options: ["Nothing", "Abuse, DoS attacks, and resource exhaustion", "Legitimate traffic", "All requests"], correct: 1, explanation: "Rate limiting protects systems from overload." },
                    { question: "HTTP 429 status code means:", options: ["Success", "Too Many Requests", "Not Found", "Server Error"], correct: 1, explanation: "429 indicates client exceeded rate limit." },
                    { question: "API rate limits are often measured in:", options: ["Bytes", "Requests per time window (e.g., 100/minute)", "Connections only", "Random units"], correct: 1, explanation: "Requests per time period is common rate limit measure." },
                    { question: "Rate limit headers tell clients:", options: ["Nothing", "Remaining requests and reset time", "Only errors", "Server info"], correct: 1, explanation: "Headers help clients manage their request rate." }
                ],
                'medium': [
                    { question: "Token bucket algorithm:", options: ["Uses physical tokens", "Refills tokens at fixed rate; requests consume tokens", "Never refills", "Has no bucket"], correct: 1, explanation: "Token bucket allows bursts while enforcing average rate." },
                    { question: "Leaky bucket algorithm:", options: ["Leaks data", "Processes requests at constant rate, queuing excess", "Has no limit", "Allows unlimited bursts"], correct: 1, explanation: "Leaky bucket smooths traffic to constant rate." },
                    { question: "Fixed window rate limiting:", options: ["No windows", "Counts requests in fixed time windows", "Rolling windows", "Unlimited windows"], correct: 1, explanation: "Fixed windows reset counters at window boundaries." },
                    { question: "Sliding window rate limiting:", options: ["Same as fixed", "Smooths limits across window boundaries", "No time consideration", "Is less accurate"], correct: 1, explanation: "Sliding window prevents boundary burst issues." },
                    { question: "Rate limiting at API Gateway:", options: ["Protects nothing", "Centralizes enforcement before requests reach services", "Only per service", "Is not possible"], correct: 1, explanation: "Gateway rate limiting provides centralized protection." }
                ],
                'hard': [
                    { question: "Distributed rate limiting requires:", options: ["Local counters only", "Shared state across nodes (Redis, etc.)", "No coordination", "Single server"], correct: 1, explanation: "Multiple nodes need shared counters for global limits." },
                    { question: "Rate limiting by user vs IP:", options: ["Same thing", "User is more accurate; IP can affect shared users", "IP is always better", "Neither works"], correct: 1, explanation: "User-based is precise; IP-based affects shared NATs." },
                    { question: "Adaptive rate limiting:", options: ["Is fixed", "Adjusts limits based on system load", "Never changes", "Is always manual"], correct: 1, explanation: "Adaptive limits respond to current system capacity." },
                    { question: "Rate limit by cost considers:", options: ["Only count", "Different request costs (expensive queries cost more)", "Equal cost", "No cost"], correct: 1, explanation: "Cost-based limits account for resource-heavy requests." },
                    { question: "Retry-After header tells clients:", options: ["Never retry", "When they can retry after rate limiting", "To retry immediately", "Random information"], correct: 1, explanation: "Retry-After indicates when limit resets." }
                ],
                'expert': [
                    { question: "Stripe's rate limiting uses:", options: ["Simple counters", "Token buckets with per-API-key limits", "No limits", "Only IP limits"], correct: 1, explanation: "Stripe has sophisticated per-key rate limiting." },
                    { question: "Cell-based rate limiting:", options: ["Global limits only", "Limits applied per cell/shard for isolation", "No cells", "Unlimited"], correct: 1, explanation: "Cell-based limits contain impact of abuse." },
                    { question: "Rate limiting vs load shedding:", options: ["Same thing", "Rate limiting is proactive; shedding reactive during overload", "Shedding is proactive", "Neither works"], correct: 1, explanation: "Rate limiting prevents overload; shedding handles it." },
                    { question: "Fairness in rate limiting ensures:", options: ["No fairness", "Resources distributed fairly among clients", "First-come only", "Random allocation"], correct: 1, explanation: "Fairness prevents some clients from monopolizing." },
                    { question: "Rate limiting bypass for critical services:", options: ["Never allowed", "May exempt health checks and internal services", "All services limited", "No exemptions possible"], correct: 1, explanation: "Critical internal traffic may need rate limit exemptions." }
                ]
            },

            // Day 12: Logging, Metrics, Tracing
            'hld_33': {
                'easy': [
                    { question: "Logs record:", options: ["Nothing", "Events and details about system behavior", "Only errors", "Only metrics"], correct: 1, explanation: "Logs capture what happened in the system." },
                    { question: "Metrics are:", options: ["Text descriptions", "Numerical measurements of system behavior", "Only logs", "Only traces"], correct: 1, explanation: "Metrics quantify system performance and state." },
                    { question: "Distributed tracing tracks:", options: ["Single service", "Requests across multiple services", "Only databases", "Only networks"], correct: 1, explanation: "Tracing follows requests through distributed systems." },
                    { question: "Log levels (DEBUG, INFO, WARN, ERROR) help:", options: ["Nothing", "Filter and prioritize log messages", "Only show errors", "Hide all logs"], correct: 1, explanation: "Log levels control verbosity and importance." },
                    { question: "Centralized logging collects logs:", options: ["Nowhere", "From all services into one searchable system", "Only locally", "From one service"], correct: 1, explanation: "Central logging enables searching across all services." }
                ],
                'medium': [
                    { question: "ELK Stack (Elasticsearch, Logstash, Kibana) provides:", options: ["Database storage", "Log aggregation, search, and visualization", "Message queuing", "Rate limiting"], correct: 1, explanation: "ELK is a popular logging and analysis stack." },
                    { question: "Prometheus is primarily for:", options: ["Logging", "Metrics collection and alerting", "Tracing", "Caching"], correct: 1, explanation: "Prometheus is a metrics and monitoring system." },
                    { question: "Jaeger and Zipkin are:", options: ["Databases", "Distributed tracing systems", "Log aggregators", "Metrics tools"], correct: 1, explanation: "Jaeger and Zipkin visualize distributed traces." },
                    { question: "Trace ID allows:", options: ["Random routing", "Following a request across all services", "Encryption", "Compression"], correct: 1, explanation: "Trace ID links related spans across services." },
                    { question: "RED metrics stand for:", options: ["Random Error Detection", "Rate, Errors, Duration for services", "Real Error Data", "Remote Event Data"], correct: 1, explanation: "RED measures request rate, error rate, and duration." }
                ],
                'hard': [
                    { question: "USE metrics stand for:", options: ["User System Errors", "Utilization, Saturation, Errors for resources", "Unified System Events", "Universal Storage Errors"], correct: 1, explanation: "USE measures resource utilization, saturation, errors." },
                    { question: "Cardinality explosion in metrics happens when:", options: ["Too few labels", "Too many unique label combinations", "No labels", "Fixed labels"], correct: 1, explanation: "High cardinality creates too many metric series." },
                    { question: "OpenTelemetry provides:", options: ["Only tracing", "Unified observability (traces, metrics, logs)", "Only metrics", "Only logs"], correct: 1, explanation: "OpenTelemetry standardizes all observability signals." },
                    { question: "Structured logging uses:", options: ["Plain text", "Key-value pairs or JSON for machine parsing", "Random formats", "No structure"], correct: 1, explanation: "Structured logs enable automated analysis." },
                    { question: "Sampling in tracing:", options: ["Captures everything", "Captures subset to reduce overhead", "Captures nothing", "Is required"], correct: 1, explanation: "Sampling reduces tracing overhead while preserving insights." }
                ],
                'expert': [
                    { question: "Google's Dapper influenced:", options: ["Nothing", "Most modern distributed tracing systems", "Only databases", "Only caching"], correct: 1, explanation: "Dapper paper inspired Zipkin, Jaeger, and others." },
                    { question: "Exemplars link:", options: ["Nothing", "Metrics to specific trace examples", "Only logs", "Only metrics"], correct: 1, explanation: "Exemplars connect metric data points to traces." },
                    { question: "Context propagation in tracing:", options: ["Is automatic always", "Passes trace context across service boundaries", "Is optional", "Only works locally"], correct: 1, explanation: "Context propagation maintains trace continuity." },
                    { question: "SLI (Service Level Indicator) is:", options: ["A promise", "A measurement of service behavior", "A target", "A penalty"], correct: 1, explanation: "SLIs measure what matters for service quality." },
                    { question: "Head-based vs tail-based sampling:", options: ["Same thing", "Head decides early; tail decides after seeing complete trace", "Tail is earlier", "Neither samples"], correct: 1, explanation: "Tail sampling can make smarter decisions with full context." }
                ]
            },

            // Day 12: Alerting
            'hld_34': {
                'easy': [
                    { question: "Alerts notify teams when:", options: ["Everything is normal", "Something requires attention", "Nothing happens", "Daily always"], correct: 1, explanation: "Alerts indicate problems needing human response." },
                    { question: "Alert fatigue occurs when:", options: ["Alerts are rare", "Too many alerts cause people to ignore them", "No alerts exist", "Alerts are perfect"], correct: 1, explanation: "Too many alerts desensitize responders." },
                    { question: "PagerDuty and Opsgenie are:", options: ["Databases", "Incident management and alerting platforms", "Message queues", "Monitoring tools"], correct: 1, explanation: "They manage on-call rotations and alert routing." },
                    { question: "Alert severity levels help:", options: ["Nothing", "Prioritize response based on impact", "Create more alerts", "Hide problems"], correct: 1, explanation: "Severity indicates urgency of response needed." },
                    { question: "On-call rotation ensures:", options: ["No one responds", "Someone is always available to respond", "Everyone responds", "Random response"], correct: 1, explanation: "Rotations distribute on-call responsibility." }
                ],
                'medium': [
                    { question: "Symptom-based alerts focus on:", options: ["Internal metrics", "User-visible problems (latency, errors)", "Only infrastructure", "Only logs"], correct: 1, explanation: "Symptom alerts catch what users actually experience." },
                    { question: "Cause-based alerts focus on:", options: ["User experience", "Underlying system problems (CPU, disk)", "Only symptoms", "Only errors"], correct: 1, explanation: "Cause alerts detect infrastructure issues." },
                    { question: "Alert thresholds should be:", options: ["As low as possible", "Based on what actually requires action", "Random", "Maximum always"], correct: 1, explanation: "Thresholds should trigger on actionable conditions." },
                    { question: "Runbooks accompany alerts to provide:", options: ["Nothing", "Steps to diagnose and resolve the issue", "Only history", "Only metrics"], correct: 1, explanation: "Runbooks guide responders through resolution." },
                    { question: "Alert aggregation combines:", options: ["Nothing", "Related alerts into single notification", "All alerts into one", "Unrelated alerts"], correct: 1, explanation: "Aggregation reduces noise from related issues." }
                ],
                'hard': [
                    { question: "SLO-based alerting fires when:", options: ["Any error occurs", "Error budget is being consumed too fast", "Never", "Always"], correct: 1, explanation: "SLO alerts focus on what matters to users." },
                    { question: "Burn rate alerts measure:", options: ["Temperature", "How fast error budget is being used", "Network speed", "Storage usage"], correct: 1, explanation: "Burn rate indicates SLO violation trajectory." },
                    { question: "Multi-window alerts reduce false positives by:", options: ["Ignoring all alerts", "Requiring sustained issues across time windows", "Single threshold", "Random checking"], correct: 1, explanation: "Multi-window catches real problems, not blips." },
                    { question: "Alert deduplication prevents:", options: ["All alerts", "Same issue triggering multiple notifications", "Important alerts", "Any notification"], correct: 1, explanation: "Deduplication reduces redundant notifications." },
                    { question: "Escalation policies define:", options: ["Nothing", "Who to notify if initial responders don't acknowledge", "Only first responder", "Random notification"], correct: 1, explanation: "Escalation ensures issues get attention." }
                ],
                'expert': [
                    { question: "Google's approach to alerting emphasizes:", options: ["More alerts", "Alerting on symptoms, fewer but more actionable alerts", "All possible alerts", "No alerts"], correct: 1, explanation: "Google recommends symptom-based, actionable alerts." },
                    { question: "Alert correlation identifies:", options: ["Nothing", "Related alerts from common root cause", "Unrelated alerts", "Random patterns"], correct: 1, explanation: "Correlation helps find underlying issues." },
                    { question: "Predictive alerting warns:", options: ["After problems", "Before problems occur based on trends", "Never", "Random times"], correct: 1, explanation: "Predictive alerts enable proactive response." },
                    { question: "Alert testing in staging:", options: ["Is not needed", "Verifies alerts fire correctly before production", "Only in production", "Never done"], correct: 1, explanation: "Testing alerts prevents gaps in monitoring." },
                    { question: "Incident response integration means alerts:", options: ["Stand alone", "Automatically create tickets and notify stakeholders", "Only notify", "Do nothing"], correct: 1, explanation: "Integration streamlines incident response workflow." }
                ]
            },

            // Day 12: APM
            'hld_35': {
                'easy': [
                    { question: "APM stands for:", options: ["Advanced Program Manager", "Application Performance Monitoring", "Automated Process Management", "Application Protocol Metrics"], correct: 1, explanation: "APM monitors application performance and health." },
                    { question: "APM tools help identify:", options: ["Nothing", "Performance bottlenecks and errors", "Only security issues", "Only network problems"], correct: 1, explanation: "APM reveals where applications are slow or failing." },
                    { question: "Datadog and New Relic are:", options: ["Databases", "APM platforms", "Message queues", "Load balancers"], correct: 1, explanation: "Datadog and New Relic provide comprehensive APM." },
                    { question: "Response time tracking shows:", options: ["Server count", "How long operations take", "Data size", "User count"], correct: 1, explanation: "Response time indicates user experience quality." },
                    { question: "Error rate in APM measures:", options: ["Speed", "Percentage of requests that fail", "Data volume", "User satisfaction"], correct: 1, explanation: "Error rate shows reliability problems." }
                ],
                'medium': [
                    { question: "APM agents collect data by:", options: ["Manual entry", "Instrumenting application code automatically", "Only logs", "External monitoring only"], correct: 1, explanation: "Agents instrument code for detailed metrics." },
                    { question: "Transaction tracing shows:", options: ["Nothing", "Complete path of a request through application", "Only database calls", "Only external calls"], correct: 1, explanation: "Transaction traces reveal request flow and timing." },
                    { question: "Percentile metrics (P99, P95) show:", options: ["Average only", "Performance for majority of requests", "Minimum only", "Maximum only"], correct: 1, explanation: "Percentiles reveal tail latency affecting many users." },
                    { question: "Database query analysis in APM identifies:", options: ["Nothing", "Slow queries and their impact", "Only count", "Only errors"], correct: 1, explanation: "Query analysis finds database bottlenecks." },
                    { question: "Real User Monitoring (RUM) captures:", options: ["Synthetic tests", "Actual user experience in browsers", "Server metrics only", "No user data"], correct: 1, explanation: "RUM measures real user performance." }
                ],
                'hard': [
                    { question: "Synthetic monitoring differs from RUM by:", options: ["Being the same", "Using scripted tests vs real user traffic", "Being less accurate", "Only running in production"], correct: 1, explanation: "Synthetic runs controlled tests; RUM captures real users." },
                    { question: "Flame graphs visualize:", options: ["Network traffic", "Code execution time as stacked visualization", "Log messages", "User journeys"], correct: 1, explanation: "Flame graphs show where code spends time." },
                    { question: "Service maps in APM show:", options: ["Nothing", "Dependencies between services", "Only errors", "Only slow services"], correct: 1, explanation: "Service maps visualize system architecture." },
                    { question: "APM correlation with infrastructure metrics:", options: ["Is not possible", "Links app issues to infrastructure problems", "Only shows app metrics", "Is always manual"], correct: 1, explanation: "Correlation connects app and infrastructure health." },
                    { question: "Anomaly detection in APM:", options: ["Requires manual thresholds", "Automatically identifies unusual patterns", "Only uses static rules", "Doesn't exist"], correct: 1, explanation: "ML-based anomaly detection finds problems without manual thresholds." }
                ],
                'expert': [
                    { question: "Continuous profiling provides:", options: ["One-time analysis", "Ongoing code performance analysis in production", "Only staging profiles", "No production data"], correct: 1, explanation: "Continuous profiling catches performance regressions." },
                    { question: "eBPF-based observability offers:", options: ["Less visibility", "Deep kernel-level insights without code changes", "More overhead", "Less accuracy"], correct: 1, explanation: "eBPF provides low-overhead deep observability." },
                    { question: "Golden signals (latency, traffic, errors, saturation):", options: ["Are outdated", "Provide core metrics for any service", "Only for specific services", "Require special tools"], correct: 1, explanation: "Golden signals are universal service health indicators." },
                    { question: "Cost optimization using APM data:", options: ["Is not possible", "Identifies over-provisioned or inefficient resources", "Only increases costs", "Requires manual analysis only"], correct: 1, explanation: "APM data helps right-size infrastructure." },
                    { question: "APM in serverless environments:", options: ["Doesn't work", "Requires different approaches for cold starts and ephemeral functions", "Is identical to servers", "Is not needed"], correct: 1, explanation: "Serverless APM needs different instrumentation strategies." }
                ]
            },

            // Day 13: Authentication & Authorization
            'hld_36': {
                'easy': [
                    { question: "Authentication verifies:", options: ["What you can do", "Who you are", "Where you are", "When you arrived"], correct: 1, explanation: "Authentication confirms identity." },
                    { question: "Authorization determines:", options: ["Who you are", "What you're allowed to do", "Your password", "Your username"], correct: 1, explanation: "Authorization controls access permissions." },
                    { question: "OAuth 2.0 is used for:", options: ["Encryption", "Delegated authorization", "Database access", "Network routing"], correct: 1, explanation: "OAuth allows apps to act on behalf of users." },
                    { question: "JWT stands for:", options: ["Java Web Token", "JSON Web Token", "JavaScript Web Transfer", "Just Write Text"], correct: 1, explanation: "JWT is a compact token format for claims." },
                    { question: "Multi-factor authentication uses:", options: ["One factor", "Multiple verification methods", "No verification", "Only passwords"], correct: 1, explanation: "MFA combines something you know, have, or are." }
                ],
                'medium': [
                    { question: "OIDC (OpenID Connect) adds to OAuth:", options: ["Nothing", "Identity layer with user info", "Database access", "Encryption"], correct: 1, explanation: "OIDC provides authentication on top of OAuth." },
                    { question: "Refresh tokens allow:", options: ["Immediate expiration", "Getting new access tokens without re-login", "Permanent access", "No renewal"], correct: 1, explanation: "Refresh tokens extend sessions securely." },
                    { question: "RBAC stands for:", options: ["Real-time Backup Access Control", "Role-Based Access Control", "Remote Basic Authentication Check", "Rapid Binary Access Code"], correct: 1, explanation: "RBAC assigns permissions through roles." },
                    { question: "ABAC (Attribute-Based Access Control) uses:", options: ["Only roles", "User, resource, and environment attributes", "Only usernames", "No attributes"], correct: 1, explanation: "ABAC makes decisions based on multiple attributes." },
                    { question: "Session tokens should be:", options: ["Predictable", "Random and unpredictable", "Sequential", "Reusable"], correct: 1, explanation: "Unpredictable tokens prevent session hijacking." }
                ],
                'hard': [
                    { question: "Token introspection allows:", options: ["Nothing", "Validating tokens at the authorization server", "Creating tokens", "Deleting tokens"], correct: 1, explanation: "Introspection verifies token validity." },
                    { question: "PKCE in OAuth protects against:", options: ["Nothing", "Authorization code interception attacks", "All attacks", "Only CSRF"], correct: 1, explanation: "PKCE secures OAuth for public clients." },
                    { question: "Zero Trust architecture assumes:", options: ["Trust everything", "Trust nothing; verify everything", "Trust network", "Trust users"], correct: 1, explanation: "Zero Trust verifies every request regardless of location." },
                    { question: "Service-to-service authentication often uses:", options: ["User passwords", "Mutual TLS or service accounts", "No authentication", "Browser cookies"], correct: 1, explanation: "Services authenticate differently than users." },
                    { question: "JWT signature verification ensures:", options: ["Encryption", "Token hasn't been tampered with", "Token is encrypted", "User exists"], correct: 1, explanation: "Signatures verify token integrity." }
                ],
                'expert': [
                    { question: "Google's BeyondCorp pioneered:", options: ["VPN access", "Zero Trust network access without VPN", "Traditional perimeter", "Password-only auth"], correct: 1, explanation: "BeyondCorp moved Google to Zero Trust model." },
                    { question: "Token binding prevents:", options: ["Nothing", "Token theft by binding to TLS connection", "All attacks", "Token creation"], correct: 1, explanation: "Binding ties tokens to specific connections." },
                    { question: "Federated identity allows:", options: ["One provider only", "Using external identity providers", "No external auth", "Only internal auth"], correct: 1, explanation: "Federation enables SSO across organizations." },
                    { question: "Policy engines like OPA evaluate:", options: ["Nothing", "Authorization decisions based on policy rules", "Only authentication", "Only encryption"], correct: 1, explanation: "OPA externalizes and standardizes authorization logic." },
                    { question: "Fine-grained authorization goes beyond RBAC by:", options: ["Being simpler", "Evaluating detailed context and relationships", "Using fewer rules", "Only checking roles"], correct: 1, explanation: "Fine-grained checks resource-level permissions." }
                ]
            },

            // Day 13: Encryption
            'hld_37': {
                'easy': [
                    { question: "Encryption at rest protects:", options: ["Network traffic", "Stored data on disk", "Data in transit", "Memory only"], correct: 1, explanation: "At-rest encryption protects stored data." },
                    { question: "Encryption in transit protects:", options: ["Stored data", "Data moving across networks", "Disk data", "Backup data"], correct: 1, explanation: "In-transit encryption secures network communication." },
                    { question: "TLS/SSL provides:", options: ["Database access", "Encrypted network communication", "File storage", "Authentication only"], correct: 1, explanation: "TLS encrypts data in transit." },
                    { question: "Symmetric encryption uses:", options: ["Two keys", "One key for encryption and decryption", "No keys", "Public keys only"], correct: 1, explanation: "Symmetric uses the same key for both operations." },
                    { question: "Asymmetric encryption uses:", options: ["One key", "Public and private key pair", "No keys", "Shared secrets"], correct: 1, explanation: "Asymmetric uses key pairs for security." }
                ],
                'medium': [
                    { question: "AES is a:", options: ["Hashing algorithm", "Symmetric encryption standard", "Asymmetric algorithm", "Protocol"], correct: 1, explanation: "AES is the standard symmetric encryption algorithm." },
                    { question: "RSA is used for:", options: ["Symmetric encryption", "Asymmetric encryption and signatures", "Hashing only", "Compression"], correct: 1, explanation: "RSA provides asymmetric encryption." },
                    { question: "Key management services (KMS) provide:", options: ["Nothing", "Secure key storage and lifecycle management", "Only encryption", "Only decryption"], correct: 1, explanation: "KMS handles key operations securely." },
                    { question: "Envelope encryption uses:", options: ["One key", "Data key encrypted by master key", "No keys", "Paper envelopes"], correct: 1, explanation: "Envelope encryption protects keys with master keys." },
                    { question: "Certificate authorities issue:", options: ["Passwords", "Digital certificates for identity verification", "API keys", "Tokens"], correct: 1, explanation: "CAs verify and issue identity certificates." }
                ],
                'hard': [
                    { question: "Perfect forward secrecy ensures:", options: ["All data is decryptable", "Past sessions can't be decrypted if keys are compromised", "No encryption", "Permanent keys"], correct: 1, explanation: "PFS uses ephemeral keys for each session." },
                    { question: "HSM (Hardware Security Module) provides:", options: ["Software encryption", "Tamper-resistant hardware for key operations", "Only storage", "No security"], correct: 1, explanation: "HSMs protect keys in dedicated hardware." },
                    { question: "End-to-end encryption ensures:", options: ["Server can read data", "Only endpoints can decrypt data", "Everyone can decrypt", "No encryption"], correct: 1, explanation: "E2E means only sender and receiver can read data." },
                    { question: "Key rotation is important because:", options: ["Keys never expire", "It limits exposure if a key is compromised", "It's optional", "Keys improve with age"], correct: 1, explanation: "Rotation limits damage from key compromise." },
                    { question: "Homomorphic encryption allows:", options: ["No computation", "Computing on encrypted data without decrypting", "Only storage", "Faster computation"], correct: 1, explanation: "Homomorphic encryption enables secure computation." }
                ],
                'expert': [
                    { question: "Google's data encryption strategy uses:", options: ["Single key", "Multiple layers of keys with automatic rotation", "No encryption", "User-managed only"], correct: 1, explanation: "Google uses layered key hierarchy." },
                    { question: "Field-level encryption protects:", options: ["Nothing", "Specific sensitive fields within documents", "Entire database only", "Only files"], correct: 1, explanation: "Field-level targets specific sensitive data." },
                    { question: "Client-side encryption means:", options: ["Server encrypts", "Data is encrypted before reaching the server", "No encryption", "Automatic encryption"], correct: 1, explanation: "Client-side gives users key control." },
                    { question: "Secrets management (Vault, AWS Secrets Manager) handles:", options: ["Only passwords", "All types of sensitive credentials securely", "Only API keys", "Only certificates"], correct: 1, explanation: "Secrets managers handle various credential types." },
                    { question: "Quantum-resistant cryptography prepares for:", options: ["Current threats", "Future quantum computer attacks on current encryption", "No threats", "Only classical attacks"], correct: 1, explanation: "Post-quantum crypto resists quantum attacks." }
                ]
            },

            // Day 13: DDoS & Security
            'hld_38': {
                'easy': [
                    { question: "DDoS stands for:", options: ["Direct Denial of Service", "Distributed Denial of Service", "Data Denial of Service", "Dynamic Denial of Service"], correct: 1, explanation: "DDoS attacks come from multiple sources." },
                    { question: "DDoS attacks aim to:", options: ["Steal data", "Overwhelm systems making them unavailable", "Encrypt data", "Modify data"], correct: 1, explanation: "DDoS floods systems to cause outages." },
                    { question: "A firewall:", options: ["Starts fires", "Filters network traffic based on rules", "Only monitors", "Only logs"], correct: 1, explanation: "Firewalls control network access." },
                    { question: "WAF (Web Application Firewall) protects against:", options: ["DDoS only", "Application-layer attacks like SQL injection", "Only network attacks", "Physical threats"], correct: 1, explanation: "WAFs filter malicious application traffic." },
                    { question: "CDNs help with DDoS by:", options: ["Nothing", "Absorbing and distributing attack traffic", "Concentrating traffic", "Increasing attacks"], correct: 1, explanation: "CDNs have capacity to absorb attack volume." }
                ],
                'medium': [
                    { question: "Layer 7 DDoS attacks target:", options: ["Network layer", "Application layer with seemingly legitimate requests", "Physical layer", "Transport layer only"], correct: 1, explanation: "Layer 7 attacks mimic real user traffic." },
                    { question: "Rate limiting helps DDoS defense by:", options: ["Nothing", "Limiting requests per client", "Increasing capacity", "Removing limits"], correct: 1, explanation: "Rate limits reduce impact of attack traffic." },
                    { question: "IP reputation filtering blocks:", options: ["All IPs", "Known malicious IP addresses", "Only good IPs", "No IPs"], correct: 1, explanation: "Reputation lists identify known bad actors." },
                    { question: "OWASP Top 10 lists:", options: ["Best practices", "Critical web application security risks", "Programming languages", "Databases"], correct: 1, explanation: "OWASP Top 10 covers major web vulnerabilities." },
                    { question: "SQL injection exploits:", options: ["Memory", "Unsanitized database queries", "Network traffic", "File systems"], correct: 1, explanation: "SQL injection inserts malicious queries." }
                ],
                'hard': [
                    { question: "Anycast routing helps DDoS by:", options: ["Concentrating traffic", "Distributing traffic to nearest data center", "Removing routing", "Slowing traffic"], correct: 1, explanation: "Anycast spreads traffic across locations." },
                    { question: "Scrubbing centers:", options: ["Clean data", "Filter attack traffic before reaching origin", "Create attacks", "Only monitor"], correct: 1, explanation: "Scrubbing separates malicious from legitimate traffic." },
                    { question: "Bot detection distinguishes:", options: ["Nothing", "Automated traffic from human users", "Only errors", "Only successes"], correct: 1, explanation: "Bot detection identifies non-human traffic." },
                    { question: "Geo-blocking can:", options: ["Stop all attacks", "Reduce attack surface by blocking certain regions", "Invite more attacks", "Do nothing"], correct: 1, explanation: "Geo-blocking limits exposure to high-risk regions." },
                    { question: "Security headers (CSP, HSTS) protect against:", options: ["DDoS only", "Various browser-based attacks", "Server attacks only", "Nothing"], correct: 1, explanation: "Security headers prevent client-side attacks." }
                ],
                'expert': [
                    { question: "Cloudflare and AWS Shield provide:", options: ["Only CDN", "DDoS protection at scale", "Only hosting", "Only DNS"], correct: 1, explanation: "These services absorb large-scale DDoS attacks." },
                    { question: "Defense in depth means:", options: ["Single defense", "Multiple layers of security controls", "No defense", "Only perimeter defense"], correct: 1, explanation: "Layered security provides redundant protection." },
                    { question: "Threat modeling identifies:", options: ["Nothing", "Potential security risks and mitigations", "Only known threats", "Only past attacks"], correct: 1, explanation: "Threat modeling proactively finds vulnerabilities." },
                    { question: "Security incident response includes:", options: ["Ignoring incidents", "Detection, containment, eradication, recovery", "Only detection", "Only recovery"], correct: 1, explanation: "Response is a structured process." },
                    { question: "Zero-day vulnerabilities are:", options: ["Old bugs", "Unknown flaws without patches", "Fixed issues", "Minor bugs"], correct: 1, explanation: "Zero-days are exploited before patches exist." }
                ]
            },

            // Day 14: REST Best Practices
            'hld_39': {
                'easy': [
                    { question: "REST stands for:", options: ["Remote State Transfer", "Representational State Transfer", "Request State Transfer", "Resource State Transfer"], correct: 1, explanation: "REST is an architectural style for web services." },
                    { question: "RESTful APIs use HTTP methods like:", options: ["Only GET", "GET, POST, PUT, DELETE, etc.", "Only POST", "Custom methods"], correct: 1, explanation: "REST uses standard HTTP verbs for operations." },
                    { question: "GET requests should be:", options: ["Modifying", "Safe and idempotent (no side effects)", "Dangerous", "Random"], correct: 1, explanation: "GET should only retrieve data." },
                    { question: "POST is typically used to:", options: ["Read data", "Create new resources", "Delete data", "Nothing"], correct: 1, explanation: "POST creates new resources on the server." },
                    { question: "HTTP status codes communicate:", options: ["Nothing", "Success, errors, and other outcomes", "Only success", "Only errors"], correct: 1, explanation: "Status codes indicate request results." }
                ],
                'medium': [
                    { question: "Idempotent methods (GET, PUT, DELETE) can be:", options: ["Called once only", "Called multiple times with same result", "Never retried", "Always different"], correct: 1, explanation: "Idempotent operations are safe to retry." },
                    { question: "HATEOAS in REST provides:", options: ["Static links", "Hypermedia links for API discoverability", "No links", "Only documentation"], correct: 1, explanation: "HATEOAS enables dynamic API navigation." },
                    { question: "API versioning strategies include:", options: ["No versioning", "URL path, query param, or header versioning", "Only breaking changes", "Random versions"], correct: 1, explanation: "Versioning manages API evolution." },
                    { question: "Pagination for large results uses:", options: ["No pagination", "Limit/offset or cursor-based approaches", "Loading everything", "Random selection"], correct: 1, explanation: "Pagination handles large result sets efficiently." },
                    { question: "Content negotiation allows:", options: ["No negotiation", "Clients to request specific response formats", "Only JSON", "Only XML"], correct: 1, explanation: "Content negotiation supports multiple formats." }
                ],
                'hard': [
                    { question: "PUT vs PATCH:", options: ["Same thing", "PUT replaces entire resource; PATCH updates partially", "PATCH replaces", "PUT is partial"], correct: 1, explanation: "PUT is full replacement; PATCH is partial update." },
                    { question: "ETag headers enable:", options: ["Nothing", "Conditional requests and caching validation", "Only caching", "Only validation"], correct: 1, explanation: "ETags support cache validation and optimistic concurrency." },
                    { question: "Rate limiting headers should include:", options: ["Nothing", "Limit, remaining, and reset time", "Only errors", "Only limits"], correct: 1, explanation: "Headers help clients manage request rate." },
                    { question: "Consistent error responses should include:", options: ["Only status code", "Status code, error code, message, and details", "Only message", "Nothing specific"], correct: 1, explanation: "Structured errors help clients handle failures." },
                    { question: "Resource naming should use:", options: ["Verbs", "Nouns (collections/resources)", "Random names", "Only IDs"], correct: 1, explanation: "REST uses nouns; HTTP verbs indicate actions." }
                ],
                'expert': [
                    { question: "Richardson Maturity Model levels measure:", options: ["API speed", "How RESTful an API is", "API size", "Error rates"], correct: 1, explanation: "RMM defines REST maturity from 0-3." },
                    { question: "JSON:API and HAL are:", options: ["Programming languages", "REST response format specifications", "Databases", "Protocols"], correct: 1, explanation: "They standardize REST response structures." },
                    { question: "Idempotency keys enable:", options: ["Nothing", "Safe retries for non-idempotent operations", "Faster requests", "Smaller responses"], correct: 1, explanation: "Idempotency keys make POSTs retriable." },
                    { question: "Backward compatibility in APIs means:", options: ["Breaking clients", "Changes don't break existing clients", "No changes", "Only additions"], correct: 1, explanation: "Backward compatibility preserves client functionality." },
                    { question: "API gateways can transform REST to:", options: ["Nothing", "gRPC, GraphQL, or other protocols", "Only REST", "Only HTTP"], correct: 1, explanation: "Gateways can translate between API protocols." }
                ]
            },

            // Day 14: GraphQL vs REST
            'hld_40': {
                'easy': [
                    { question: "GraphQL is:", options: ["A database", "A query language for APIs", "A programming language", "A REST replacement"], correct: 1, explanation: "GraphQL lets clients query exactly what they need." },
                    { question: "GraphQL uses a single endpoint because:", options: ["Limitation", "Clients specify what data they want in queries", "Performance", "Simplicity only"], correct: 1, explanation: "GraphQL's flexibility eliminates multiple endpoints." },
                    { question: "Over-fetching in REST means:", options: ["Too few fields", "Getting more data than needed", "No data", "Perfect data"], correct: 1, explanation: "REST often returns extra unnecessary fields." },
                    { question: "Under-fetching in REST requires:", options: ["One request", "Multiple requests for related data", "No requests", "Faster requests"], correct: 1, explanation: "Getting related data often needs multiple REST calls." },
                    { question: "GraphQL schema defines:", options: ["Nothing", "Types and operations available", "Only queries", "Only mutations"], correct: 1, explanation: "Schema is the contract for GraphQL APIs." }
                ],
                'medium': [
                    { question: "GraphQL mutations are for:", options: ["Reading data", "Creating, updating, or deleting data", "Only schema", "Only queries"], correct: 1, explanation: "Mutations modify data in GraphQL." },
                    { question: "GraphQL subscriptions enable:", options: ["Nothing special", "Real-time updates via WebSocket", "Only polling", "Only REST"], correct: 1, explanation: "Subscriptions provide push-based updates." },
                    { question: "REST is better when:", options: ["Never", "Simple CRUD, caching important, or team familiarity", "Always", "Only for reads"], correct: 1, explanation: "REST has advantages in certain scenarios." },
                    { question: "GraphQL is better when:", options: ["Never", "Complex data needs, multiple clients, or reducing requests", "Always", "Only for writes"], correct: 1, explanation: "GraphQL excels with complex client requirements." },
                    { question: "GraphQL N+1 problem occurs when:", options: ["Never", "Resolvers make separate database calls for each item", "Always optimal", "Only with REST"], correct: 1, explanation: "DataLoader helps batch and cache to prevent N+1." }
                ],
                'hard': [
                    { question: "Apollo and Relay are:", options: ["Databases", "GraphQL client libraries", "REST frameworks", "Languages"], correct: 1, explanation: "Apollo and Relay are popular GraphQL clients." },
                    { question: "GraphQL depth limiting prevents:", options: ["Deep queries", "Malicious deeply nested queries", "Shallow queries", "All queries"], correct: 1, explanation: "Depth limits prevent expensive nested queries." },
                    { question: "Query complexity analysis in GraphQL:", options: ["Is not needed", "Assigns costs to prevent expensive queries", "Always blocks", "Only allows simple"], correct: 1, explanation: "Complexity limits protect against resource abuse." },
                    { question: "Persisted queries improve GraphQL by:", options: ["Making it slower", "Reducing request size and preventing arbitrary queries", "Allowing any query", "Removing security"], correct: 1, explanation: "Persisted queries are pre-approved query strings." },
                    { question: "Federation in GraphQL enables:", options: ["Monolithic graph", "Composing multiple GraphQL services into one", "Only single service", "No composition"], correct: 1, explanation: "Federation distributes schema across services." }
                ],
                'expert': [
                    { question: "GitHub chose GraphQL because:", options: ["Trend following", "Complex data needs of their integrations", "REST didn't exist", "Random choice"], correct: 1, explanation: "GitHub's diverse API needs suited GraphQL." },
                    { question: "Netflix uses GraphQL Federation for:", options: ["Simple APIs", "Unifying hundreds of microservices", "Only client apps", "No special reason"], correct: 1, explanation: "Federation helps Netflix manage API complexity." },
                    { question: "GraphQL vs REST for mobile:", options: ["REST always better", "GraphQL reduces over-fetching important on mobile", "No difference", "Mobile doesn't matter"], correct: 1, explanation: "GraphQL's precision helps bandwidth-constrained mobile." },
                    { question: "Schema stitching vs federation:", options: ["Same thing", "Stitching is older; federation is more scalable", "Federation is older", "Neither works"], correct: 1, explanation: "Federation is the modern approach to distributed GraphQL." },
                    { question: "Migrating REST to GraphQL typically:", options: ["Replaces immediately", "Starts with GraphQL layer over existing REST", "Deletes REST", "Requires rewrite"], correct: 1, explanation: "GraphQL can wrap REST during migration." }
                ]
            },

            // Day 15: URL Shortener Case Study
            'hld_41': {
                'easy': [
                    { question: "URL shorteners convert:", options: ["Nothing", "Long URLs to short, memorable links", "Short to long", "Only domains"], correct: 1, explanation: "Shorteners create brief aliases for long URLs." },
                    { question: "Short codes like 'abc123' are:", options: ["Random only", "Unique identifiers mapping to original URLs", "Same for all", "Not unique"], correct: 1, explanation: "Short codes uniquely identify each URL." },
                    { question: "Redirection (301/302) sends users:", options: ["Nowhere", "From short URL to original long URL", "To errors", "To random sites"], correct: 1, explanation: "Shorteners redirect to the original destination." },
                    { question: "Analytics in URL shorteners track:", options: ["Nothing", "Clicks, locations, referrers, etc.", "Only errors", "Only URLs"], correct: 1, explanation: "Analytics provide insights on link usage." },
                    { question: "Base62 encoding uses:", options: ["Only numbers", "Letters and numbers (a-z, A-Z, 0-9)", "Only letters", "Special characters"], correct: 1, explanation: "Base62 creates readable short codes." }
                ],
                'medium': [
                    { question: "Short code collision happens when:", options: ["Never", "Two URLs get the same short code", "Codes are too long", "Codes are too short"], correct: 1, explanation: "Collisions must be detected and handled." },
                    { question: "Counter-based vs hash-based short codes:", options: ["Same thing", "Counter is sequential; hash is distributed", "Hash is sequential", "Neither works"], correct: 1, explanation: "Each approach has tradeoffs for uniqueness." },
                    { question: "Read vs write ratio for URL shorteners:", options: ["Equal", "Read-heavy (many redirects, few creates)", "Write-heavy", "Unpredictable"], correct: 1, explanation: "URLs are created once but accessed many times." },
                    { question: "Custom short codes require:", options: ["No checking", "Availability checking before creation", "Random assignment", "Only system codes"], correct: 1, explanation: "Custom codes need uniqueness validation." },
                    { question: "Caching benefits URL shorteners because:", options: ["No benefit", "Popular links can be served from memory", "All links are equal", "Caching doesn't work"], correct: 1, explanation: "Hot links benefit greatly from caching." }
                ],
                'hard': [
                    { question: "Consistent hashing in URL shorteners helps:", options: ["Nothing", "Distribute URLs across database shards", "Only caching", "Only reads"], correct: 1, explanation: "Consistent hashing enables horizontal scaling." },
                    { question: "URL expiration requires:", options: ["No special handling", "TTL tracking and cleanup processes", "Immediate deletion", "No expiration"], correct: 1, explanation: "Expired URLs need cleanup mechanisms." },
                    { question: "Rate limiting for URL creation prevents:", options: ["Nothing", "Abuse and spam link creation", "Legitimate use", "All creation"], correct: 1, explanation: "Rate limits prevent malicious use." },
                    { question: "High availability for redirects requires:", options: ["Single server", "Multiple replicas and geographic distribution", "Only backups", "No special design"], correct: 1, explanation: "Redirects must be highly available." },
                    { question: "Malware/spam detection in shorteners:", options: ["Is optional", "Scans destinations for safety", "Is not possible", "Only manual"], correct: 1, explanation: "Shorteners should protect users from malicious links." }
                ],
                'expert': [
                    { question: "bit.ly handles billions of redirects by:", options: ["Single server", "Distributed caching, CDN, and database sharding", "Only caching", "Only CDN"], correct: 1, explanation: "bit.ly uses multiple scaling techniques." },
                    { question: "Snowflake IDs help URL shorteners with:", options: ["Nothing", "Unique, sortable, distributed ID generation", "Only sorting", "Only uniqueness"], correct: 1, explanation: "Snowflake generates unique IDs across servers." },
                    { question: "301 vs 302 redirects affect:", options: ["Nothing", "SEO and caching behavior", "Only speed", "Only security"], correct: 1, explanation: "301 is permanent (cached); 302 is temporary." },
                    { question: "URL shortener database design often uses:", options: ["Only SQL", "NoSQL for high write throughput and simple lookups", "Only graph DB", "No database"], correct: 1, explanation: "Key-value stores suit the access patterns well." },
                    { question: "Preview/safety features show users:", options: ["Nothing", "Destination URL before redirect", "Only errors", "Only analytics"], correct: 1, explanation: "Preview helps users verify link safety." }
                ]
            },

            // Day 15: Chat System Case Study
            'hld_42': {
                'easy': [
                    { question: "Chat systems require:", options: ["Batch processing", "Real-time message delivery", "Daily updates", "Weekly sync"], correct: 1, explanation: "Chat needs instant message delivery." },
                    { question: "WebSockets enable:", options: ["One-way communication", "Persistent bidirectional connections", "Only HTTP", "Only polling"], correct: 1, explanation: "WebSockets maintain open connections for real-time chat." },
                    { question: "Message persistence stores:", options: ["Nothing", "Chat history for later retrieval", "Only current messages", "Only metadata"], correct: 1, explanation: "Persistence enables chat history and offline access." },
                    { question: "Online presence shows:", options: ["Nothing", "Which users are currently active", "Only offline users", "Only message counts"], correct: 1, explanation: "Presence indicates user availability." },
                    { question: "Group chats require:", options: ["Same as 1:1", "Fan-out to multiple recipients", "Only two users", "No special handling"], correct: 1, explanation: "Group messages must reach all members." }
                ],
                'medium': [
                    { question: "Message ordering in chat ensures:", options: ["Random order", "Messages appear in correct sequence", "No ordering", "Only by time"], correct: 1, explanation: "Chat must maintain conversation order." },
                    { question: "Push notifications for offline users:", options: ["Are not needed", "Deliver message alerts when app is closed", "Only when online", "Never work"], correct: 1, explanation: "Push notifies users of new messages." },
                    { question: "Read receipts track:", options: ["Nothing", "Whether recipients have seen messages", "Only delivery", "Only sending"], correct: 1, explanation: "Read receipts show message acknowledgment." },
                    { question: "Message queues in chat help:", options: ["Nothing", "Buffer messages during delivery", "Only storage", "Only retrieval"], correct: 1, explanation: "Queues handle asynchronous message delivery." },
                    { question: "Connection management handles:", options: ["Nothing", "User connect/disconnect and reconnection", "Only connects", "Only disconnects"], correct: 1, explanation: "Managing connections is crucial for reliability." }
                ],
                'hard': [
                    { question: "WhatsApp's architecture uses:", options: ["Only servers", "Erlang for concurrent connections", "Only Java", "Only Python"], correct: 1, explanation: "WhatsApp uses Erlang for massive concurrency." },
                    { question: "Sharding chat data by:", options: ["Random", "User or conversation ID for locality", "Only time", "Only size"], correct: 1, explanation: "Sharding strategy affects query performance." },
                    { question: "End-to-end encryption in chat means:", options: ["Server can read", "Only participants can decrypt messages", "No encryption", "Server encrypts"], correct: 1, explanation: "E2E prevents even service providers from reading." },
                    { question: "Typing indicators require:", options: ["No real-time", "Real-time lightweight status updates", "Only WebSocket", "Heavy processing"], correct: 1, explanation: "Typing status needs fast, ephemeral updates." },
                    { question: "Message search at scale requires:", options: ["Full table scan", "Inverted index or search engine", "No indexing", "Only recent messages"], correct: 1, explanation: "Search needs efficient indexing for performance." }
                ],
                'expert': [
                    { question: "Slack handles millions of messages using:", options: ["Single database", "Sharded data, message queues, and caching", "Only caching", "Only queues"], correct: 1, explanation: "Slack combines multiple scaling approaches." },
                    { question: "Discord supports millions of concurrent users with:", options: ["Traditional architecture", "Elixir, Cassandra, and careful connection management", "Only WebSocket", "Only caching"], correct: 1, explanation: "Discord uses specialized tech stack for scale." },
                    { question: "Fanout on write vs read for group chat:", options: ["Same thing", "Write copies to all; read assembles from source", "Write is always better", "Read is always better"], correct: 1, explanation: "Tradeoff between write cost and read latency." },
                    { question: "Message delivery guarantees (at-least-once):", options: ["Lose messages", "May duplicate but won't lose messages", "Exactly once always", "No guarantees"], correct: 1, explanation: "At-least-once prioritizes delivery over deduplication." },
                    { question: "Horizontal scaling chat servers requires:", options: ["Sticky sessions only", "Session state sharing or stateless design", "No special design", "Vertical scaling only"], correct: 1, explanation: "Scaling needs session management strategy." }
                ]
            }
        };

        // ==================== LEARNING PATH STRUCTURE ====================
        const LEARNING_PATHS = {
            hld: {
                id: 'hld',
                title: 'High-Level System Design Master',
                description: 'Master system design from fundamentals to advanced concepts in 15 days',
                icon: 'üèóÔ∏è',
                estimatedDays: 15,
                concepts: [
                    // Day 1: Fundamentals
                    { day: 1, title: 'Introduction to System Design', prompt: `Create an engaging, beginner-friendly introduction to system design that:

1. OPENS WITH A RELATABLE STORY: Start with a real scenario where poor system design failed (like Instagram's early growth challenges, Twitter's fail whale moments, or Facebook's scaling crisis). Help learners understand WHY system design matters through narrative, not theory.

2. PROVIDES A SIMPLE ANALOGY: Compare system design to something familiar (city planning, restaurant management, a library, etc.) to make abstract concepts concrete.

3. COVERS THE FUNDAMENTALS:
   - What is system design (definition through real examples, not textbook)
   - Why it matters (connect to real-world impact: speed, reliability, user experience)
   - Key objectives (scalability, reliability, performance) with brief real examples for each

4. INCLUDES A PRACTICAL EXAMPLE: Show a concrete system (Instagram feed, e-commerce checkout, video streaming) and explain how system design principles apply.

5. HIGHLIGHTS KEY CONCEPTS: Use **{bold braces}** to emphasize important terms that readers should remember.

6. ENDS WITH ACTIONABLE INSIGHTS: A tip about approaching system design and a warning about common beginner mistakes.

Structure your response with clear sections so the output renders as:
- Opening narrative story (one paragraph, compelling)
- Analogy explanation (one paragraph, relatable)
- Key concept definitions (bulleted list)
- Practical example with code/pseudocode
- Actionable tip
- Common pitfall warning
- Connection to real systems (Netflix, Google, etc.)

Make it engaging and memorable - someone should WANT to learn more after reading this.` },
                    
                    { day: 1, title: 'Requirements Gathering & Constraints', prompt: `Create an engaging lesson on requirements gathering that:

1. STARTS WITH CONSEQUENCES: Show a real scenario where unclear requirements caused problems (Netflix's 4K rollout complexity, Uber's geographic constraints, or infrastructure scaling miscalculations).

2. EXPLAINS THE DISTINCTION: Help readers understand the difference between what a system DOES (functional) vs HOW WELL it does it (non-functional), with concrete examples they can relate to.

3. COVERS CORE CONCEPTS:
   - Functional requirements (features users see)
   - Non-functional requirements (speed, reliability, capacity)
   - Constraints (budget, regulatory, infrastructure limits)

4. PROVIDES ACTIONABLE FRAMEWORK: Show how to systematically gather requirements (clarifying questions, estimation, documentation).

5. INCLUDES WORKED EXAMPLES: Walk through requirement gathering for a real system (URL shortener, chat app, streaming service).

6. HIGHLIGHTS COMMON MISTAKES: What happens when requirements aren't clear or are underestimated.

7. CONNECTS TO REAL SYSTEMS: Show how actual companies handle this phase.

Make it practical‚Äîsomeone should feel confident asking the right questions in their next design interview.` },

                    { day: 1, title: 'Back-of-Envelope Calculations', prompt: `Create an engaging guide to estimation that shows readers WHY accurate math matters:

1. OPENS WITH REAL STAKES: YouTube storing petabytes, Google's trillion queries‚Äîshow why math errors compound catastrophically.

2. TEACHES THE FUNDAMENTALS: QPS, storage, bandwidth, memory estimation through relatable scenarios.

3. PROVIDES WORKED EXAMPLES: Step through actual calculations (DAU to QPS conversions, storage calculations, bandwidth estimates) so readers can replicate.

4. EXPLAINS THE PATTERNS: Key conversions (seconds per day, replication factors, cache hit rates) and when to apply them.

5. INCLUDES SANITY CHECKS: How to validate estimates (cross-check with real systems, identify obviously wrong results).

6. HIGHLIGHTS COMMON ERRORS: Off-by-magnitude mistakes, ignoring replication, underestimating growth.

7. SHOWS REAL SYSTEM EXAMPLES: Netflix, LinkedIn, Twitter‚Äîhow they estimated when starting.

Make it hands-on‚Äîsomeone should confidently do estimation in an interview or design document.` },

                    // Day 2: Scalability Basics
                    { day: 2, title: 'Vertical vs Horizontal Scaling', prompt: `Teach scaling strategies by showing real consequences:

1. START WITH NETFLIX'S JOURNEY: Show how early vertical scaling hit hard limits, forcing architectural shift to horizontal‚Äîthis transformation matters.

2. EXPLAIN THE TRADE-OFF CLEARLY:
   - Vertical: Simplicity, single-point-of-failure, hardware ceiling
   - Horizontal: Complexity, resilience, unlimited scale

3. WALK THROUGH LIMITS: Real numbers on CPU cores, RAM, cost curves‚Äîhelp readers visualize why hitting a ceiling is inevitable.

4. PROVIDE ARCHITECTURE EXAMPLES: Monolith (vertical) vs distributed system (horizontal) with concrete differences.

5. DISCUSS OPERATIONAL IMPACT: How scaling strategies affect team size, monitoring, deployment.

6. SHOW REAL SYSTEMS: Amazon, Google, Netflix‚Äîall exclusively horizontal at scale.

Make readers understand that this isn't just a technical choice‚Äîit's an architectural philosophy that changes everything.` },

                    { day: 2, title: 'Load Balancers Deep Dive', prompt: `Create an engaging exploration of load balancing that explains distribution strategies:

1. OPENS WITH FAILURE SCENARIO: Twitter during Super Bowl‚Äîtraffic spikes 10x, unbalanced load causes outages.

2. EXPLAINS DISTRIBUTION ALGORITHMS: Round-robin, least connections, consistent hashing‚Äîwhat each does and when to use it.

3. INCLUDES WORKED EXAMPLES: Show requests flowing to different servers, how algorithm choices affect outcomes.

4. DISCUSSES TRADE-OFFS: L4 vs L7 load balancers, latency implications, complexity.

5. ADDRESSES FAILURE MODES: What happens when load balancer itself fails? Active-passive, multiple redundancy.

6. SHOWS REAL IMPLEMENTATIONS: AWS ALB, Nginx, HAProxy‚Äîhow they differ.

Help readers understand that load balancing is both science (algorithms) and art (configuration), and getting it right is crucial.` },

                    { day: 2, title: 'Stateless vs Stateful Architecture', prompt: `Explain architectural paradigms by showing their implications:

1. OPENS WITH A PROBLEM: Early e-commerce sites where users couldn't switch servers, sessions were lost‚Äîwhy this mattered operationally.

2. DEFINES THE FUNDAMENTAL DIFFERENCE: Stateless servers don't need memory of past interactions; stateful servers do.

3. EXPLAINS CASCADING EFFECTS:
   - Stateless: Horizontal scaling becomes trivial
   - Stateful: Sticky sessions, affinity, complexity
   - Database state: Moves problem to persistent layer

4. SHOWS IMPLEMENTATION PATTERNS: JWT tokens, session stores (Redis), how requests flow differently.

5. DISCUSSES TRADE-OFFS: Simplicity vs scalability, latency implications.

6. PROVIDES REAL EXAMPLES: REST APIs (stateless), WebSockets (stateful), hybrid approaches.

Help readers see this isn't about purity‚Äîit's about choosing the right tool for each layer.` },

                    // Day 3: Networking & Communication
                    { day: 3, title: 'HTTP, REST, and API Design', prompt: `Create a comprehensive guide to building scalable APIs:

1. OPENS WITH CHAOS NARRATIVE: Early pre-REST APIs, confusion, conventions emerging, Stripe bringing order.

2. TEACHES REST PRINCIPLES: Resources, HTTP methods (GET/POST/PUT/DELETE), stateless design, cacheability‚Äîthrough examples, not definitions.

3. INCLUDES DESIGN PATTERNS:
   - Resource naming conventions
   - Status code meanings
   - Pagination strategies
   - Error representation

4. PROVIDES WORKED EXAMPLE: Trace a real API (user CRUD operations) with proper REST design.

5. DISCUSSES EVOLUTION & VERSIONING: How companies handle API changes without breaking clients.

6. COMPARES ALTERNATIVES: REST vs GraphQL, SOAP‚Äîwhy REST won for most use cases.

7. SHOWS PRODUCTION REALITY: Stripe, GitHub, Twitter‚Äîexemplary API design.

Make readers understand REST as both craft and science‚Äîthere's right and wrong, and the difference matters.` },

                    { day: 3, title: 'WebSockets and Real-Time Communication', prompt: `Teach real-time architectures by explaining the fundamental tension:

1. OPENS WITH LIMITATION STORY: HTTP polling's waste, Facebook's real-time chat requirements, why persistent connections were invented.

2. EXPLAINS COMMUNICATION MODELS:
   - HTTP polling (wasteful)
   - WebSockets (persistent bidirectional)
   - Server-Sent Events (server push only)
   - Long polling (compromise)

3. WALKS THROUGH WEBSOCKET MECHANICS: Handshake, frame format, connection management‚Äîmake it concrete.

4. ADDRESSES SCALING CHALLENGES: Connection pooling, horizontal scaling, session persistence.

5. INCLUDES FAILURE MODES: Network disconnects, reconnection logic, message ordering guarantees.

6. SHOWS REAL IMPLEMENTATIONS: WhatsApp, Discord, live dashboards‚Äîhow they handle millions of concurrent connections.

7. DISCUSSES TRADE-OFFS: Complexity vs real-time experience.

Help readers see that real-time systems are both beautiful and complex.` },

                    { day: 3, title: 'RPC and gRPC', prompt: `Explain service-to-service communication evolution:

1. OPENS WITH MICROSERVICES CONTEXT: Services need to talk. HTTP/REST is convenient but verbose. Why gRPC?

2. EXPLAINS RPC CONCEPT: Remote Procedure Call‚Äîmaking service calls feel like local calls.

3. INTRODUCES gRPC ADVANTAGES:
   - Performance (binary protocol, HTTP/2 multiplexing)
   - Type safety (Protocol Buffers)
   - Bidirectional streaming
   - Language agnostic

4. PROVIDES PROTO DEFINITION EXAMPLES: Show how to define services and messages.

5. DISCUSSES TRADE-OFFS: Simplicity of REST vs efficiency of gRPC. When each is appropriate.

6. SHOWS REAL USAGE: Netflix, Uber, Shopify‚Äîwhere gRPC is the standard.

7. ADDRESSES OPERATIONAL ASPECTS: Monitoring, debugging, debugging binary protocols.

Help readers see gRPC not as a replacement for REST, but as a tool for specific problems.` },

                    // Day 4: Databases & Storage I
                    { day: 4, title: 'SQL vs NoSQL Databases', prompt: `Explore the database choice that impacts entire systems:

1. OPENS WITH CONTEXT: The NoSQL hype era, "SQL is dead"‚Äîreality is nuanced. Both exist. Why?

2. EXPLAINS FUNDAMENTAL DIFFERENCES: Schemas, consistency models, query flexibility, scaling approach.

3. TEACHES THE TRADE-OFF MATRIX:
   - SQL: ACID, joins, complex queries, scaling limits
   - NoSQL: Flexible schema, eventual consistency, horizontal scale, limited query patterns

4. SHOWS DECISION FRAMEWORK: Requirements determine choice‚Äînot dogma.

5. INCLUDES WORKED EXAMPLES: Financial transactions (SQL), user profiles (NoSQL), hybrid approaches.

6. DISCUSSES OPERATIONAL DIFFERENCES: Backups, scaling, monitoring differ significantly.

7. SHOWS REAL SYSTEMS: Google uses BigTable (NoSQL) for scale, Spanner (SQL) for consistency. Both.

Help readers see databases as tools for different jobs, not religious wars.` },

                    { day: 4, title: 'Database Sharding Strategies', prompt: `Teach partitioning for massive scale:

1. OPENS WITH PAIN: Twitter's growth‚Äîsingle database hit limits. Sharding was forced, not optional.

2. EXPLAINS SHARDING: Partitioning data across independent databases, choosing what data goes where.

3. TEACHES SHARD KEY SELECTION:
   - Hash-based: Uniform distribution
   - Range-based: Easier migration, hot shards risk
   - Directory-based: Flexible, requires lookup
   - Choosing the right key makes or breaks the system

4. WALKS THROUGH TRADE-OFFS:
   - Pros: Unlimited scale, parallel processing
   - Cons: No joins, distributed transactions, rebalancing complexity

5. ADDRESSES OPERATIONAL CONCERNS: Resharding as data grows, monitoring shard imbalance.

6. SHOWS REAL IMPLEMENTATIONS: Instagram, Facebook‚Äîhow they shard and why.

7. DISCUSSES ANTI-PATTERNS: Hot shards, uneven distribution, poor shard key choices.

Help readers understand sharding isn't just technical‚Äîit shapes application architecture fundamentally.` },

                    { day: 4, title: 'Database Replication Patterns', prompt: `Understand high availability through replication:

1. OPENS WITH FAILURE: Single database down = entire system down. Replication prevents this.

2. TEACHES REPLICATION PATTERNS:
   - Master-Slave (standard): One writes, many read
   - Master-Master (complex): Both write, conflict resolution needed
   - Chain/Tree: Reduces master load

3. EXPLAINS CONSISTENCY IMPLICATIONS:
   - Synchronous: Slow but consistent
   - Asynchronous: Fast but lag possible (temporary inconsistency)

4. WALKS THROUGH FAILOVER: Master fails ‚Üí promote replica ‚Üí coordinate, clients reconnect.

5. DISCUSSES SCALING READ CAPACITY: Replicas enable read scale‚Äîcritical for read-heavy workloads.

6. SHOWS REAL IMPLEMENTATIONS: MySQL replication, MongoDB replica sets, PostgreSQL streaming.

7. ADDRESSES OPERATIONAL CHALLENGES: Monitoring lag, handling failover, backup strategies.

Help readers see replication as foundation of reliability, not optional feature.` },

                    // Day 5: Databases & Storage II
                    { day: 5, title: 'Indexing and Query Optimization', prompt: `Master database performance through strategic indexing:

1. OPENS WITH PERFORMANCE CLIFF: Query on 1B rows‚Äî10 seconds without index, 10ms with one. Why?

2. EXPLAINS INDEX MECHANICS: Data structures (B-Tree, hash, full-text), trade-offs.

3. TEACHES INDEXING STRATEGY:
   - Index columns in WHERE clauses
   - Consider ORDER BY, GROUP BY columns
   - Balance read speed vs write cost
   - Too many indexes slow writes significantly

4. INCLUDES WORKED EXAMPLES: Query analysis, execution plans, understanding why index helps (or doesn't).

5. DISCUSSES ADVANCED PATTERNS: Composite indexes, partial indexes, covering indexes.

6. SHOWS OPERATIONAL MONITORING: Query logs, slow query analysis, identifying missing indexes.

7. ADDRESSES PITFALLS: Over-indexing, index fragmentation, monitoring index effectiveness.

Help readers see database performance as result of thoughtful index design, not luck.` },

                    { day: 5, title: 'Object Storage and Blob Storage', prompt: `Understand storage for unstructured data at massive scale:

1. OPENS WITH INSTAGRAM'S PROBLEM: Billions of images‚Äîcan't store in database. What's the solution?

2. EXPLAINS OBJECT STORAGE CONCEPT: Key-value store for arbitrary data, unlimited capacity, geographic distribution.

3. TEACHES OPERATIONAL CHARACTERISTICS:
   - Unlimited scale
   - Durability (replicated across datacenters)
   - Geographic distribution
   - Simple API (PUT, GET, DELETE)

4. ADDRESSES CONCERNS:
   - Latency (slower than databases, mitigated by CDN)
   - Costs (cheap storage, expensive egress)
   - Lifecycle policies (archive old data)
   - Versioning and retention

5. SHOWS CDN INTEGRATION: Object storage as origin, CDN edge caching, user experience optimization.

6. DISCUSSES REAL IMPLEMENTATIONS: AWS S3 (the standard), GCS, Azure Blob, on-premises MinIO.

7. INCLUDES OPERATIONAL TOPICS: Access control, encryption, compliance, monitoring.

Help readers see object storage as critical infrastructure, not just a place to throw files.` },

                    { day: 5, title: 'Time-Series Databases', prompt: `Specialize in metrics, monitoring, and time-ordered data:

1. OPENS WITH SCALE PROBLEM: Prometheus collecting millions of metrics/second‚Äîregular databases can't handle this write rate.

2. EXPLAINS TIME-SERIES CHARACTERISTICS:
   - Append-only (immutable history)
   - Time-ordered
   - Massive write volume
   - Time-decay value (old data less important)

3. TEACHES SPECIALIZED OPTIMIZATIONS:
   - Compression (timestamps differ predictably)
   - Downsampling (keep 1-min data for 1 week, 1-hour for 1 year)
   - Retention policies
   - Efficient aggregation (sums, averages over time)

4. WALKS THROUGH REAL USE CASES: System monitoring, application metrics, financial tick data, IoT sensors.

5. SHOWS QUERY PATTERNS: Rate calculations, anomaly detection, trend analysis.

6. INCLUDES OPERATIONAL ASPECTS: Storage planning, retention strategy, cost optimization.

7. DISCUSSES TOOLS: Prometheus, InfluxDB, TimescaleDB, CloudWatch‚Äîeach with trade-offs.

Help readers see time-series databases not as exotic, but essential for production observability.` },

                    // Day 6: Caching Strategies
                    { day: 6, title: 'Caching Fundamentals and Layers', prompt: `Master the multiplicative effect of layered caching:

1. OPENS WITH LATENCY COMPARISON: Database queries (100ms) vs cache hits (1ms)‚Äî1000x difference.

2. EXPLAINS CACHE LAYERS:
   - Browser cache (client-side)
   - CDN cache (geographic distribution)
   - Application cache (in-process, Redis)
   - Database cache (query results)
   - Layering these multiplies benefits

3. TEACHES CACHING PATTERNS:
   - Cache-Aside (check cache, miss = fetch + cache)
   - Read-Through (cache responsible for loading)
   - Write-Through (write both places)
   - Write-Behind (write cache, async to storage)

4. DISCUSSES CONSISTENCY CHALLENGES: Cache invalidation, TTLs, consistency guarantees.

5. SHOWS IMPLEMENTATION STRATEGIES: When each pattern makes sense, trade-offs.

6. ADDRESSES MONITORING: Cache hit rates, eviction patterns, effectiveness analysis.

7. INCLUDES PRODUCTION LESSONS: Facebook, Netflix‚Äîhow they stack caches at every layer.

Help readers understand caching as multiplier on system performance, not afterthought.` },

                    { day: 6, title: 'Distributed Caching with Redis/Memcached', prompt: `Scale caching beyond single machine limits:

1. OPENS WITH SCALE PROBLEM: Single cache server is bottleneck. How to distribute?

2. EXPLAINS REDIS vs MEMCACHED:
   - Redis: Data structures, persistence, pub/sub, persistence, Lua scripting
   - Memcached: Simple key-value, pure speed, distributed responsibility

3. TEACHES DISTRIBUTED ARCHITECTURE: Consistent hashing, replication, failure handling.

4. WALKS THROUGH USE CASES:
   - Session storage
   - Rate limiting
   - Leaderboards (sorted sets)
   - Pub/sub messaging
   - Cache-aside pattern implementation

5. DISCUSSES OPERATIONAL CONCERNS: Eviction policies, memory limits, monitoring.

6. SHOWS REAL DEPLOYMENTS: How Uber, Airbnb, Pinterest use Redis at massive scale.

7. ADDRESSES RELIABILITY: Replication, failover, persistence options.

Help readers see Redis as foundational infrastructure, not just caching layer.` },

                    { day: 6, title: 'Cache Eviction Policies and Strategies', prompt: `Manage limited cache memory intelligently:

1. OPENS WITH CONSTRAINT: Cache has finite memory. Data keeps arriving. What do you remove?

2. EXPLAINS EVICTION POLICIES:
   - LRU (Least Recently Used): Most common, works for most patterns
   - LFU (Least Frequently Used): Better for skewed access patterns
   - FIFO/Random: Simpler, acceptable in some cases
   - TTL-based: Explicit expiration

3. TEACHES POLICY SELECTION: Access patterns determine optimal strategy.

4. WALKS THROUGH SCENARIOS:
   - Uniform access (LRU sufficient)
   - Temporal locality (LRU excellent)
   - Zipfian distribution (LFU better)

5. DISCUSSES MONITORING: Eviction rates, cache effectiveness, identifying bad choices.

6. SHOWS CONFIGURATION: Redis, Memcached‚Äîhow to set policies.

7. INCLUDES PITFALL: Over-eviction indicates cache misalignment, fix cache size or strategy, not symptoms.

Help readers see eviction as indication of cache sizing/design issues, solve root cause.` },

                    // Day 7: Advanced Load Balancing
                    { day: 7, title: 'Consistent Hashing', prompt: `Solve cache rebalancing elegantly through clever math:

1. OPENS WITH PROBLEM: Add 1 server to 10-server cache cluster‚Äîwith modulo hashing, 90% of keys rehash to wrong servers. Cache invalidation cascade.

2. EXPLAINS CONSISTENT HASHING: Map servers and keys to a ring. Add server? Only nearby keys rebalance. Genius.

3. WALKS THROUGH MECHANICS:
   - Hash ring with virtual nodes
   - Key routing: Find first server clockwise on ring
   - Adding/removing servers: Minimal rehashing

4. SHOWS THE MATHEMATICS: Why it works, distribution uniformity with virtual nodes.

5. ADDRESSES LIMITATIONS: Not a silver bullet‚Äîrequires careful virtual node count selection.

6. SHOWS REAL USE CASES: Memcached clusters, load balancing, CDN edge distribution.

7. DISCUSSES MONITORING: Detecting hotspots, uneven distribution, adjusting virtual nodes.

Help readers see elegant algorithms that solve scaling problems, not just brute force.` },

                    { day: 7, title: 'Service Discovery and Registration', prompt: `Enable dynamic infrastructure where services find each other:

1. OPENS WITH MICROSERVICES CHAOS: 1000 microservices, each with 10 instances, IPs constantly changing. How does service A find service B?

2. EXPLAINS DISCOVERY APPROACHES:
   - Client-side: Client queries registry
   - Server-side: Load balancer queries registry
   - Trade-offs in complexity and resilience

3. TEACHES REGISTRATION PATTERNS:
   - Service registers on startup
   - Health checks mark services unhealthy
   - Deregistration on shutdown
   - Handling failures (crashed services)

4. WALKS THROUGH IMPLEMENTATION: Consul, etcd, Eureka‚Äîeach with philosophy.

5. DISCUSSES INTEGRATION WITH ORCHESTRATION: Kubernetes service discovery, built-in.

6. SHOWS OPERATIONAL MONITORING: Registry consistency, detecting stale entries.

7. INCLUDES FAILURE MODES: Registry down, cascading discovery failures, mitigation.

Help readers see service discovery as critical infrastructure, enabled by containers.` },

                    { day: 7, title: 'API Gateway Pattern', prompt: `Create single stable interface to microservices ecosystem:

1. OPENS WITH MICROSERVICES PAIN: 50 services exposed directly = chaos. API Gateway = stability.

2. EXPLAINS GATEWAY RESPONSIBILITIES:
   - Routing (request to correct service)
   - Authentication (unified identity)
   - Rate limiting (protect services)
   - Transformation (format adaptation)
   - Aggregation (combine multiple service responses)

3. WALKS THROUGH FLOWS: How requests traverse gateway, service selection logic.

4. DISCUSSES PATTERNS:
   - Single gateway (simplicity, bottleneck)
   - Multiple gateways per region (scalability)
   - Combining with service mesh (advanced)

5. ADDRESSES OPERATIONAL CHALLENGES: Gateway scaling, debugging, failure modes.

6. SHOWS EXAMPLES: Kong, AWS API Gateway, Netflix Zuul.

7. INCLUDES EVOLUTION: From gateway to service mesh (Istio, Linkerd).

Help readers see gateway as essential layer managing complexity of microservices.` },

                    // Day 8: CAP Theorem and Consistency
                    { day: 8, title: 'CAP Theorem Explained', prompt: `Understand the fundamental constraint in distributed systems:

1. OPENS WITH NETWORK PARTITION: Systems spanning continents. Network split: Americas cut off from Europe. Can't communicate. Now what?

2. EXPLAINS THE THEOREM: Choose 2 of 3 in presence of partition:
   - Consistency (all nodes agree)
   - Availability (system responds)
   - Partition tolerance (survives network split)

3. EMPHASIZES PARTITION IS MANDATORY: You can't avoid network failures. Choose C or A.

4. WALKS THROUGH SCENARIOS:
   - CP: Stop serving Americas, ensure consistency
   - AP: Serve both, allow temporary inconsistency
   - CA: Impossible if networks fail (partition must occur)

5. DISCUSSES REAL SYSTEMS: Cassandra (AP), Google Spanner (CA despite distributed).

6. ADDRESSES MISCONCEPTIONS: It's not about choosing at design time‚Äîchoice emerges at partition.

7. SHOWS APPLICATION IMPLICATIONS: How choice affects application logic, testing, failure handling.

Help readers see CAP as law of physics in distributed systems, not opinion.` },

                    { day: 8, title: 'Consistency Models', prompt: `Explore spectrum from strong to eventual consistency:

1. OPENS WITH CHOICE: Strong consistency slow but safe. Eventual consistency fast but requires rethinking.

2. EXPLAINS MODELS:
   - Strong: Replicas always agree (expensive)
   - Eventual: Replicas converge over time
   - Causal: Cause-effect relationships preserved
   - Read-Your-Writes: See your own changes immediately
   - Monotonic Reads: Don't go backwards in time

3. TEACHES TRADE-OFFS:
   - Strong: Latency penalty, synchronous
   - Eventual: Low latency, async, application complexity

4. WALKS THROUGH PATTERNS:
   - When each model is appropriate
   - Application architectural implications
   - Testing eventual consistency (hard!)

5. SHOWS DATABASE OPTIONS: DynamoDB, Cassandra offer choices.

6. ADDRESSES CHALLENGES: Application correctness with eventual consistency.

7. INCLUDES REAL EXAMPLES: Social media likes (eventual), banking (strong).

Help readers see consistency as spectrum, not binary, choose based on requirements.` },

                    { day: 8, title: 'Consensus Algorithms: Paxos and Raft', prompt: `Solve agreement problem across unreliable servers:

1. OPENS WITH DISTRIBUTED CONSENSUS CHALLENGE: Multiple replicas must agree on value despite failures, messages delays, crashes.

2. EXPLAINS PAXOS:
   - Provably correct
   - Complex, hard to implement
   - Used in Google Chubby, Apache Zookeeper

3. EXPLAINS RAFT:
   - Simpler than Paxos
   - Equally robust
   - Engineered for understandability
   - Used in etcd, Consul

4. WALKS THROUGH RAFT OPERATION: Leader election, log replication, safety.

5. DISCUSSES OPERATIONAL ASPECTS:
   - When to use consensus (critical data only)
   - Performance implications (slower)
   - Monitoring consensus health

6. EMPHASIZES USING BATTLE-TESTED IMPLEMENTATIONS: Don't implement yourself.

7. SHOWS REAL USAGE: Kubernetes uses etcd (Raft), database replication.

Help readers respect consensus as solution to hard problem, use proven implementations.` },

                    // Day 9: Distributed Systems Principles
                    { day: 9, title: 'Distributed Transactions and 2PC', prompt: `Coordinate atomic operations across multiple databases:

1. OPENS WITH HARD PROBLEM: Money transfer‚Äîdebit account A, credit account B. System crashes between. Both could fail. Disaster.

2. EXPLAINS TWO-PHASE COMMIT:
   - Prepare phase: All parties agree they can commit
   - Commit phase: All execute simultaneously
   - Atomicity across systems

3. WALKS THROUGH MECHANICS: Coordinator role, blocking issues, failure modes.

4. DISCUSSES LIMITATIONS:
   - Blocks on failures
   - Coordinator becoming single point of failure
   - Reduced availability during partition

5. SHOWS ALTERNATIVES: Saga pattern (orchestrated or choreographed) better for most distributed systems.

6. ADDRESSES REAL-WORLD ADVICE: Avoid 2PC when possible, use eventual consistency.

7. INCLUDES ADVANCED PATTERNS: Compensating transactions, saga implementation.

Help readers understand 2PC as tool of last resort, modern systems prefer sagas.` },

                    { day: 9, title: 'Eventual Consistency Patterns', prompt: `Build correct systems without requiring strong consistency:

1. OPENS WITH PARADIGM SHIFT: Social media likes appear instantly for you, eventually for everyone else. Works perfectly.

2. EXPLAINS EVENTUAL CONSISTENCY: Systems converge to consistent state over time, temporary divergence acceptable.

3. TEACHES PATTERNS:
   - Last-Write-Wins (simple, sometimes incorrect)
   - Vector Clocks (causal ordering)
   - CRDTs (conflict-free data types)

4. DISCUSSES APPLICATION DESIGN: Rethinking correctness in eventually consistent world.

5. WALKS THROUGH EXAMPLES:
   - Like counters (independent increments)
   - Inventory (overallocation acceptable)
   - Carts (concurrent modifications)

6. ADDRESSES MONITORING: Detecting inconsistencies, measuring convergence time.

7. SHOWS REAL SYSTEMS: Cassandra, DynamoDB, Facebook all rely on eventual consistency.

Help readers see eventual consistency not as compromise, but elegant design choice.` },

                    { day: 9, title: 'Distributed Locking', prompt: `Prevent concurrent access to shared resources:

1. OPENS WITH PROBLEM: Database rebalancing‚Äîtwo coordinators try simultaneously. Need distributed lock.

2. EXPLAINS LOCKING APPROACHES:
   - Centralized lock (single point of failure)
   - Distributed lock (requires majority agreement)
   - Redlock (Redis-based, approximate)

3. TEACHES LOCK ALGORITHMS: Ensuring safety, handling failures, preventing deadlock.

4. DISCUSSES TRADE-OFFS:
   - Simplicity vs robustness
   - Performance implications
   - Failure handling

5. ADDRESSES REAL CHALLENGES:
   - Network delays
   - Clock skew
   - Handling lock holder death

6. EMPHASIZES PREFER IDEMPOTENCY: Locks are complex, idempotent operations better.

7. SHOWS WHEN LOCKS ARE NECESSARY: Critical sections, resource contention.

Help readers see locks as last resort, idempotency and eventual consistency preferred.` },

                    // Day 10: Messaging & Event-Driven Architecture
                    { day: 10, title: 'Message Queues Deep Dive', prompt: `Decouple services via asynchronous communication:

1. OPENS WITH COUPLING PROBLEM: E-commerce‚Äîorder service calls payment immediately. Payment down = orders fail.

2. EXPLAINS DECOUPLING BENEFIT: Queue acts as buffer. Order saved, payment processes later.

3. TEACHES QUEUE MODELS:
   - Point-to-Point: One producer, one consumer
   - Pub/Sub: One producer, many subscribers
   - Each with different semantics

4. EXPLAINS DELIVERY GUARANTEES:
   - At-Least-Once (retried until acked)
   - At-Most-Once (best effort)
   - Exactly-Once (hardest, rare need)

5. WALKS THROUGH IMPLEMENTATIONS: RabbitMQ (traditional), Kafka (event log), AWS SQS.

6. DISCUSSES OPERATIONAL ASPECTS: Monitoring queue depth, handling poison messages.

7. ADDRESSES FAILURE MODES: Consumer crashes, reprocessing, idempotency.

Help readers see queues as fundamental building block for resilient systems.` },

                    { day: 10, title: 'Event Streaming with Kafka', prompt: `Build event-driven systems at massive scale with immutable event log:

1. OPENS WITH SCALE: LinkedIn‚Äîtrillions of events daily. Traditional queues insufficient.

2. EXPLAINS KAFKA MODEL:
   - Append-only log (immutable history)
   - Topics (event categories)
   - Partitions (parallelism, ordering per partition)
   - Consumer groups (coordinated reading)

3. TEACHES KAFKA DESIGN PRINCIPLES:
   - Durability (replication)
   - Scalability (partitions)
   - Ordering guarantees (per partition)
   - Replay capability (log retention)

4. WALKS THROUGH ARCHITECTURE: Brokers, ZooKeeper/KRaft, producers, consumers.

5. DISCUSSES USE CASES:
   - Event sourcing
   - Stream processing
   - Analytics pipelines
   - Real-time dashboards

6. SHOWS OPERATIONAL MONITORING: Lag, rebalancing, performance.

7. ADDRESSES ECOSYSTEM: Stream processing (Spark, Flink), connectors.

Help readers see Kafka as foundational platform for event-driven architecture.` },

                    { day: 10, title: 'Event-Driven Architecture Patterns', prompt: `Build loosely coupled, scalable systems through events:

1. OPENS WITH MONOLITH PAIN: Order service calls payment, shipping, analytics. One down cascades. Event-driven inverts flow.

2. EXPLAINS PATTERNS:
   - Event Sourcing (store all events, rebuild state)
   - CQRS (separate read and write models)
   - Choreography (services coordinate via events)
   - Orchestration (central coordinator directs)

3. TEACHES DESIGN PRINCIPLES:
   - Loose coupling (services independent)
   - Resilience (async, eventual)
   - Auditability (events are history)
   - Scalability (parallel processing)

4. WALKS THROUGH WORKFLOWS:
   - Choreography: Order ‚Üí Payment ‚Üí Shipping (sequential events)
   - Orchestration: Orchestrator sends commands, handles responses

5. DISCUSSES TRADE-OFFS:
   - Choreography simpler, harder to debug
   - Orchestration centralized, single point of failure risk

6. SHOWS REAL IMPLEMENTATIONS: Netflix (event-driven), Airbnb (choreography).

7. ADDRESSES CHALLENGES: Distributed tracing, debugging event flows, consistency.

Help readers see event-driven architecture as evolution beyond microservices.` },

                    // Day 11: Reliability & Fault Tolerance
                    { day: 11, title: 'Designing for Failure', prompt: `Build systems assuming failure is guaranteed:

1. OPENS WITH PHILOSOPHY: Netflix's "Everything fails, all the time." Embrace this. Design around it.

2. TEACHES STRATEGIES:
   - Redundancy (multiple copies, no single point of failure)
   - Bulkheads (isolation prevents cascade)
   - Graceful degradation (reduce features if degraded)
   - Chaos engineering (intentionally inject failures)

3. WALKS THROUGH FAILURES:
   - Server crashes (traffic rerouted)
   - Database fails (replica takes over)
   - Network partition (breaker activates, degrade)
   - Slow service (timeout + circuit breaker)

4. DISCUSSES DETECTION & RECOVERY: Health checks, automatic failover, graceful shutdown.

5. SHOWS TESTING STRATEGIES: Chaos monkey, fault injection, failure scenario planning.

6. ADDRESSES CULTURAL CHANGE: Blameless postmortems, learning from failures.

7. INCLUDES REAL EXAMPLES: Netflix Chaos Monkey, Amazon's operational excellence.

Help readers see failure as expected, design proactively.` },

                    { day: 11, title: 'Circuit Breakers and Retry Logic', prompt: `Prevent cascading failures through intelligent request handling:

1. OPENS WITH FAILURE CASCADE: Service A calls B, B overloaded. A retries, makes it worse. Thundering herd. System collapses.

2. EXPLAINS CIRCUIT BREAKER: Three states (Closed, Open, Half-Open) prevent cascading.

3. TEACHES RETRY STRATEGIES: Exponential backoff, jitter, retry budget.

4. WALKS THROUGH COMBINATIONS: Circuit breaker wraps retry logic.

5. DISCUSSES CONFIGURATION: Thresholds, timeouts, tuning.

6. SHOWS IMPLEMENTATION: Hystrix, resilience4j libraries.

7. ADDRESSES MONITORING: Circuit breaker state, retry rates.

Help readers see these as defensive layers, each necessary.` },

                    { day: 11, title: 'Rate Limiting and Throttling', prompt: `Protect services from overload through request gating:

1. OPENS WITH ATTACK: Million bot requests. Without rate limiting, system overloaded.

2. EXPLAINS ALGORITHMS: Token Bucket, Leaky Bucket, Sliding Window, Fixed Window.

3. TEACHES DISTRIBUTED RATE LIMITING: Coordinating limits across servers.

4. WALKS THROUGH DESIGN: Per-user vs global, token cost, fair queuing.

5. DISCUSSES COMMUNICATION: 429 responses with retry-after.

6. SHOWS IMPLEMENTATIONS: API gateways, libraries.

7. ADDRESSES MONITORING: Detecting rate limit exceedances.

Help readers see rate limiting as essential protection.` },

                    // Day 12: Observability & Monitoring
                    { day: 12, title: 'Logging, Metrics, and Tracing', prompt: `Build systems you can understand and debug:

1. OPENS WITH MYSTERY: Production issue. Logs show what. Metrics show when. Traces show where and why.

2. EXPLAINS THREE PILLARS: Logs (events), Metrics (quantitative), Tracing (flow).

3. TEACHES IMPLEMENTATION: Structured logging, instrumentation, distributed tracing.

4. WALKS THROUGH ANALYSIS: Using all three together.

5. DISCUSSES TOOLS: ELK, Prometheus, Jaeger, Splunk.

6. ADDRESSES VOLUME: Storage costs, retention policies.

7. INCLUDES BEST PRACTICES: Sampling, aggregation, alerting.

Help readers see observability as prerequisite for reliability.` },

                    { day: 12, title: 'Alerting and On-Call Best Practices', prompt: `Alert when action is needed, not when noise occurs:

1. OPENS WITH ALERT FATIGUE: Ignored alerts = missed outages. Focus.

2. EXPLAINS PROPER METRICS: SLI, SLO, SLA, error budgets.

3. TEACHES ALERT DESIGN: Outcomes not metrics, severity levels, actionability.

4. WALKS THROUGH ON-CALL: Schedules, escalation, postmortems.

5. DISCUSSES TOOLING: PagerDuty, Opsgenie.

6. ADDRESSES CULTURE: On-call responsibility, learning from incidents.

7. INCLUDES MONITORING: Is alerting effective?

Help readers balance signal and noise.` },

                    { day: 12, title: 'Application Performance Monitoring', prompt: `Identify bottlenecks before users complain:

1. OPENS WITH SYMPTOM: Users slow. APM reveals: new query, 80% time. Fix: index.

2. EXPLAINS APM VALUE: Real-time visibility, dependency mapping, bottlenecks.

3. TEACHES INSTRUMENTATION: Where and what to measure.

4. WALKS THROUGH ANALYSIS: Latency distribution, call graphs, resources.

5. DISCUSSES DRILL-DOWN: High-level slow to specific bottleneck.

6. SHOWS TOOLS: New Relic, Datadog, Dynatrace.

7. ADDRESSES PRODUCTION PROFILING: Sampling strategies.

Help readers catch problems early.` },

                    // Day 13: Security Fundamentals
                    { day: 13, title: 'Authentication and Authorization', prompt: `Verify identity and enforce permissions systematically:

1. OPENS WITH DEFINITIONS: Authentication (who?), Authorization (what?). Both essential.

2. TEACHES AUTHENTICATION: Passwords, OAuth 2.0, JWT, MFA.

3. TEACHES AUTHORIZATION: RBAC, ABAC, scope-based.

4. WALKS THROUGH FLOWS: Login, token generation, validation.

5. DISCUSSES SECURITY: Password hashing, token expiration, secure storage.

6. SHOWS IMPLEMENTATIONS: OAuth providers, JWT libraries.

7. ADDRESSES ADVANCED: Session management, CORS, CSRF.

Help readers implement carefully.` },

                    { day: 13, title: 'Encryption and Data Security', prompt: `Protect data at rest and in transit:

1. OPENS WITH BREACH: Database stolen, plaintext passwords. Preventable.

2. EXPLAINS ENCRYPTION: Symmetric (AES), Asymmetric (RSA), Hashing.

3. TEACHES IMPLEMENTATION: TLS for transit, database encryption, KMS.

4. WALKS THROUGH CHALLENGES: Key storage, rotation, performance.

5. DISCUSSES TOOLS: AWS KMS, HashiCorp Vault.

6. SHOWS BEST PRACTICES: Encrypting sensitive fields.

7. ADDRESSES COMPLIANCE: GDPR, HIPAA, regulations.

Help readers see encryption as fundamental.` },

                    { day: 13, title: 'DDoS Protection and Security Layers', prompt: `Defend against attacks at multiple levels:

1. OPENS WITH ATTACK: DDoS floods unprotected system. Multiple layers needed.

2. EXPLAINS LAYERS: Network, CDN, WAF, application.

3. WALKS THROUGH ATTACK TYPES: Volumetric, protocol, application.

4. TEACHES DEFENSES: Rate limiting, IP filtering, challenges, traffic shaping.

5. DISCUSSES TOOLS: Cloudflare, AWS Shield, Akamai.

6. SHOWS REAL-TIME: Detection, escalation, mitigation.

7. INCLUDES TESTING: Authorized penetration testing, simulation.

Help readers see defense-in-depth, no single solution.` },

                    // Day 14: API & Gateway Design
                    { day: 14, title: 'RESTful API Best Practices', prompt: `Design APIs that scale, last, and stay maintainable:

1. OPENS WITH HISTORY: Pre-REST chaos ‚Üí REST brings order.

2. TEACHES REST: Resources (nouns), methods, statelessness, cacheability, status codes.

3. WALKS THROUGH DESIGN: Naming, HTTP semantics, proper codes, pagination.

4. ADDRESSES PAGINATION: Cursor-based beats offset.

5. DISCUSSES EVOLUTION: Versioning, deprecation strategy.

6. SHOWS EXEMPLARY: Stripe, GitHub, Twitter‚Äîas reference.

7. INCLUDES ADVANCED: Webhooks, CORS, rate limiting design.

Help readers make good design decisions.` },

                    { day: 14, title: 'GraphQL vs REST', prompt: `Choose query language matching client needs:

1. OPENS WITH CONTRAST: Mobile app needs user+posts+comments. REST: 3 calls. GraphQL: 1.

2. EXPLAINS GRAPHQL: Precise fetching, flexible, type system, introspection.

3. EXPLAINS REST: Simplicity, maturity, HTTP compliance, statelessness.

4. TEACHES DECISION FRAMEWORK: GraphQL for complex. REST for simple. Hybrid common.

5. WALKS THROUGH GRAPHQL: Schema, resolvers, subscriptions.

6. DISCUSSES OPERATIONAL: Query complexity, monitoring, caching.

7. SHOWS REAL USAGE: GitHub, Shopify.

Help readers see as complementary.` },

                    { day: 14, title: 'API Versioning and Evolution', prompt: `Maintain backward compatibility while evolving:

1. OPENS WITH CONSTRAINT: Millions of clients. Can't break them. New features needed.

2. EXPLAINS STRATEGIES: URI versioning (/v1/), headers, semantic versioning.

3. TEACHES COMPATIBILITY: Adding fields (safe), renaming (breaking).

4. WALKS THROUGH DEPRECATION: Announce, guide, support, sunset.

5. DISCUSSES TOOLING: Versioning frameworks, documentation.

6. SHOWS EXAMPLES: Stripe's versioning masterclass.

7. ADDRESSES COMMUNICATION: Awareness, support, migrations.

Help readers see as strategic challenge.` },

                    // Day 15: Case Studies & Practice
                    { day: 15, title: 'Design URL Shortener (Case Study)', prompt: `Apply system design concepts to real problem:

1. OPENS WITH REQUIREMENTS: Billions of mappings, instant redirects, 99.99% uptime.

2. WALKS THROUGH ESTIMATION: QPS, data, sharding needs.

3. TEACHES DESIGN: URL encoding (Base62), database (key-value), caching (Redis), replication.

4. WALKS THROUGH ARCHITECTURE: Components, data flow, scaling.

5. DISCUSSES DEEP DIVES: Collisions, custom aliases, analytics.

6. ADDRESSES CHALLENGES: Hot URL optimization, deletion, privacy.

7. SHOWS VARIATIONS: Private, expiring links, branded.

Help readers practice framework.` },

                    { day: 15, title: 'Design Chat System (Case Study)', prompt: `Build real-time messaging at scale:

1. OPENS WITH REQUIREMENTS: Billions of messages, instant, offline queueing, read receipts.

2. WALKS THROUGH ARCHITECTURE: WebSockets, append-only log, presence, offline queue.

3. TEACHES DESIGN DECISIONS: When each component needed, trade-offs.

4. DISCUSSES CHALLENGES: Connection management, ordering, consistency, encryption.

5. SHOWS SCALING: Partitioning by conversation, load balancing.

6. ADDRESSES OPERATIONAL: Monitoring, debugging, connection limits.

7. INCLUDES VARIATIONS: Group chat, file sharing, search.

Help readers understand real-time complexity.` },

                    { day: 15, title: 'Design News Feed (Case Study)', prompt: `Build ranking and personalization at scale:

1. OPENS WITH CHALLENGE: 3B users, generation expensive, ranking matters.

2. WALKS THROUGH TRADE-OFFS: Pull (fresh, slow) vs Push (fast, complex) vs Hybrid (best).

3. TEACHES DESIGN: Hybrid approach, ML ranking, caching, partitioning.

4. WALKS THROUGH IMPLEMENTATION: Fan-out, ranking, caching, staleness.

5. DISCUSSES CHALLENGES: Thundering herd, consistency, search, ads.

6. SHOWS EVOLUTION: Facebook's journey.

7. ADDRESSES OPERATIONAL: Ranking effectiveness, A/B testing.

Help readers balance push and pull, importance of ranking.` },

                    { day: 15, title: 'Mock Interview Preparation', prompt: `Practice system design under realistic conditions:

1. OPENS WITH MINDSET: No perfect answer. Assess thinking, communication, depth, flexibility.

2. TEACHES FRAMEWORK: Clarify requirements ‚Üí estimate ‚Üí high-level ‚Üí detailed ‚Üí bottlenecks ‚Üí trade-offs ‚Üí follow-ups.

3. WALKS THROUGH REAL INTERVIEW: Candidate clarifies, proposes, answers deep questions.

4. EMPHASIZES: Clarity > perfection. Think aloud. Acknowledge trade-offs.

5. TEACHES DOMAIN KNOWLEDGE: Understand real systems (YouTube, Instagram, Uber).

6. SHOWS COMMON FOLLOW-UPS: "10x growth?" "Service failure?" "Regional scaling?"

7. ADDRESSES NERVES: Practice, preparation, clear communication.

Help readers build confidence through practice.` },
                ]
            }
        };

        // ==================== INITIALIZATION ====================
        function init() {
            loadSettings();
            loadProgress();
            renderLearningPaths();
            applyTheme();
        }

        function loadSettings() {
            const savedTheme = localStorage.getItem('theme') || 'light';
            const savedApiKey = localStorage.getItem('groq_api_key') || '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';
            const savedModel = localStorage.getItem('groq_model') || 'llama-3.3-70b-versatile';
            
            APP_STATE.theme = savedTheme;
            APP_STATE.apiKey = savedApiKey;
            APP_STATE.model = savedModel;
            
            document.getElementById('apiKeyInput').value = savedApiKey;
            document.getElementById('modelSelect').value = savedModel;
            
            // Clear old cache on format upgrade
            clearOldCache();
        }

        function clearOldCache() {
            // Clear concept cache to regenerate with new formatting
            Object.keys(localStorage).forEach(key => {
                if (key.startsWith('concept_')) {
                    localStorage.removeItem(key);
                }
            });
        }

        function loadProgress() {
            const savedPath = localStorage.getItem('currentPath');
            const savedIndex = localStorage.getItem('currentConceptIndex');
            
            if (savedPath) {
                APP_STATE.currentPath = savedPath;
                APP_STATE.currentConceptIndex = parseInt(savedIndex) || 0;
            }
        }

        // ==================== THEME ====================
        function toggleTheme() {
            APP_STATE.theme = APP_STATE.theme === 'light' ? 'dark' : 'light';
            applyTheme();
            localStorage.setItem('theme', APP_STATE.theme);
        }

        function applyTheme() {
            document.body.className = APP_STATE.theme + '-mode';
        }

        // ==================== SETTINGS ====================
        function openSettings() {
            document.getElementById('settingsModal').classList.add('active');
        }

        function closeSettings() {
            document.getElementById('settingsModal').classList.remove('active');
        }

        function saveSettings() {
            const apiKey = document.getElementById('apiKeyInput').value.trim();
            const model = document.getElementById('modelSelect').value;
            
            if (!apiKey) {
                alert('Please enter a valid Groq API key');
                return;
            }
            
            APP_STATE.apiKey = apiKey;
            APP_STATE.model = model;
            
            localStorage.setItem('groq_api_key', apiKey);
            localStorage.setItem('groq_model', model);
            
            closeSettings();
            alert('Settings saved successfully!');
        }

        function resetProgress() {
            if (confirm('Are you sure you want to reset all progress? This cannot be undone.')) {
                // Clear all learning progress
                Object.keys(localStorage).forEach(key => {
                    if (key.startsWith('concept_') || key.startsWith('quiz_') || 
                        key.startsWith('progress_') || key.startsWith('stats_')) {
                        localStorage.removeItem(key);
                    }
                });
                
                APP_STATE.currentConceptIndex = 0;
                localStorage.removeItem('currentPath');
                localStorage.removeItem('currentConceptIndex');
                
                closeSettings();
                goHome();
                alert('All progress has been reset.');
            }
        }

        // ==================== NAVIGATION ====================
        function goHome() {
            document.getElementById('homeView').classList.add('active');
            document.getElementById('learningView').classList.remove('active');
            document.getElementById('historyView').classList.remove('active');
            APP_STATE.isQuizMode = false;
            renderLearningPaths();
        }

        function startLearningPath(pathId) {
            // Store the path ID to be used when mode is selected
            APP_STATE.pendingPathId = pathId;
            console.log('Starting learning path:', pathId);
            
            // Show the mode selection modal
            const modeModal = document.getElementById('modeModal');
            if (modeModal) {
                modeModal.classList.add('active');
                console.log('Mode modal shown');
            } else {
                console.error('Mode modal not found');
            }
        }

        function selectMode(mode) {
            console.log('Mode selected:', mode);
            APP_STATE.isOfflineMode = (mode === 'offline');
            
            // Check if API key is needed for online mode
            if (!APP_STATE.isOfflineMode && !APP_STATE.apiKey) {
                alert('Please configure your Groq API key in Settings first.');
                document.getElementById('modeModal').classList.remove('active');
                openSettings();
                return;
            }
            
            // Close modal and start learning
            document.getElementById('modeModal').classList.remove('active');
            console.log('Pending path ID before proceed:', APP_STATE.pendingPathId);
            proceedWithLearning();
        }

        function closeModeModal() {
            document.getElementById('modeModal').classList.remove('active');
        }

        function cancelModeSelection() {
            document.getElementById('modeModal').classList.remove('active');
            APP_STATE.pendingPathId = null;
        }

        function switchToOnlineMode() {
            APP_STATE.isOfflineMode = false;
            localStorage.setItem('offlineMode', 'false');
            loadConcept();
        }

        function proceedWithLearning() {
            const pathId = APP_STATE.pendingPathId;
            console.log('Proceeding with learning, pathId:', pathId);
            
            if (!pathId) {
                console.error('No path ID found');
                return;
            }
            
            APP_STATE.currentPath = pathId;
            APP_STATE.currentConceptIndex = parseInt(localStorage.getItem(`progress_${pathId}`)) || 0;
            
            localStorage.setItem('currentPath', pathId);
            localStorage.setItem('offlineMode', APP_STATE.isOfflineMode);
            
            console.log('About to hide home view and show learning view');
            document.getElementById('homeView').classList.remove('active');
            document.getElementById('learningView').classList.add('active');
            document.getElementById('historyView').classList.remove('active');
            
            console.log('Loading concept...');
            loadConcept();
        }

        function returnToLearning() {
            document.getElementById('homeView').classList.remove('active');
            document.getElementById('learningView').classList.add('active');
            document.getElementById('historyView').classList.remove('active');
            loadConcept();
        }

        function nextConcept() {
            const path = LEARNING_PATHS[APP_STATE.currentPath];
            if (APP_STATE.currentConceptIndex < path.concepts.length - 1) {
                APP_STATE.currentConceptIndex++;
                localStorage.setItem('currentConceptIndex', APP_STATE.currentConceptIndex);
                loadConcept();
            }
        }

        function prevConcept() {
            if (APP_STATE.currentConceptIndex > 0) {
                APP_STATE.currentConceptIndex--;
                localStorage.setItem('currentConceptIndex', APP_STATE.currentConceptIndex);
                loadConcept();
            }
        }

        // ==================== RENDERING ====================
        function renderLearningPaths() {
            const container = document.getElementById('learningPaths');
            container.innerHTML = '';
            
            Object.values(LEARNING_PATHS).forEach(path => {
                const progress = calculatePathProgress(path.id);
                const card = document.createElement('div');
                card.className = 'path-card';
                card.onclick = () => startLearningPath(path.id);
                
                card.innerHTML = `
                    <div class="path-icon">${path.icon}</div>
                    <div class="path-title">${path.title}</div>
                    <div class="path-description">${path.description}</div>
                    <div class="path-stats">
                        <span>üìö ${path.concepts.length} Concepts</span>
                        <span>üìÖ ${path.estimatedDays} Days</span>
                    </div>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: ${progress}%"></div>
                    </div>
                    <div class="path-actions">
                        <button class="btn btn-primary">
                            ${progress > 0 ? 'Continue Learning' : 'Start Learning'}
                        </button>
                    </div>
                `;
                
                container.appendChild(card);
            });
        }

        function calculatePathProgress(pathId) {
            const path = LEARNING_PATHS[pathId];
            let completed = 0;
            
            path.concepts.forEach((_, index) => {
                const quizKey = `quiz_${pathId}_${index}`;
                const quizData = localStorage.getItem(quizKey);
                if (quizData) {
                    const data = JSON.parse(quizData);
                    if (data.attempts && data.attempts.length > 0) {
                        completed++;
                    }
                }
            });
            
            return Math.round((completed / path.concepts.length) * 100);
        }

        // ==================== CONCEPT LOADING ====================
        async function loadConcept() {
            APP_STATE.isQuizMode = false;
            // Restore offline mode from localStorage or use current state
            const savedOfflineMode = localStorage.getItem('offlineMode') === 'true';
            APP_STATE.isOfflineMode = savedOfflineMode;
            
            const path = LEARNING_PATHS[APP_STATE.currentPath];
            const concept = path.concepts[APP_STATE.currentConceptIndex];
            const conceptKey = `concept_${APP_STATE.currentPath}_${APP_STATE.currentConceptIndex}`;
            const fallbackKey = `${APP_STATE.currentPath}_${APP_STATE.currentConceptIndex}`;
            
            updateProgressSidebar();
            updateNavigationButtons();
            
            const container = document.getElementById('contentContainer');
            
            // If offline mode is selected, try fallback content first
            if (APP_STATE.isOfflineMode) {
                if (FALLBACK_CONTENT[fallbackKey]) {
                    const cached = localStorage.getItem(conceptKey);
                    const conceptData = cached ? JSON.parse(cached) : {
                        title: concept.title,
                        content: FALLBACK_CONTENT[fallbackKey],
                        day: concept.day,
                        timestamp: Date.now(),
                        isOffline: true
                    };
                    renderConcept(conceptData);
                    return;
                } else {
                    container.innerHTML = `
                        <div style="text-align: center; padding: 60px;">
                            <h2 style="color: var(--danger); margin-bottom: 20px;">‚ö†Ô∏è Content Not Available Offline</h2>
                            <p style="margin-bottom: 20px;">Offline content is not available for this topic.</p>
                            <button class="btn btn-primary" onclick="switchToOnlineMode()">Switch to Online Mode</button>
                            <button class="btn btn-secondary" onclick="prevConcept()" style="margin-left: 10px;">Try Previous Concept</button>
                        </div>
                    `;
                    return;
                }
            }
            
            // Check if concept is cached
            const cached = localStorage.getItem(conceptKey);
            if (cached) {
                const data = JSON.parse(cached);
                renderConcept(data);
                return;
            }
            
            // Generate concept using AI (Online mode)
            container.innerHTML = `
                <div class="loading-spinner">
                    <div class="spinner"></div>
                    <div class="loading-text">Generating personalized content...</div>
                </div>
            `;
            
            try {
                const content = await generateContent(concept.prompt);
                const conceptData = {
                    title: concept.title,
                    content: content,
                    day: concept.day,
                    timestamp: Date.now(),
                    isOffline: false
                };
                
                localStorage.setItem(conceptKey, JSON.stringify(conceptData));
                renderConcept(conceptData);
            } catch (error) {
                // Try fallback content on error
                if (FALLBACK_CONTENT[fallbackKey]) {
                    APP_STATE.isOfflineMode = true;
                    const conceptData = {
                        title: concept.title,
                        content: FALLBACK_CONTENT[fallbackKey],
                        day: concept.day,
                        timestamp: Date.now(),
                        isOffline: true
                    };
                    renderConcept(conceptData);
                    // Update navigation buttons after setting offline mode
                    updateNavigationButtons();
                } else {
                    container.innerHTML = `
                        <div style="text-align: center; padding: 60px;">
                            <h2 style="color: var(--danger); margin-bottom: 20px;">‚ö†Ô∏è Error Loading Content</h2>
                            <p style="margin-bottom: 20px;">${error.message}</p>
                            <p style="margin-bottom: 20px; color: var(--text-muted);">Offline content is not available for this topic yet.</p>
                            <button class="btn btn-primary" onclick="loadConcept()">Try Again</button>
                        </div>
                    `;
                }
            }
        }

        function renderConcept(data) {
            const container = document.getElementById('contentContainer');
            const offlineBadge = data.isOffline ? `<span style="background: #f59e0b; color: white; padding: 4px 12px; border-radius: 20px; font-size: 12px; margin-left: 10px;">üì¥ Offline Mode</span>` : '';
            container.innerHTML = `
                <div class="concept-header">
                    <div class="concept-header-content">
                        <div class="concept-title">${data.title}${offlineBadge}</div>
                        <div class="concept-meta">
                            <span>üìÖ Day ${data.day}</span>
                            <span>üìñ Concept ${APP_STATE.currentConceptIndex + 1} of ${LEARNING_PATHS[APP_STATE.currentPath].concepts.length}</span>
                        </div>
                    </div>
                    <button class="regenerate-btn" onclick="regenerateContent()" title="Generate new content for this concept" ${data.isOffline ? 'style="display:none;"' : ''}>
                        üîÑ Regenerate
                    </button>
                </div>
                <div class="concept-content">${formatContent(data.content)}</div>
            `;
        }

        function regenerateContent() {
            const btn = event.target;
            btn.disabled = true;
            btn.textContent = '‚è≥ Regenerating...';
            
            // Clear cache for this concept
            const cacheKey = `concept_${APP_STATE.currentPath}_${APP_STATE.currentConceptIndex}`;
            localStorage.removeItem(cacheKey);
            
            // Reload concept (will fetch fresh from API)
            loadConcept().then(() => {
                // Re-enable button
                const newBtn = document.querySelector('.regenerate-btn');
                if (newBtn) {
                    newBtn.disabled = false;
                    newBtn.textContent = 'üîÑ Regenerate';
                }
            }).catch(() => {
                btn.disabled = false;
                btn.textContent = 'üîÑ Regenerate';
            });
        }

        function formatContent(content) {
            // First, extract and preserve code blocks
            const codeBlocks = [];
            let processedContent = content.replace(/```[\s\S]*?```/g, (match) => {
                codeBlocks.push(match);
                return `__CODE_BLOCK_${codeBlocks.length - 1}__`;
            });

            let html = '';
            
            // Split into lines to process structured content
            const lines = processedContent.split('\n');
            let i = 0;
            
            while (i < lines.length) {
                const line = lines[i];
                const trimmed = line.trim();
                
                // Skip empty lines
                if (!trimmed) {
                    i++;
                    continue;
                }
                
                // Handle headers
                if (trimmed.match(/^#{1,3}\s/)) {
                    const level = trimmed.match(/^#+/)[0].length;
                    const title = trimmed.replace(/^#+\s+/, '');
                    html += `<h${level}>${title}</h${level}>`;
                    i++;
                    continue;
                }
                
                // Handle structured boxes - can appear inline or standalone
                const boxTypes = [
                    { marker: 'STORY', class: 'story-box', icon: 'üìñ' },
                    { marker: 'ANALOGY', class: 'analogy-box', icon: 'üí°' },
                    { marker: 'EXAMPLE', class: 'example-box', icon: 'üíª' },
                    { marker: 'WARNING', class: 'warning-box', icon: '‚ö†Ô∏è' },
                    { marker: 'TIP', class: 'tip-box', icon: '‚ú®' },
                    { marker: 'REAL-WORLD', class: 'real-world-box', icon: 'üöÄ' }
                ];
                
                let foundBox = false;
                for (const boxType of boxTypes) {
                    if (trimmed.includes(`[${boxType.marker}]`)) {
                        // Extract content after the marker
                        const match = trimmed.match(new RegExp(`\\[${boxType.marker}\\]\\s*(.*)`));
                        if (match) {
                            let boxContent = match[1];
                            
                            // Collect following lines until we hit a marker or empty line
                            i++;
                            while (i < lines.length) {
                                const nextLine = lines[i].trim();
                                if (!nextLine) break;
                                if (nextLine.match(/^\[(?:STORY|ANALOGY|EXAMPLE|WARNING|TIP|REAL-WORLD)\]/)) break;
                                if (nextLine.match(/^#{1,3}\s/)) break;
                                if (nextLine.match(/^[-‚Ä¢*]\s/) || nextLine.match(/^\d+\.\s/)) break;
                                
                                boxContent += '\n' + nextLine;
                                i++;
                            }
                            
                            html += `<div class="content-box ${boxType.class}"><strong class="box-title">${boxType.icon} ${boxType.marker.charAt(0) + boxType.marker.slice(1).toLowerCase()}</strong>${formatInlineContent(boxContent)}</div>`;
                            foundBox = true;
                            break;
                        }
                    }
                }
                
                if (foundBox) continue;
                
                // Handle ordered lists
                if (trimmed.match(/^\d+\.\s/)) {
                    html += '<ol>';
                    while (i < lines.length && lines[i].trim().match(/^\d+\.\s/)) {
                        const content = lines[i].trim().replace(/^\d+\.\s+/, '');
                        html += `<li>${formatInlineContent(content)}</li>`;
                        i++;
                    }
                    html += '</ol>';
                    continue;
                }
                
                // Handle unordered lists
                if (trimmed.match(/^[-‚Ä¢*]\s/)) {
                    html += '<ul>';
                    while (i < lines.length && lines[i].trim().match(/^[-‚Ä¢*]\s/)) {
                        const content = lines[i].trim().replace(/^[-‚Ä¢*]\s+/, '');
                        html += `<li>${formatInlineContent(content)}</li>`;
                        i++;
                    }
                    html += '</ul>';
                    continue;
                }
                
                // Handle blockquotes
                if (trimmed.match(/^>\s/)) {
                    html += '<blockquote>';
                    while (i < lines.length && lines[i].trim().match(/^>\s/)) {
                        const content = lines[i].trim().replace(/^>\s+/, '');
                        html += formatInlineContent(content);
                        i++;
                    }
                    html += '</blockquote>';
                    continue;
                }
                
                // Regular paragraph
                html += `<p>${formatInlineContent(trimmed)}</p>`;
                i++;
            }
            
            // Restore code blocks
            html = html.replace(/__CODE_BLOCK_(\d+)__/g, (match, idx) => {
                const codeBlock = codeBlocks[parseInt(idx)];
                const codeContent = codeBlock.replace(/^```[\w]*\n?([\s\S]*?)\n?```$/m, '$1');
                return '<pre><code>' + escapeHtml(codeContent) + '</code></pre>';
            });
            
            return html;
        }

        function formatInlineContent(text) {
            // Handle key concepts with {braces}
            text = text.replace(/\*\*\{([^}]+)\}\*\*/g, '<span class="key-concept">$1</span>');
            
            // Handle highlights
            text = text.replace(/==([^=]+)==/g, '<span class="highlight">$1</span>');
            
            // Handle bold
            text = text.replace(/\*\*([^*]+)\*\*/g, '<strong>$1</strong>');
            
            // Handle italics
            text = text.replace(/\*([^*]+)\*/g, '<em>$1</em>');
            
            // Handle inline code
            text = text.replace(/`([^`]+)`/g, '<code>$1</code>');
            
            return text;
        }

        function escapeHtml(text) {
            const map = {
                '&': '&amp;',
                '<': '&lt;',
                '>': '&gt;',
                '"': '&quot;',
                "'": '&#039;'
            };
            return text.replace(/[&<>"']/g, m => map[m]);
        }

        // ==================== QUIZ SYSTEM ====================
        async function startQuiz() {
            // Note: API key check removed - will fall back to offline quizzes if API unavailable
            
            APP_STATE.isQuizMode = true;
            const container = document.getElementById('contentContainer');
            
            container.innerHTML = `
                <div class="quiz-container">
                    <div class="quiz-header">
                        <div class="quiz-title">Quiz: ${LEARNING_PATHS[APP_STATE.currentPath].concepts[APP_STATE.currentConceptIndex].title}</div>
                        <div class="quiz-meta">
                            <span>Select difficulty level to generate quiz</span>
                        </div>
                    </div>
                    <div class="difficulty-selector">
                        <button class="difficulty-btn" onclick="generateQuiz('easy')">Easy</button>
                        <button class="difficulty-btn" onclick="generateQuiz('medium')">Medium</button>
                        <button class="difficulty-btn" onclick="generateQuiz('hard')">Hard</button>
                        <button class="difficulty-btn" onclick="generateQuiz('expert')">Expert</button>
                    </div>
                </div>
            `;
        }

        async function generateQuiz(difficulty) {
            const concept = LEARNING_PATHS[APP_STATE.currentPath].concepts[APP_STATE.currentConceptIndex];
            const fallbackKey = `${APP_STATE.currentPath}_${APP_STATE.currentConceptIndex}`;
            const container = document.getElementById('contentContainer');
            
            // Get the currently displayed content
            const conceptKey = `concept_${APP_STATE.currentPath}_${APP_STATE.currentConceptIndex}`;
            const cached = localStorage.getItem(conceptKey);
            let displayedContent = '';
            let isOfflineMode = false;
            
            if (cached) {
                const data = JSON.parse(cached);
                displayedContent = data.content;
                isOfflineMode = data.isOffline || APP_STATE.isOfflineMode;
            } else if (FALLBACK_CONTENT[fallbackKey]) {
                displayedContent = FALLBACK_CONTENT[fallbackKey];
                isOfflineMode = true;
            }
            
            // If in offline mode, try to use fallback quiz
            if (isOfflineMode || APP_STATE.isOfflineMode) {
                if (FALLBACK_QUIZZES[fallbackKey] && FALLBACK_QUIZZES[fallbackKey][difficulty]) {
                    const fallbackQuestions = FALLBACK_QUIZZES[fallbackKey][difficulty];
                    const questions = fallbackQuestions.map(q => ({
                        question: q.question,
                        options: q.options,
                        correctAnswer: q.correct,
                        explanation: q.explanation
                    }));
                    renderQuiz(questions, difficulty, true);
                    return;
                } else {
                    container.innerHTML = `
                        <div style="text-align: center; padding: 60px;">
                            <h2 style="color: var(--danger); margin-bottom: 20px;">‚ö†Ô∏è Offline Quiz Not Available</h2>
                            <p style="margin-bottom: 20px;">Quiz is not available in offline mode for this topic.</p>
                            <button class="btn btn-primary" onclick="switchToOnlineMode()">Switch to Online Mode</button>
                            <button class="btn btn-secondary" onclick="loadConcept()" style="margin-left: 10px;">Back to Content</button>
                        </div>
                    `;
                    return;
                }
            }
            
            container.innerHTML = `
                <div class="loading-spinner">
                    <div class="spinner"></div>
                    <div class="loading-text">Generating ${difficulty} quiz...</div>
                </div>
            `;
            
            try {
                const quizData = await generateQuizContent(displayedContent, concept.title, difficulty);
                const questions = JSON.parse(quizData);
                renderQuiz(questions, difficulty, false);
            } catch (error) {
                // Try fallback quiz on error
                if (FALLBACK_QUIZZES[fallbackKey] && FALLBACK_QUIZZES[fallbackKey][difficulty]) {
                    const fallbackQuestions = FALLBACK_QUIZZES[fallbackKey][difficulty];
                    const questions = fallbackQuestions.map(q => ({
                        question: q.question,
                        options: q.options,
                        correctAnswer: q.correct,
                        explanation: q.explanation
                    }));
                    renderQuiz(questions, difficulty, true);
                } else {
                    container.innerHTML = `
                        <div style="text-align: center; padding: 60px;">
                            <h2 style="color: var(--danger); margin-bottom: 20px;">‚ö†Ô∏è Error Generating Quiz</h2>
                            <p style="margin-bottom: 20px;">${error.message}</p>
                            <p style="margin-bottom: 20px; color: var(--text-muted);">Offline quizzes are not available for this topic yet.</p>
                            <button class="btn btn-primary" onclick="startQuiz()">Try Again</button>
                        </div>
                    `;
                }
            }
        }

        let currentQuiz = null;
        let userAnswers = [];

        function renderQuiz(questions, difficulty, isOffline = false) {
            currentQuiz = { questions, difficulty, userAnswers: [], isOffline };
            userAnswers = new Array(questions.length).fill(null);
            
            const offlineBadge = isOffline ? `<span style="background: #f59e0b; color: white; padding: 4px 12px; border-radius: 20px; font-size: 12px; margin-left: 10px;">üì¥ Offline Mode</span>` : '';
            
            const container = document.getElementById('contentContainer');
            let html = `
                <div class="quiz-container">
                    <div class="quiz-header">
                        <div class="quiz-title">Quiz: ${LEARNING_PATHS[APP_STATE.currentPath].concepts[APP_STATE.currentConceptIndex].title}${offlineBadge}</div>
                        <div class="quiz-meta">
                            <span>üìä Difficulty: ${difficulty.toUpperCase()}</span>
                            <span>‚ùì ${questions.length} Questions</span>
                        </div>
                    </div>
            `;
            
            questions.forEach((q, qIndex) => {
                html += `
                    <div class="question-card" id="question-${qIndex}">
                        <div class="question-number">Question ${qIndex + 1}</div>
                        <div class="question-text">${q.question}</div>
                        <div class="options-list">
                `;
                
                q.options.forEach((option, oIndex) => {
                    html += `
                        <div class="option" onclick="selectOption(${qIndex}, ${oIndex})">
                            <input type="radio" name="question-${qIndex}" id="q${qIndex}-o${oIndex}" style="margin-right: 10px;">
                            <label for="q${qIndex}-o${oIndex}">${option}</label>
                        </div>
                    `;
                });
                
                html += `
                        </div>
                    </div>
                `;
            });
            
            html += `
                    <div class="quiz-actions">
                        <button class="btn btn-primary" onclick="submitQuiz()">Submit Quiz</button>
                        <button class="btn btn-secondary" onclick="startQuiz()">Regenerate Quiz</button>
                        <button class="btn btn-secondary" onclick="loadConcept()">Back to Concept</button>
                    </div>
                </div>
            `;
            
            container.innerHTML = html;
        }

        function selectOption(questionIndex, optionIndex) {
            userAnswers[questionIndex] = optionIndex;
            
            // Update UI
            const questionCard = document.getElementById(`question-${questionIndex}`);
            const options = questionCard.querySelectorAll('.option');
            options.forEach((opt, idx) => {
                opt.classList.remove('selected');
                if (idx === optionIndex) {
                    opt.classList.add('selected');
                }
            });
        }

        function submitQuiz() {
            if (userAnswers.includes(null)) {
                alert('Please answer all questions before submitting.');
                return;
            }
            
            const questions = currentQuiz.questions;
            let correct = 0;
            
            questions.forEach((q, qIndex) => {
                const questionCard = document.getElementById(`question-${qIndex}`);
                const options = questionCard.querySelectorAll('.option');
                
                options.forEach((opt, oIndex) => {
                    opt.style.pointerEvents = 'none';
                    
                    if (oIndex === q.correctAnswer) {
                        opt.classList.add('correct');
                    }
                    
                    if (oIndex === userAnswers[qIndex] && oIndex !== q.correctAnswer) {
                        opt.classList.add('incorrect');
                    }
                });
                
                if (userAnswers[qIndex] === q.correctAnswer) {
                    correct++;
                }
                
                // Add explanation
                const explanation = document.createElement('div');
                explanation.style.cssText = 'margin-top: 15px; padding: 15px; background: rgba(99, 102, 241, 0.1); border-radius: 8px; font-size: 14px;';
                explanation.innerHTML = `<strong>Explanation:</strong> ${q.explanation}`;
                questionCard.appendChild(explanation);
            });
            
            const score = Math.round((correct / questions.length) * 100);
            
            // Save quiz attempt
            saveQuizAttempt(score, correct, questions.length, currentQuiz.difficulty);
            
            // Show results
            showQuizResults(score, correct, questions.length);
            
            // Enable next concept if passed
            if (score >= 70) {
                const nextBtn = document.getElementById('nextConceptBtn');
                nextBtn.disabled = false;
                
                // Update progress
                const currentProgress = parseInt(localStorage.getItem(`progress_${APP_STATE.currentPath}`)) || 0;
                if (APP_STATE.currentConceptIndex >= currentProgress) {
                    localStorage.setItem(`progress_${APP_STATE.currentPath}`, APP_STATE.currentConceptIndex + 1);
                }
            }
            
            updateProgressSidebar();
        }

        function showQuizResults(score, correct, total) {
            const container = document.getElementById('contentContainer');
            const resultsDiv = document.createElement('div');
            resultsDiv.className = 'quiz-results';
            resultsDiv.innerHTML = `
                <div class="results-score">${score}%</div>
                <div class="results-message">
                    ${score >= 70 ? 'üéâ Great job! You passed!' : 'üí™ Keep practicing!'}
                </div>
                <p>You got ${correct} out of ${total} questions correct.</p>
                <div style="display: flex; gap: 15px; justify-content: center; margin-top: 20px;">
                    <button class="btn btn-primary" onclick="startQuiz()">Take Another Quiz</button>
                    ${score >= 70 ? '<button class="btn btn-primary" onclick="nextConcept()">Next Concept</button>' : ''}
                    <button class="btn btn-secondary" onclick="loadConcept()">Review Concept</button>
                </div>
            `;
            
            container.insertBefore(resultsDiv, container.firstChild);
            container.scrollTop = 0;
        }

        function saveQuizAttempt(score, correct, total, difficulty) {
            const quizKey = `quiz_${APP_STATE.currentPath}_${APP_STATE.currentConceptIndex}`;
            const existing = localStorage.getItem(quizKey);
            const data = existing ? JSON.parse(existing) : { attempts: [] };
            
            data.attempts.push({
                timestamp: Date.now(),
                score: score,
                correct: correct,
                total: total,
                difficulty: difficulty,
                questions: currentQuiz.questions.map((q, idx) => ({
                    question: q.question,
                    userAnswer: userAnswers[idx],
                    correctAnswer: q.correctAnswer,
                    isCorrect: userAnswers[idx] === q.correctAnswer
                }))
            });
            
            localStorage.setItem(quizKey, JSON.stringify(data));
        }

        // ==================== PROGRESS TRACKING ====================
        function updateProgressSidebar() {
            const path = LEARNING_PATHS[APP_STATE.currentPath];
            let totalQuizzes = 0;
            let totalScore = 0;
            let conceptsCompleted = 0;
            
            path.concepts.forEach((_, index) => {
                const quizKey = `quiz_${APP_STATE.currentPath}_${index}`;
                const quizData = localStorage.getItem(quizKey);
                if (quizData) {
                    const data = JSON.parse(quizData);
                    if (data.attempts && data.attempts.length > 0) {
                        conceptsCompleted++;
                        totalQuizzes += data.attempts.length;
                        data.attempts.forEach(attempt => {
                            totalScore += attempt.score;
                        });
                    }
                }
            });
            
            const avgScore = totalQuizzes > 0 ? Math.round(totalScore / totalQuizzes) : 0;
            const progress = Math.round((conceptsCompleted / path.concepts.length) * 100);
            
            document.getElementById('overallProgress').textContent = progress + '%';
            document.getElementById('progressFill').style.width = progress + '%';
            document.getElementById('conceptsCompleted').textContent = conceptsCompleted;
            document.getElementById('quizzesCompleted').textContent = totalQuizzes;
            document.getElementById('averageScore').textContent = avgScore + '%';
            
            // Calculate streak (simplified)
            const streakDays = calculateStreak();
            document.getElementById('streakDays').textContent = streakDays;
        }

        function calculateStreak() {
            // Simplified streak calculation
            const today = new Date().toDateString();
            const lastActive = localStorage.getItem('lastActive');
            
            if (lastActive === today) {
                return parseInt(localStorage.getItem('streakDays')) || 1;
            } else {
                const streak = 1;
                localStorage.setItem('lastActive', today);
                localStorage.setItem('streakDays', streak);
                return streak;
            }
        }

        function updateNavigationButtons() {
            const path = LEARNING_PATHS[APP_STATE.currentPath];
            const prevBtn = document.getElementById('prevConceptBtn');
            const nextBtn = document.getElementById('nextConceptBtn');
            
            prevBtn.disabled = APP_STATE.currentConceptIndex === 0;
            
            // In offline mode, allow free navigation (no quiz requirement)
            if (APP_STATE.isOfflineMode) {
                nextBtn.disabled = APP_STATE.currentConceptIndex >= path.concepts.length - 1;
                return;
            }
            
            // Check if current concept quiz is passed (only in online mode)
            const quizKey = `quiz_${APP_STATE.currentPath}_${APP_STATE.currentConceptIndex}`;
            const quizData = localStorage.getItem(quizKey);
            let passed = false;
            
            if (quizData) {
                const data = JSON.parse(quizData);
                if (data.attempts && data.attempts.length > 0) {
                    passed = data.attempts.some(attempt => attempt.score >= 70);
                }
            }
            
            nextBtn.disabled = !passed || APP_STATE.currentConceptIndex >= path.concepts.length - 1;
        }

        // ==================== QUIZ HISTORY ====================
        function viewHistory() {
            document.getElementById('homeView').classList.remove('active');
            document.getElementById('learningView').classList.remove('active');
            document.getElementById('historyView').classList.add('active');
            
            renderQuizHistory();
        }

        function renderQuizHistory() {
            const path = LEARNING_PATHS[APP_STATE.currentPath];
            const historyList = document.getElementById('historyList');
            const conceptFilter = document.getElementById('conceptFilter');
            
            // Populate concept filter
            conceptFilter.innerHTML = '<option value="all">All Concepts</option>';
            path.concepts.forEach((concept, index) => {
                conceptFilter.innerHTML += `<option value="${index}">${concept.title}</option>`;
            });
            
            // Render history
            historyList.innerHTML = '';
            let allAttempts = [];
            
            path.concepts.forEach((concept, index) => {
                const quizKey = `quiz_${APP_STATE.currentPath}_${index}`;
                const quizData = localStorage.getItem(quizKey);
                
                if (quizData) {
                    const data = JSON.parse(quizData);
                    if (data.attempts) {
                        data.attempts.forEach(attempt => {
                            allAttempts.push({
                                ...attempt,
                                conceptIndex: index,
                                conceptTitle: concept.title
                            });
                        });
                    }
                }
            });
            
            // Sort by timestamp (newest first)
            allAttempts.sort((a, b) => b.timestamp - a.timestamp);
            
            if (allAttempts.length === 0) {
                historyList.innerHTML = '<p style="text-align: center; padding: 40px; opacity: 0.7;">No quiz attempts yet. Start taking quizzes to see your history here.</p>';
                return;
            }
            
            allAttempts.forEach(attempt => {
                const date = new Date(attempt.timestamp);
                const item = document.createElement('div');
                item.className = 'history-item';
                item.innerHTML = `
                    <div class="history-header">
                        <div>
                            <strong>${attempt.conceptTitle}</strong>
                            <div style="font-size: 14px; opacity: 0.7; margin-top: 5px;">
                                ${date.toLocaleDateString()} at ${date.toLocaleTimeString()}
                            </div>
                        </div>
                        <div style="text-align: right;">
                            <div style="font-size: 24px; font-weight: 700; color: var(--primary);">${attempt.score}%</div>
                            <div style="font-size: 14px; opacity: 0.7;">Difficulty: ${attempt.difficulty}</div>
                        </div>
                    </div>
                    <div style="margin-top: 10px; font-size: 14px;">
                        ‚úÖ ${attempt.correct} correct | ‚ùå ${attempt.total - attempt.correct} incorrect
                    </div>
                `;
                historyList.appendChild(item);
            });
        }

        function filterHistory() {
            const conceptFilter = document.getElementById('conceptFilter').value;
            const difficultyFilter = document.getElementById('difficultyFilter').value;
            
            // Re-render with filters (implementation simplified for space)
            renderQuizHistory();
        }

        function generateWeakAreasQuiz() {
            alert('Weak Areas Quiz feature will analyze all your incorrect answers and generate a targeted practice quiz. Implementation in progress!');
        }

        // ==================== API INTEGRATION ====================
        async function generateContent(prompt) {
            if (!APP_STATE.apiKey) {
                throw new Error('API key not configured. Please add your Groq API key in Settings.');
            }
            
            const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${APP_STATE.apiKey}`
                },
                body: JSON.stringify({
                    model: APP_STATE.model,
                    messages: [
                        {
                            role: 'system',
                            content: `You are an expert system design educator specializing in engaging, story-driven education. Your goal is to make complex concepts memorable and exciting.

FORMATTING INSTRUCTIONS:
- Use markdown: **bold**, *italics*, \`code\`, ##headers, ###subheaders
- Mark key concepts with **{CONCEPT_NAME}** (these will be highlighted in boxes)
- Use code blocks with triple backticks for examples
- Use lists with - or 1. for structured information

OPTIONAL SECTION MARKERS (use naturally, don't force):
When these fit organically into your explanation, mark them clearly:
- [STORY] for narrative-driven explanations that hook the reader
- [ANALOGY] for comparisons that make abstract concepts concrete
- [EXAMPLE] for code examples or real system walkthroughs
- [TIP] for practical, actionable advice
- [WARNING] for common pitfalls or mistakes
- [REAL-WORLD] for how actual systems (Facebook, Netflix, Uber, Twitter, Google, Amazon, etc.) solve this problem

CONTENT GUIDELINES:
1. START ENGAGING: Open with a story or question that makes the reader care about this concept
2. USE CONCRETE EXAMPLES: Real systems are more interesting than abstract theory
3. EXPLAIN THE WHY: Help readers understand not just what, but why this matters
4. PROVIDE MENTAL MODELS: Use analogies to complex concepts - relate them to everyday experiences
5. HIGHLIGHT KEY CONCEPTS: Mark important terms with **{braces}** so they stand out
6. INCLUDE EXAMPLES: Code snippets, architectural diagrams (ASCII is fine), or system walkthroughs
7. ADD PRACTICAL VALUE: Give actionable tips that readers can use
8. WARN ABOUT PITFALLS: Help readers avoid common mistakes
9. CONNECT TO REALITY: Show how real companies apply these concepts

TONE & STYLE:
- Conversational and approachable (not academic or textbook-like)
- Expert-level depth but accessible to beginners
- Enthusiastic about the topic
- Make readers excited to learn, not bored

AVOID:
- Generic "by definition" explanations
- Listing features without context
- Disconnected real-world examples
- Being prescriptive or robotic

Remember: Your goal is to make someone understand WHY this concept matters and HOW to think about it, not just to deliver information.`
                        },
                        {
                            role: 'user',
                            content: prompt
                        }
                    ],
                    temperature: 0.7,
                    max_tokens: 8000
                })
            });
            
            if (!response.ok) {
                const error = await response.json();
                throw new Error(error.error?.message || 'API request failed');
            }
            
            const data = await response.json();
            return data.choices[0].message.content;
        }

        async function generateQuizContent(content, topic, difficulty) {
            if (!APP_STATE.apiKey) {
                throw new Error('API key not configured. Please add your Groq API key in Settings.');
            }
            
            const difficultyInstructions = {
                'easy': 'Easy: Ask straightforward comprehension questions about key terms and concepts mentioned in the content. Focus on definitions and basic understanding.',
                'medium': 'Medium: Ask questions that require understanding relationships between concepts in the content. Include scenario-based questions.',
                'hard': 'Hard: Ask complex questions that combine multiple concepts from the content. Test deeper understanding and analysis.',
                'expert': 'Expert: Ask advanced questions that test nuanced understanding and application of concepts from the content. Include edge cases and trade-offs.'
            };
            
            const systemPrompt = `You are a STRICT quiz generator. Your ONLY purpose is to create quiz questions based EXCLUSIVELY on the provided content.

CRITICAL RULES:
1. You MUST create questions ONLY from the provided content
2. Do NOT use any external knowledge or examples
3. Do NOT reference anything not explicitly mentioned in the content
4. Every correct answer MUST be directly supported by the content
5. Every explanation MUST quote or reference the content
6. Return ONLY valid JSON, nothing else`;

            const userPrompt = `Topic: "${topic}"

CONTENT TO USE (this is the ONLY source for all questions):
==========================================
${content}
==========================================

REQUIREMENTS:
- Generate EXACTLY 5 multiple-choice questions
- Difficulty Level: ${difficultyInstructions[difficulty]}
- Each question must have 4 options
- Each option must be plausible based on the content
- Mark correct answer as an index (0, 1, 2, or 3)
- Provide explanation that references the content

RESPONSE FORMAT (ONLY JSON, NO OTHER TEXT):
[
  {
    "question": "Question text",
    "options": ["Option A", "Option B", "Option C", "Option D"],
    "correctAnswer": 0,
    "explanation": "Explanation with reference to content"
  }
]`;

            const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${APP_STATE.apiKey}`
                },
                body: JSON.stringify({
                    model: APP_STATE.model,
                    messages: [
                        {
                            role: 'system',
                            content: systemPrompt
                        },
                        {
                            role: 'user',
                            content: userPrompt
                        }
                    ],
                    temperature: 0.3,
                    max_tokens: 2000
                })
            });
            
            if (!response.ok) {
                const error = await response.json();
                throw new Error(error.error?.message || 'Quiz generation failed');
            }
            
            const data = await response.json();
            return data.choices[0].message.content;
        }

        // ==================== KEYBOARD SHORTCUTS ====================
        document.addEventListener('keydown', (e) => {
            if (e.altKey) {
                if (e.key === 'n') {
                    e.preventDefault();
                    nextConcept();
                } else if (e.key === 'p') {
                    e.preventDefault();
                    prevConcept();
                } else if (e.key === 'q') {
                    e.preventDefault();
                    startQuiz();
                } else if (e.key === 'h') {
                    e.preventDefault();
                    goHome();
                }
            }
        });

        // ==================== START APPLICATION ====================
        init();
    </script>
</body>
</html>
